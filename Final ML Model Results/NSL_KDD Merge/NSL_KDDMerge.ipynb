{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEXLxeOtIqKY"
      },
      "source": [
        "# **Interactive 3D Visualization Framework For Machine Learning Based Network Intrusion Detection Systems**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bh-Cs1HMIGlm"
      },
      "source": [
        "# Libraries\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Iu70wj01szV",
        "outputId": "249d424e-7dd4-400a-e9c9-99c120788a18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: catboost in /usr/local/lib/python3.12/dist-packages (1.2.8)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.5)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "from math import log2\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n",
        "from sklearn.compose import ColumnTransformer, make_column_selector as selector\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, RFECV, RFE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier,AdaBoostClassifier, BaggingClassifier,VotingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import metrics\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "\n",
        "# Enable inline plotting for Jupyter notebooks\n",
        "%matplotlib inline\n",
        "\n",
        "# Filter warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rmt8sv_Jfpv"
      },
      "source": [
        "# Dataset Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXGV6kyC4YTO"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('NSL_KDD.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtoBQI4G4lYe"
      },
      "source": [
        "# Pipeline Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0dsBjGI3_Sv",
        "outputId": "060c229d-bebd-4cac-fef8-fa7ac70e74c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After One Hot shapes: (185559, 123)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# step 1.1: Safe fallback to keep this cell runnable\n",
        "COL_LABEL = \"label\"\n",
        "_need_fallback = False\n",
        "try:\n",
        "    df\n",
        "    COL_LABEL\n",
        "except NameError:\n",
        "    _need_fallback = True\n",
        "\n",
        "if _need_fallback:\n",
        "    rng = np.random.RandomState(42)\n",
        "    n = 300\n",
        "    df = pd.DataFrame({\n",
        "        \"num1\": rng.randn(n),\n",
        "        \"num2\": rng.rand(n) * 5,\n",
        "        \"cat1\": rng.choice([\"tcp\", \"udp\", \"icmp\"], size=n),\n",
        "        \"cat2\": rng.choice([\"low\", \"med\", \"high\"], size=n),\n",
        "        \"label\": ((rng.randn(n) + rng.rand(n) * 2) > 1).astype(int)\n",
        "    })\n",
        "\n",
        "\n",
        "# Step 1.2: Identify feature matrix and target BEFORE split\n",
        "X_all = df.drop(columns=[COL_LABEL]).copy()\n",
        "y_all = df[COL_LABEL].copy()\n",
        "\n",
        "# find column types from the full dataset\n",
        "cat_cols = X_all.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
        "num_cols = X_all.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Step 1.3: One Hot on categoricals and numeric pass through, done BEFORE split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "try:\n",
        "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
        "except TypeError:\n",
        "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
        "\n",
        "# encode categoricals on the full data\n",
        "if len(cat_cols) > 0:\n",
        "    X_all_cat = ohe.fit_transform(X_all[cat_cols].astype(str))\n",
        "    ohe_cols = ohe.get_feature_names_out(cat_cols).tolist()\n",
        "else:\n",
        "    X_all_cat = np.empty((len(X_all), 0))\n",
        "    ohe_cols = []\n",
        "\n",
        "# numeric part\n",
        "X_all_num = X_all[num_cols].to_numpy(dtype=float) if len(num_cols) > 0 else np.empty((len(X_all), 0))\n",
        "\n",
        "# combine\n",
        "X_all_oh = np.hstack([X_all_num, X_all_cat])\n",
        "feat_names = num_cols + ohe_cols\n",
        "\n",
        "print(\"After One Hot shapes:\", X_all_oh.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3n2n0YOI4Hfy"
      },
      "outputs": [],
      "source": [
        "# split AFTER encoding\n",
        "X_train_oh, X_test_oh, y_train, y_test = train_test_split(\n",
        "    X_all_oh, y_all, test_size=0.25, random_state=42, stratify=y_all if y_all.nunique() > 1 else None\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_0tqOiHnj9y",
        "outputId": "35908576-6839-4816-c5ae-94c36b0cc2cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After Normalization shapes: (139169, 123) (46390, 123)\n"
          ]
        }
      ],
      "source": [
        "# STEP 2: Normalization with MinMaxScaler\n",
        "# ================================\n",
        "scaler =  StandardScaler()\n",
        "X_train_normalized = scaler.fit_transform(X_train_oh)\n",
        "X_test_normalized  = scaler.transform(X_test_oh)\n",
        "\n",
        "print(\"After Normalization shapes:\", X_train_normalized.shape, X_test_normalized.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bs2tHbK9nqMS"
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "# STEP 3.1: Training set information gain calculation\n",
        "# ================================\n",
        "mi = mutual_info_classif(X_train_normalized, np.asarray(y_train), random_state=42, discrete_features=False)\n",
        "\n",
        "# 3.2: Convert scores to weights in [0,1], avoid exact zeros\n",
        "mi_max = np.max(mi) if np.max(mi) > 0 else 1.0\n",
        "weights = mi / mi_max\n",
        "weights = np.where(weights == 0, 1e-6, weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQV7xhoenzEL"
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "# STEP 4: Weighted Transform each feature\n",
        "# ================================\n",
        "X_train_weighted = X_train_normalized * weights\n",
        "X_test_weighted  = X_test_normalized  * weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3dMNgnIoCWc"
      },
      "outputs": [],
      "source": [
        "# STEP 5: Transform features to zero mean\n",
        "# ================================\n",
        "train_means = X_train_weighted.mean(axis=0)\n",
        "X_train_centered = X_train_weighted - train_means\n",
        "X_test_centered  = X_test_weighted  - train_means"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiZtTQ2CBXQC",
        "outputId": "8b879d94-bf1e-4089-cf45-916683b0fdd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== PCA output ===\n",
            "Number of components that explain 99.0% variance: 19\n",
            "\n",
            "=== LDA  output ===\n",
            "LDA components used: 4\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 6.1: PCA on SFS output to explain at least 95 percent variance\n",
        "print(\"\\n=== PCA output ===\")\n",
        "pca_probe = PCA().fit(X_train_centered)\n",
        "cum_var = np.cumsum(pca_probe.explained_variance_ratio_)\n",
        "n_components = int(np.argmax(cum_var >= 0.99) + 1)\n",
        "pca = PCA(n_components=max(1, n_components)).fit(X_train_centered)\n",
        "X_train_pca = pca.transform(X_train_centered)\n",
        "X_test_pca  = pca.transform(X_test_centered)\n",
        "print(f'Number of components that explain 99.0% variance: {pca.n_components_}')\n",
        "\n",
        "\n",
        "# 6.2: LDA output\n",
        "\n",
        "if len(np.unique(y_train)) > 1:\n",
        "    print(\"\\n=== LDA  output ===\")\n",
        "    n_classes = len(np.unique(y_train))\n",
        "    n_comp_lda = min(max(1, n_classes - 1), X_train_centered.shape[1])\n",
        "    lda = LDA(n_components=n_comp_lda).fit(X_train_pca, y_train)\n",
        "    X_train_lda = lda.transform(X_train_pca)\n",
        "    X_test_lda  = lda.transform(X_test_pca)\n",
        "    print(f\"LDA components used: {n_comp_lda}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SB6_xOy4Lxmk"
      },
      "source": [
        "# ML Model Results Storage Framework"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fODUcnRmKkZ0",
        "outputId": "2baf0752-eb60-4a57-9434-f845341d65a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model results storage framework loaded successfully!\n",
            "Available functions:\n",
            "- storeResults(model, config, accuracy, f1, recall, precision, auc_roc)\n",
            "- displayAndSaveResults(filename_prefix='model_results')\n",
            "- clearResults()\n",
            "- plotModelComparison(result_df)\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# ML MODEL RESULTS STORAGE FRAMEWORK\n",
        "# =============================================================================\n",
        "\n",
        "# Creating holders to store the model performance results\n",
        "ML_Model = []\n",
        "ML_Config = []\n",
        "accuracy = []\n",
        "f1_score = []\n",
        "recall = []\n",
        "precision = []\n",
        "auc_roc = []  # Adding a holder for AUC-ROC\n",
        "\n",
        "# Function to call for storing the results\n",
        "def storeResults(model, config, a, b, c, d, e):\n",
        "    \"\"\"\n",
        "    Store model performance results\n",
        "\n",
        "    Parameters:\n",
        "    model: Name of the ML model\n",
        "    config: Configuration name (preprocessing steps applied)\n",
        "    a: Accuracy score\n",
        "    b: F1 score\n",
        "    c: Recall score\n",
        "    d: Precision score\n",
        "    e: AUC-ROC score\n",
        "    \"\"\"\n",
        "    ML_Model.append(model)\n",
        "    ML_Config.append(config)\n",
        "    accuracy.append(round(a, 6))\n",
        "    f1_score.append(round(b, 6))\n",
        "    recall.append(round(c, 6))\n",
        "    precision.append(round(d, 6))\n",
        "    auc_roc.append(round(e, 6))\n",
        "\n",
        "# Function to display and save results\n",
        "def displayAndSaveResults(filename_prefix='model_results'):\n",
        "    \"\"\"\n",
        "    Create dataframe from results, display, and save to CSV\n",
        "\n",
        "    Parameters:\n",
        "    filename_prefix: Prefix for the CSV filenames\n",
        "    \"\"\"\n",
        "    # Creating the dataframe\n",
        "    result = pd.DataFrame({\n",
        "        'ML Model': ML_Model,\n",
        "        'Configuration': ML_Config,\n",
        "        'Accuracy': [f\"{acc * 100:.3f}%\" for acc in accuracy],\n",
        "        'F1 Score': [f\"{f1 * 100:.3f}%\" for f1 in f1_score],\n",
        "        'Recall': [f\"{rec * 100:.3f}%\" for rec in recall],\n",
        "        'Precision': [f\"{prec * 100:.3f}%\" for prec in precision],\n",
        "        'ROC_AUC': [f\"{roc * 100:.3f}%\" for roc in auc_roc],\n",
        "    })\n",
        "\n",
        "    # Remove duplicates if any\n",
        "    result.drop_duplicates(subset=[\"ML Model\", \"Configuration\"], inplace=True)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(\"MODEL PERFORMANCE RESULTS\")\n",
        "    print(\"=\"*100)\n",
        "    print(result.to_string(index=False))\n",
        "\n",
        "    # Saving the result to a CSV file\n",
        "    result.to_csv(f'{filename_prefix}.csv', index=False)\n",
        "    print(f\"\\nResults saved to {filename_prefix}.csv\")\n",
        "\n",
        "    # Sorting the dataframe on accuracy and F1 Score\n",
        "    sorted_result = result.sort_values(by=['Accuracy', 'F1 Score'], ascending=False).reset_index(drop=True)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(\"SORTED MODEL PERFORMANCE RESULTS (by Accuracy and F1 Score)\")\n",
        "    print(\"=\"*100)\n",
        "    print(sorted_result.to_string(index=False))\n",
        "\n",
        "    # Saving the sorted result to a CSV file\n",
        "    sorted_result.to_csv(f'sorted_{filename_prefix}.csv', index=False)\n",
        "    print(f\"\\nSorted results saved to sorted_{filename_prefix}.csv\")\n",
        "\n",
        "    return result, sorted_result\n",
        "\n",
        "# Function to clear results (useful when running multiple experiments)\n",
        "def clearResults():\n",
        "    \"\"\"Clear all stored results\"\"\"\n",
        "    global ML_Model, ML_Config, accuracy, f1_score, recall, precision, auc_roc\n",
        "    ML_Model.clear()\n",
        "    ML_Config.clear()\n",
        "    accuracy.clear()\n",
        "    f1_score.clear()\n",
        "    recall.clear()\n",
        "    precision.clear()\n",
        "    auc_roc.clear()\n",
        "    print(\"Results cleared!\")\n",
        "\n",
        "# Function to plot model comparison\n",
        "def plotModelComparison(result_df):\n",
        "    \"\"\"\n",
        "    Create visualization comparing model performances\n",
        "\n",
        "    Parameters:\n",
        "    result_df: DataFrame with model results\n",
        "    \"\"\"\n",
        "    # Convert percentage strings back to floats for plotting\n",
        "    metrics_cols = ['Accuracy', 'F1 Score', 'Recall', 'Precision', 'ROC_AUC']\n",
        "    plot_df = result_df.copy()\n",
        "\n",
        "    for col in metrics_cols:\n",
        "        plot_df[col] = plot_df[col].str.rstrip('%').astype(float)\n",
        "\n",
        "    # Create subplot for each metric\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "    axes = axes.ravel()\n",
        "\n",
        "    for idx, metric in enumerate(metrics_cols):\n",
        "        # Group by model and get mean performance across configurations\n",
        "        model_performance = plot_df.groupby('ML Model')[metric].mean().sort_values(ascending=False)\n",
        "\n",
        "        # Create bar plot\n",
        "        ax = axes[idx]\n",
        "        bars = ax.bar(range(len(model_performance)), model_performance.values,\n",
        "                      color=plt.cm.Blues(np.linspace(0.4, 0.9, len(model_performance))))\n",
        "        ax.set_xticks(range(len(model_performance)))\n",
        "        ax.set_xticklabels(model_performance.index, rotation=45, ha='right')\n",
        "        ax.set_ylabel(f'{metric} (%)')\n",
        "        ax.set_title(f'Average {metric} by Model', fontweight='bold')\n",
        "        ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "        # Add value labels on bars\n",
        "        for bar in bars:\n",
        "            height = bar.get_height()\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                   f'{height:.1f}%', ha='center', va='bottom')\n",
        "\n",
        "    # Hide the last subplot if we have 5 metrics\n",
        "    if len(metrics_cols) == 5:\n",
        "        axes[5].set_visible(False)\n",
        "\n",
        "    plt.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "print(\"Model results storage framework loaded successfully!\")\n",
        "print(\"Available functions:\")\n",
        "print(\"- storeResults(model, config, accuracy, f1, recall, precision, auc_roc)\")\n",
        "print(\"- displayAndSaveResults(filename_prefix='model_results')\")\n",
        "print(\"- clearResults()\")\n",
        "print(\"- plotModelComparison(result_df)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHwhvio-qU3A"
      },
      "source": [
        "#SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiHesVrJO0mI",
        "outputId": "14f87eca-8983-4c76-8659-9c87f19b7375"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== SVM Model Performance  ===\n",
            "\n",
            "Running SVM with PCA configuration...\n",
            "\n",
            "SVM Model Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.986958  0.785539 0.788589   0.965700 0.998706\n",
            "    Test  0.987088  0.773726 0.782342   0.765859 0.998304\n",
            "\n",
            "Running SVM with LDA configuration...\n",
            "\n",
            "SVM Model Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.954624  0.737193 0.748722   0.727092 0.994087\n",
            "    Test  0.955702  0.737363 0.747554   0.728329 0.994134\n"
          ]
        }
      ],
      "source": [
        "# Configuration list to store different data setups\n",
        "configurations = []\n",
        "\n",
        "configurations.append((f'PCA', X_train_pca, X_test_pca, y_train))\n",
        "configurations.append((f'LDA', X_train_lda, X_test_lda, y_train))\n",
        "\n",
        "# Step 7: Run SVM  on different configurations\n",
        "print(\"\\n=== SVM Model Performance  ===\")\n",
        "svm = SVC(\n",
        "    kernel=\"rbf\",\n",
        "    C=1.0,\n",
        "    gamma=\"scale\",\n",
        "    probability=True,\n",
        "    random_state=42\n",
        "    )\n",
        "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "    print(f\"\\nRunning SVM with {name} configuration...\")\n",
        "\n",
        "    svm.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    y_train_pred = svm.predict(X_train_cfg)\n",
        "    y_test_pred  = svm.predict(X_test_cfg)\n",
        "\n",
        "    y_train_proba = svm.predict_proba(X_train_cfg)\n",
        "    y_test_proba  = svm.predict_proba(X_test_cfg)\n",
        "\n",
        "\n",
        "    metrics_dict = {\n",
        "        \"Dataset\": [\"Training\", \"Test\"],\n",
        "        \"Accuracy\": [\n",
        "            metrics.accuracy_score(y_train_cfg, y_train_pred),\n",
        "            metrics.accuracy_score(y_test,      y_test_pred),\n",
        "        ],\n",
        "        \"F1 Score\": [\n",
        "            metrics.f1_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.f1_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"Recall\": [\n",
        "            metrics.recall_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.recall_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"Precision\": [\n",
        "            metrics.precision_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.precision_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"AUC-ROC\": [\n",
        "            metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_proba, multi_class='ovr', average='macro'),\n",
        "            metrics.roc_auc_score(pd.get_dummies(y_test),      y_test_proba,  multi_class='ovr', average='macro'),\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics_dict)\n",
        "    print(\"\\nSVM Model Performance Metrics\")\n",
        "    print(df_metrics.to_string(index=False))\n",
        "\n",
        "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_proba, multi_class='ovr', average='macro')\n",
        "    storeResults('Support Vector Machine', name,\n",
        "                 metrics.accuracy_score(y_test, y_test_pred),\n",
        "                 metrics.f1_score(y_test, y_test_pred, average='macro'),\n",
        "                 metrics.recall_score(y_test, y_test_pred, average='macro'),\n",
        "                 metrics.precision_score(y_test, y_test_pred, average='macro'),\n",
        "                 auc_score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "ucZiWQH7y9Pq",
        "outputId": "227ed507-c33a-49b6-b0d5-9b176a600d9b"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1512222955.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_cfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_cfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_cfg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfigurations\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rbf\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"scale\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobability\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_cfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_cfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0my_pred_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_cfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"i\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_status_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibsvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# === Confusion matrices for all configurations (safe to run anytime) ===\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.svm import SVC\n",
        "import numpy as np, pandas as pd\n",
        "\n",
        "def nslkdd_confusion_table(y_true, y_pred, class_order=None, title_no=5):\n",
        "    if class_order is None:\n",
        "        classes = list(pd.unique(pd.Series(list(y_true) + list(y_pred))))\n",
        "    else:\n",
        "        present = set(pd.unique(pd.Series(list(y_true) + list(y_pred))))\n",
        "        classes = [c for c in class_order if c in present] or list(present)\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
        "    df = pd.DataFrame(cm, index=[f\"Actual  {c}\" for c in classes], columns=classes)\n",
        "\n",
        "    row_tot = cm.sum(axis=1)\n",
        "    recalls = np.divide(np.diag(cm), row_tot, out=np.zeros_like(row_tot, dtype=float), where=row_tot != 0) * 100.0\n",
        "    df[\"Recall (%)\"] = np.round(recalls, 1)\n",
        "\n",
        "    col_tot = cm.sum(axis=0)\n",
        "    precisions = np.divide(np.diag(cm), col_tot, out=np.zeros_like(col_tot, dtype=float), where=col_tot != 0) * 100.0\n",
        "    prec_row = pd.Series(np.round(precisions, 1), index=classes, name=\"Precision (%)\")\n",
        "    prec_row[\"Recall (%)\"] = \"\"\n",
        "    df = pd.concat([df, prec_row.to_frame().T], axis=0)\n",
        "\n",
        "    print(f\"\\nTable {title_no}\")\n",
        "    print(\"Multi category classification confusion matrix for the NSL KDD dataset.\\n\")\n",
        "    print(\"Predicted\\n\")\n",
        "    print(df.to_string())\n",
        "    return df\n",
        "\n",
        "# --- Use cached predictions if available; otherwise rebuild them now ---\n",
        "if 'y_pred_cache' not in globals() or not isinstance(y_pred_cache, dict) or len(y_pred_cache) == 0:\n",
        "    y_pred_cache = {}\n",
        "    for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "        clf = SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\", probability=True, random_state=42)\n",
        "        clf.fit(X_train_cfg, y_train_cfg)\n",
        "        y_pred_cache[name] = clf.predict(X_test_cfg)\n",
        "\n",
        "# --- Build & save confusion matrices for all configs ---\n",
        "nsl_order = [\"DoS\", \"Normal\", \"Probe\", \"R2L\", \"U2R\"]\n",
        "for name, y_pred in y_pred_cache.items():\n",
        "    print(f\"\\n=== Confusion Matrix: {name} ===\")\n",
        "    tbl = nslkdd_confusion_table(y_test, y_pred, class_order=nsl_order, title_no=5)\n",
        "    path = f\"nslkdd_confusion_table_SVM_{name}.csv\"\n",
        "    tbl.to_csv(path, index=True)\n",
        "    print(f\"Saved to {path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfra_hFKaulE"
      },
      "source": [
        "# Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvIm4D6eQZ50",
        "outputId": "0a5e93fe-6530-41c4-86cc-aae24c0e06dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Random Forestr Model Performance ===\n",
            "\n",
            "Running Random Forest with PCA configuration...\n",
            "\n",
            "RF Model Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  1.000000  1.000000 1.000000   1.000000 1.000000\n",
            "    Test  0.998232  0.966739 0.956347   0.978985 0.999904\n",
            "\n",
            "Running Random Forest with LDA configuration...\n",
            "\n",
            "RF Model Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  1.000000  1.000000 1.000000   1.000000 1.000000\n",
            "    Test  0.994546  0.967677 0.961053   0.975118 0.997484\n"
          ]
        }
      ],
      "source": [
        "# Configuration list to store different data setups\n",
        "configurations = []\n",
        "\n",
        "configurations.append((f'PCA', X_train_pca, X_test_pca, y_train))\n",
        "configurations.append((f'LDA', X_train_lda, X_test_lda, y_train))\n",
        "\n",
        "# Step 7: Run RF different configurations\n",
        "print(\"\\n=== Random Forestr Model Performance ===\")\n",
        "\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=None,\n",
        "    min_samples_split=2,\n",
        "    min_samples_leaf=1,\n",
        "    max_features=\"sqrt\",\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "    print(f\"\\nRunning Random Forest with {name} configuration...\")\n",
        "\n",
        "    rf.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    y_train_pred = rf.predict(X_train_cfg)\n",
        "    y_test_pred  = rf.predict(X_test_cfg)\n",
        "\n",
        "    y_train_proba = rf.predict_proba(X_train_cfg)\n",
        "    y_test_proba  = rf.predict_proba(X_test_cfg)\n",
        "\n",
        "\n",
        "    metrics_dict = {\n",
        "        \"Dataset\": [\"Training\", \"Test\"],\n",
        "        \"Accuracy\": [\n",
        "            metrics.accuracy_score(y_train_cfg, y_train_pred),\n",
        "            metrics.accuracy_score(y_test,      y_test_pred),\n",
        "        ],\n",
        "        \"F1 Score\": [\n",
        "            metrics.f1_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.f1_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"Recall\": [\n",
        "            metrics.recall_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.recall_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"Precision\": [\n",
        "            metrics.precision_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.precision_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"AUC-ROC\": [\n",
        "            metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_proba, multi_class='ovr', average='macro'),\n",
        "            metrics.roc_auc_score(pd.get_dummies(y_test),      y_test_proba,  multi_class='ovr', average='macro'),\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics_dict)\n",
        "    print(\"\\nRF Model Performance Metrics\")\n",
        "    print(df_metrics.to_string(index=False))\n",
        "\n",
        "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_proba, multi_class='ovr', average='macro')\n",
        "    storeResults('Random Forest', name,\n",
        "                 metrics.accuracy_score(y_test, y_test_pred),\n",
        "                 metrics.f1_score(y_test, y_test_pred, average='macro'),\n",
        "                 metrics.recall_score(y_test, y_test_pred, average='macro'),\n",
        "                 metrics.precision_score(y_test, y_test_pred, average='macro'),\n",
        "                 auc_score)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Km6NhCQObcrt"
      },
      "source": [
        " # KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRI3hY30RI6N",
        "outputId": "71c95ff0-3bff-44fe-8415-cb97f9766cc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== KNN Model Performance ===\n",
            "\n",
            "Running KNN with PCA configuration...\n",
            "\n",
            "KNN Model Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training   1.00000  1.000000 1.000000   1.000000 1.000000\n",
            "    Test   0.99737  0.970783 0.964408   0.977984 0.994882\n",
            "\n",
            "Running KNN with LDA configuration...\n",
            "\n",
            "KNN Model Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  1.000000  1.000000 1.000000   1.000000 1.000000\n",
            "    Test  0.993684  0.958819 0.951677   0.967244 0.994301\n"
          ]
        }
      ],
      "source": [
        "# Configuration list to store different data setups\n",
        "configurations = []\n",
        "\n",
        "configurations.append((f'PCA', X_train_pca, X_test_pca, y_train))\n",
        "configurations.append((f'LDA', X_train_lda, X_test_lda, y_train))\n",
        "\n",
        "\n",
        "# Step 7: Run KNN  on different configurations\n",
        "print(\"\\n=== KNN Model Performance ===\")\n",
        "\n",
        "knn = KNeighborsClassifier(\n",
        "    n_neighbors=11,\n",
        "    weights=\"distance\",\n",
        "    metric=\"minkowski\",\n",
        "    p=2,\n",
        "    n_jobs=-1\n",
        ")\n",
        "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "    print(f\"\\nRunning KNN with {name} configuration...\")\n",
        "\n",
        "    knn.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    y_train_pred = knn.predict(X_train_cfg)\n",
        "    y_test_pred  = knn.predict(X_test_cfg)\n",
        "\n",
        "    y_train_proba = knn.predict_proba(X_train_cfg)\n",
        "    y_test_proba  = knn.predict_proba(X_test_cfg)\n",
        "\n",
        "\n",
        "    metrics_dict = {\n",
        "        \"Dataset\": [\"Training\", \"Test\"],\n",
        "        \"Accuracy\": [\n",
        "            metrics.accuracy_score(y_train_cfg, y_train_pred),\n",
        "            metrics.accuracy_score(y_test,      y_test_pred),\n",
        "        ],\n",
        "        \"F1 Score\": [\n",
        "            metrics.f1_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.f1_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"Recall\": [\n",
        "            metrics.recall_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.recall_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"Precision\": [\n",
        "            metrics.precision_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.precision_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"AUC-ROC\": [\n",
        "            metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_proba, multi_class='ovr', average='macro'),\n",
        "            metrics.roc_auc_score(pd.get_dummies(y_test),      y_test_proba,  multi_class='ovr', average='macro'),\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics_dict)\n",
        "    print(\"\\nKNN Model Performance Metrics\")\n",
        "    print(df_metrics.to_string(index=False))\n",
        "\n",
        "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_proba, multi_class='ovr', average='macro')\n",
        "    storeResults('KNN', name,\n",
        "                 metrics.accuracy_score(y_test, y_test_pred),\n",
        "                 metrics.f1_score(y_test, y_test_pred, average='macro'),\n",
        "                 metrics.recall_score(y_test, y_test_pred, average='macro'),\n",
        "                 metrics.precision_score(y_test, y_test_pred, average='macro'),\n",
        "                 auc_score)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbLnqqocdWZR"
      },
      "source": [
        "# Gradient Boosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyc9p_6zRqUK",
        "outputId": "ae4871d1-6055-48e7-d1bb-bf71b652d177"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Gradient Boosting Model Performance ===\n",
            "\n",
            "Running Gradient Boosting with PCA configuration...\n",
            "\n",
            "Gradient Boosting Model Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.993310  0.887161 0.865444   0.935985 0.974450\n",
            "    Test  0.990645  0.878778 0.863546   0.908042 0.979998\n",
            "\n",
            "Running Gradient Boosting with LDA configuration...\n",
            "\n",
            "Gradient Boosting Model Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.968995  0.853290 0.832289   0.902218 0.968327\n",
            "    Test  0.966415  0.840327 0.823272   0.880442 0.958179\n"
          ]
        }
      ],
      "source": [
        "# Configuration list to store different data setups\n",
        "configurations = []\n",
        "\n",
        "configurations.append((f'PCA', X_train_pca, X_test_pca, y_train))\n",
        "configurations.append((f'LDA', X_train_lda, X_test_lda, y_train))\n",
        "\n",
        "# Step 7: Running Gradient Boosting  on different configurations\n",
        "print(\"\\n=== Gradient Boosting Model Performance ===\")\n",
        "\n",
        "gbc = GradientBoostingClassifier(\n",
        "    n_estimators=200,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=3,\n",
        "    min_samples_split=2,\n",
        "    min_samples_leaf=1,\n",
        "    subsample=1.0,\n",
        "    random_state=42\n",
        ")\n",
        "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "    print(f\"\\nRunning Gradient Boosting with {name} configuration...\")\n",
        "\n",
        "    gbc.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    y_train_pred = gbc.predict(X_train_cfg)\n",
        "    y_test_pred  = gbc.predict(X_test_cfg)\n",
        "\n",
        "    y_train_proba = gbc.predict_proba(X_train_cfg)\n",
        "    y_test_proba  = gbc.predict_proba(X_test_cfg)\n",
        "\n",
        "\n",
        "    metrics_dict = {\n",
        "        \"Dataset\": [\"Training\", \"Test\"],\n",
        "        \"Accuracy\": [\n",
        "            metrics.accuracy_score(y_train_cfg, y_train_pred),\n",
        "            metrics.accuracy_score(y_test,      y_test_pred),\n",
        "        ],\n",
        "        \"F1 Score\": [\n",
        "            metrics.f1_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.f1_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"Recall\": [\n",
        "            metrics.recall_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.recall_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"Precision\": [\n",
        "            metrics.precision_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.precision_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"AUC-ROC\": [\n",
        "            metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_proba, multi_class='ovr', average='macro'),\n",
        "            metrics.roc_auc_score(pd.get_dummies(y_test),      y_test_proba,  multi_class='ovr', average='macro'),\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics_dict)\n",
        "    print(\"\\nGradient Boosting Model Performance Metrics\")\n",
        "    print(df_metrics.to_string(index=False))\n",
        "\n",
        "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_proba, multi_class='ovr', average='macro')\n",
        "    storeResults('Gradient Boosting', name,\n",
        "                 metrics.accuracy_score(y_test, y_test_pred),\n",
        "                 metrics.f1_score(y_test, y_test_pred, average='macro'),\n",
        "                 metrics.recall_score(y_test, y_test_pred, average='macro'),\n",
        "                 metrics.precision_score(y_test, y_test_pred, average='macro'),\n",
        "                 auc_score)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hfc0wi85eWOv"
      },
      "source": [
        "#AdaBoosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzUf7LW6SQrF",
        "outputId": "38060c6c-dc2c-42e5-cc81-f1a5cc924414"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== AdaBoost Model Performance ===\n",
            "\n",
            "Running AdaBoost with PCA configuration...\n",
            "\n",
            "AdaBoosting Model Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.889861  0.617671 0.590525   0.661199 0.970492\n",
            "    Test  0.890494  0.619279 0.592466   0.661399 0.970474\n",
            "\n",
            "Running AdaBoost with LDA configuration...\n",
            "\n",
            "AdaBoosting Model Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.903944  0.642798 0.630292   0.658439 0.978965\n",
            "    Test  0.902824  0.640876 0.628797   0.655935 0.978678\n"
          ]
        }
      ],
      "source": [
        "# Configuration list to store different data setups\n",
        "configurations = []\n",
        "\n",
        "configurations.append((f'PCA', X_train_pca, X_test_pca, y_train))\n",
        "configurations.append((f'LDA', X_train_lda, X_test_lda, y_train))\n",
        "\n",
        "# Step 7: Run AdaBoost  on different configurations\n",
        "print(\"\\n=== AdaBoost Model Performance ===\")\n",
        "\n",
        "ada = AdaBoostClassifier(\n",
        "        estimator=DecisionTreeClassifier(max_depth=1, random_state=42),\n",
        "        n_estimators=200,\n",
        "        learning_rate=0.1,\n",
        "        algorithm=\"SAMME\",\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "    print(f\"\\nRunning AdaBoost with {name} configuration...\")\n",
        "\n",
        "    ada.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    y_train_pred = ada.predict(X_train_cfg)\n",
        "    y_test_pred  = ada.predict(X_test_cfg)\n",
        "\n",
        "    y_train_proba = ada.predict_proba(X_train_cfg)\n",
        "    y_test_proba  = ada.predict_proba(X_test_cfg)\n",
        "\n",
        "\n",
        "    metrics_dict = {\n",
        "        \"Dataset\": [\"Training\", \"Test\"],\n",
        "        \"Accuracy\": [\n",
        "            metrics.accuracy_score(y_train_cfg, y_train_pred),\n",
        "            metrics.accuracy_score(y_test,      y_test_pred),\n",
        "        ],\n",
        "        \"F1 Score\": [\n",
        "            metrics.f1_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.f1_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"Recall\": [\n",
        "            metrics.recall_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.recall_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"Precision\": [\n",
        "            metrics.precision_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.precision_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"AUC-ROC\": [\n",
        "            metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_proba, multi_class='ovr', average='macro'),\n",
        "            metrics.roc_auc_score(pd.get_dummies(y_test),      y_test_proba,  multi_class='ovr', average='macro'),\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics_dict)\n",
        "    print(\"\\nAdaBoosting Model Performance Metrics\")\n",
        "    print(df_metrics.to_string(index=False))\n",
        "\n",
        "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_proba, multi_class='ovr', average='macro')\n",
        "    storeResults('AdaBoosting', name,\n",
        "                 metrics.accuracy_score(y_test, y_test_pred),\n",
        "                 metrics.f1_score(y_test, y_test_pred, average='macro'),\n",
        "                 metrics.recall_score(y_test, y_test_pred, average='macro'),\n",
        "                 metrics.precision_score(y_test, y_test_pred, average='macro'),\n",
        "                 auc_score)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyjP2M1SY-E6"
      },
      "source": [
        "# XGBoosting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZR9tnx0S5hD",
        "outputId": "aaeca68a-1907-4fb0-9910-3d28790251f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== XGBoost Model Performance ===\n",
            "\n",
            "Running XGBoosting with PCA configuration...\n",
            "\n",
            "XGBoosting Model Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.999813  0.999571 0.999864   0.999279 1.000000\n",
            "    Test  0.997887  0.970168 0.969220   0.971175 0.999905\n",
            "\n",
            "Running XGBoosting with LDA configuration...\n",
            "\n",
            "XGBoosting Model Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.989344  0.951134 0.944464   0.959344 0.999775\n",
            "    Test  0.984889  0.905786 0.889268   0.932112 0.999316\n"
          ]
        }
      ],
      "source": [
        "# Configuration list to store different data setups\n",
        "configurations = []\n",
        "\n",
        "le = LabelEncoder()\n",
        "# Fit on the union to avoid unseen class errors if test has a class not in train\n",
        "le.fit(pd.concat([y_train.astype(str), y_test.astype(str)], axis=0))\n",
        "\n",
        "y_train_enc = le.transform(y_train.astype(str))\n",
        "y_test_enc  = le.transform(y_test.astype(str))\n",
        "\n",
        "configurations.append((f'PCA', X_train_pca, X_test_pca,y_train_enc))\n",
        "configurations.append((f'LDA', X_train_lda, X_test_lda, y_train_enc))\n",
        "\n",
        "\n",
        "# Step 7: Run XGBoosting on different configurations\n",
        "print(\"\\n=== XGBoost Model Performance ===\")\n",
        "\n",
        "xgb = XGBClassifier(\n",
        "    n_estimators=200,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=6,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    tree_method=\"hist\",\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "    print(f\"\\nRunning XGBoosting with {name} configuration...\")\n",
        "\n",
        "    xgb.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    y_train_pred = xgb.predict(X_train_cfg)\n",
        "    y_test_pred  = xgb.predict(X_test_cfg)\n",
        "\n",
        "    y_train_proba = xgb.predict_proba(X_train_cfg)\n",
        "    y_test_proba  = xgb.predict_proba(X_test_cfg)\n",
        "\n",
        "\n",
        "    metrics_dict = {\n",
        "        \"Dataset\": [\"Training\", \"Test\"],\n",
        "        \"Accuracy\": [\n",
        "            metrics.accuracy_score(y_train_cfg, y_train_pred),\n",
        "            metrics.accuracy_score(y_test_enc,      y_test_pred),\n",
        "        ],\n",
        "        \"F1 Score\": [\n",
        "            metrics.f1_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.f1_score(y_test_enc,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"Recall\": [\n",
        "            metrics.recall_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.recall_score(y_test_enc,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"Precision\": [\n",
        "            metrics.precision_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.precision_score(y_test_enc,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"AUC-ROC\": [\n",
        "            metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_proba, multi_class='ovr', average='macro'),\n",
        "            metrics.roc_auc_score(pd.get_dummies(y_test_enc),      y_test_proba,  multi_class='ovr', average='macro'),\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics_dict)\n",
        "    print(\"\\nXGBoosting Model Performance Metrics\")\n",
        "    print(df_metrics.to_string(index=False))\n",
        "\n",
        "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_proba, multi_class='ovr', average='macro')\n",
        "    storeResults('XGBoosting', name,\n",
        "                 metrics.accuracy_score(y_test_enc, y_test_pred),\n",
        "                 metrics.f1_score(y_test_enc, y_test_pred, average='macro'),\n",
        "                 metrics.recall_score(y_test_enc, y_test_pred, average='macro'),\n",
        "                 metrics.precision_score(y_test_enc, y_test_pred, average='macro'),\n",
        "                 auc_score)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xFOCq0pwChT"
      },
      "source": [
        "#CatBoosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzTaGUy_wHGq",
        "outputId": "218633d4-853f-4181-90e1-855895d09726"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== CatBoost Model Performance ===\n",
            "\n",
            "Running CatBoosting with PCA configuration...\n",
            "\n",
            "CatBoosting Model Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.949745  0.714573 0.712716   0.716611 0.993252\n",
            "    Test  0.949946  0.715118 0.714248   0.716221 0.993899\n",
            "\n",
            "Running CatBoosting with LDA configuration...\n",
            "\n",
            "CatBoosting Model Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.930933  0.703927 0.716191   0.694327 0.992479\n",
            "    Test  0.931666  0.704764 0.716549   0.695598 0.992742\n"
          ]
        }
      ],
      "source": [
        "# Configuration list to store different data setups\n",
        "configurations = []\n",
        "\n",
        "configurations.append((f'PCA', X_train_pca, X_test_pca, y_train))\n",
        "configurations.append((f'LDA', X_train_lda, X_test_lda, y_train))\n",
        "\n",
        "\n",
        "# Step 7: Run CatBoosting on different configurations\n",
        "print(\"\\n=== CatBoost Model Performance ===\")\n",
        "cat = CatBoostClassifier(\n",
        "\n",
        "\n",
        "    bagging_temperature =0.05,\n",
        "    boosting_type = 'Plain',\n",
        "    learning_rate=0.05,\n",
        "    depth=3,\n",
        "    n_estimators=100,\n",
        "    random_seed=42,\n",
        "    silent =True\n",
        "\n",
        ")\n",
        "\n",
        "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "    print(f\"\\nRunning CatBoosting with {name} configuration...\")\n",
        "\n",
        "    cat.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    y_train_pred = cat.predict(X_train_cfg)\n",
        "    y_test_pred  = cat.predict(X_test_cfg)\n",
        "\n",
        "    y_train_proba = cat.predict_proba(X_train_cfg)\n",
        "    y_test_proba  = cat.predict_proba(X_test_cfg)\n",
        "\n",
        "\n",
        "    metrics_dict = {\n",
        "        \"Dataset\": [\"Training\", \"Test\"],\n",
        "        \"Accuracy\": [\n",
        "            metrics.accuracy_score(y_train_cfg, y_train_pred),\n",
        "            metrics.accuracy_score(y_test,      y_test_pred),\n",
        "        ],\n",
        "        \"F1 Score\": [\n",
        "            metrics.f1_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.f1_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"Recall\": [\n",
        "            metrics.recall_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.recall_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"Precision\": [\n",
        "            metrics.precision_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.precision_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"AUC-ROC\": [\n",
        "            metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_proba, multi_class='ovr', average='macro'),\n",
        "            metrics.roc_auc_score(pd.get_dummies(y_test),      y_test_proba,  multi_class='ovr', average='macro'),\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics_dict)\n",
        "    print(\"\\nCatBoosting Model Performance Metrics\")\n",
        "    print(df_metrics.to_string(index=False))\n",
        "\n",
        "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_proba, multi_class='ovr', average='macro')\n",
        "    storeResults('CatBoosting', name,\n",
        "                 metrics.accuracy_score(y_test, y_test_pred),\n",
        "                 metrics.f1_score(y_test, y_test_pred, average='macro'),\n",
        "                 metrics.recall_score(y_test, y_test_pred, average='macro'),\n",
        "                 metrics.precision_score(y_test, y_test_pred, average='macro'),\n",
        "                 auc_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmOcixna2M5w"
      },
      "source": [
        "#Bagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NM05mZ_7UqbV",
        "outputId": "309aff68-9e1e-4913-ed30-231ae190952c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Bagging Model Performance ===\n",
            "\n",
            "Running Bagging with PCA configuration...\n",
            "\n",
            "Bagging Model Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.946353  0.709919 0.720889   0.702906 0.990496\n",
            "    Test  0.946756  0.709147 0.720151   0.702007 0.991983\n",
            "\n",
            "Running Bagging with LDA configuration...\n",
            "\n",
            "Bagging Model Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.935977  0.716155 0.734860   0.701037 0.992353\n",
            "    Test  0.936322  0.716661 0.734418   0.702346 0.991928\n"
          ]
        }
      ],
      "source": [
        "# Configuration list to store different data setups\n",
        "configurations = []\n",
        "\n",
        "configurations.append((f'PCA', X_train_pca, X_test_pca, y_train))\n",
        "configurations.append((f'LDA', X_train_lda, X_test_lda, y_train))\n",
        "\n",
        "# Step 7: Run Bagging Classifier on different configurations\n",
        "print(\"\\n=== Bagging Model Performance ===\")\n",
        "\n",
        "bag = BaggingClassifier(\n",
        "        estimator=DecisionTreeClassifier(max_depth=5, random_state=42),\n",
        "        n_estimators=200,\n",
        "        max_samples=1.0,\n",
        "        max_features=1.0,\n",
        "        bootstrap=True,\n",
        "        bootstrap_features=False,\n",
        "        n_jobs=-1,\n",
        "        random_state=42\n",
        "    )\n",
        "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "    print(f\"\\nRunning Bagging with {name} configuration...\")\n",
        "\n",
        "    bag.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    y_train_pred = bag.predict(X_train_cfg)\n",
        "    y_test_pred  = bag.predict(X_test_cfg)\n",
        "\n",
        "    y_train_proba = bag.predict_proba(X_train_cfg)\n",
        "    y_test_proba  = bag.predict_proba(X_test_cfg)\n",
        "\n",
        "\n",
        "    metrics_dict = {\n",
        "        \"Dataset\": [\"Training\", \"Test\"],\n",
        "        \"Accuracy\": [\n",
        "            metrics.accuracy_score(y_train_cfg, y_train_pred),\n",
        "            metrics.accuracy_score(y_test,      y_test_pred),\n",
        "        ],\n",
        "        \"F1 Score\": [\n",
        "            metrics.f1_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.f1_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"Recall\": [\n",
        "            metrics.recall_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.recall_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"Precision\": [\n",
        "            metrics.precision_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.precision_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"AUC-ROC\": [\n",
        "            metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_proba, multi_class='ovr', average='macro'),\n",
        "            metrics.roc_auc_score(pd.get_dummies(y_test),      y_test_proba,  multi_class='ovr', average='macro'),\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics_dict)\n",
        "    print(\"\\nBagging Model Performance Metrics\")\n",
        "    print(df_metrics.to_string(index=False))\n",
        "\n",
        "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_proba, multi_class='ovr', average='macro')\n",
        "    storeResults('Bagging Classifier', name,\n",
        "                 metrics.accuracy_score(y_test, y_test_pred),\n",
        "                 metrics.f1_score(y_test, y_test_pred, average='macro'),\n",
        "                 metrics.recall_score(y_test, y_test_pred, average='macro'),\n",
        "                 metrics.precision_score(y_test, y_test_pred, average='macro'),\n",
        "                 auc_score)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mmlx0Lwv4lMI"
      },
      "source": [
        "# Voting Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "930yU9PYVQhQ",
        "outputId": "3e7f295b-057d-4615-cb5e-bf97eff024e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Voting Model Performance ===\n",
            "\n",
            "Training models with PCA configuration...\n",
            "\n",
            "=== Voting Classifier (hard) with PCA ===\n",
            "\n",
            "Voting Classifier (hard) Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  1.000000  1.000000 1.000000   1.000000        0\n",
            "    Test  0.998168  0.972309 0.965323   0.980106        0\n",
            "\n",
            "=== Voting Classifier (soft) with PCA ===\n",
            "\n",
            "Voting Classifier (soft) Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  1.000000  1.000000 1.000000   1.000000 1.000000\n",
            "    Test  0.998081  0.971986 0.965221   0.979567 0.999933\n",
            "\n",
            "=== Voting Classifier (weighted_hard) with PCA ===\n",
            "\n",
            "Voting Classifier (weighted_hard) Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  1.000000  1.000000 1.000000   1.000000        0\n",
            "    Test  0.998232  0.972394 0.965455   0.980144        0\n",
            "\n",
            "=== Voting Classifier (weighted_soft) with PCA ===\n",
            "\n",
            "Voting Classifier (weighted_soft) Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  1.000000  1.000000 1.000000   1.000000 1.000000\n",
            "    Test  0.998146  0.966685 0.956207   0.979014 0.999933\n",
            "\n",
            "Training models with LDA configuration...\n",
            "\n",
            "=== Voting Classifier (hard) with LDA ===\n",
            "\n",
            "Voting Classifier (hard) Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  1.000000  1.000000 1.000000   1.000000        0\n",
            "    Test  0.994201  0.961816 0.952025   0.973476        0\n",
            "\n",
            "=== Voting Classifier (soft) with LDA ===\n",
            "\n",
            "Voting Classifier (soft) Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  1.000000  1.000000 1.000000   1.000000 1.000000\n",
            "    Test  0.994007  0.961365 0.951815   0.972794 0.999795\n",
            "\n",
            "=== Voting Classifier (weighted_hard) with LDA ===\n",
            "\n",
            "Voting Classifier (weighted_hard) Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  1.000000  1.000000 1.000000   1.000000        0\n",
            "    Test  0.994158  0.961744 0.951998   0.973362        0\n",
            "\n",
            "=== Voting Classifier (weighted_soft) with LDA ===\n",
            "\n",
            "Voting Classifier (weighted_soft) Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  1.000000  1.000000 1.000000   1.000000   1.0000\n",
            "    Test  0.994094  0.961587 0.951985   0.973063   0.9998\n"
          ]
        }
      ],
      "source": [
        "# Configuration list to store different data setups\n",
        "configurations = []\n",
        "\n",
        "le = LabelEncoder()\n",
        "# Fit on the union to avoid unseen class errors if test has a class not in train\n",
        "le.fit(pd.concat([y_train.astype(str), y_test.astype(str)], axis=0))\n",
        "\n",
        "y_train_enc = le.transform(y_train.astype(str))\n",
        "y_test_enc  = le.transform(y_test.astype(str))\n",
        "\n",
        "configurations.append((f'PCA', X_train_pca, X_test_pca,y_train_enc))\n",
        "configurations.append((f'LDA', X_train_lda, X_test_lda, y_train_enc))\n",
        "\n",
        "\n",
        "# Step 7: Run Voting Classifier on different configurations\n",
        "print(\"\\n=== Voting Model Performance ===\")\n",
        "\n",
        "# base learners\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=None,\n",
        "    min_samples_split=2,\n",
        "    min_samples_leaf=1,\n",
        "    max_features=\"sqrt\",\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "knn = KNeighborsClassifier(\n",
        "    n_neighbors=11,\n",
        "    weights=\"distance\",\n",
        "    metric=\"minkowski\",\n",
        "    p=2,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "xgb = XGBClassifier(\n",
        "    n_estimators=200,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=6,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    tree_method=\"hist\",\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "    print(f\"\\nTraining models with {name} configuration...\")\n",
        "\n",
        "    rf.fit(X_train_cfg, y_train_cfg)\n",
        "    knn.fit(X_train_cfg, y_train_cfg)\n",
        "    xgb.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    # Define y_test_cfg for each configuration\n",
        "    y_test_cfg = y_test_enc\n",
        "    # Voting configurations using the actual model objects\n",
        "    # === Voting classifier configurations ===\n",
        "    voting_configs = [\n",
        "       (\"hard\", VotingClassifier(estimators=[(\"rf\", rf), (\"knn\", knn), (\"xgb\", xgb)], voting=\"hard\")),\n",
        "       (\"soft\", VotingClassifier(estimators=[(\"rf\", rf), (\"knn\", knn), (\"xgb\", xgb)], voting=\"soft\")),\n",
        "       (\"weighted_hard\", VotingClassifier(estimators=[(\"rf\", rf), (\"knn\", knn), (\"xgb\", xgb)], voting=\"hard\", weights=[0.3, 0.3, 0.4])),\n",
        "       (\"weighted_soft\", VotingClassifier(estimators=[(\"rf\", rf), (\"knn\", knn), (\"xgb\", xgb)], voting=\"soft\", weights=[0.4, 0.3, 0.3]))\n",
        "    ]\n",
        "\n",
        "    for voting_type, voting_clf in voting_configs:\n",
        "        print(f\"\\n=== Voting Classifier ({voting_type}) with {name} ===\")\n",
        "\n",
        "        # Fit voting classifier\n",
        "        voting_clf.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "        # Predictions\n",
        "        y_train_pred = voting_clf.predict(X_train_cfg)\n",
        "        y_test_pred = voting_clf.predict(X_test_cfg)\n",
        "\n",
        "        # Check if voting is hard or soft for probabilities\n",
        "        if 'hard' in voting_type:\n",
        "            # For hard voting, we cannot get probabilities, so set AUC-ROC to 0\n",
        "            auc_train = 0\n",
        "            auc_test = 0\n",
        "        else:\n",
        "            # For soft voting, we can get probabilities\n",
        "            y_train_proba = voting_clf.predict_proba(X_train_cfg)\n",
        "            y_test_proba = voting_clf.predict_proba(X_test_cfg)\n",
        "            auc_train = metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_proba, multi_class='ovr', average='macro')\n",
        "            auc_test = metrics.roc_auc_score(pd.get_dummies(y_test_cfg), y_test_proba, multi_class='ovr', average='macro')\n",
        "\n",
        "        # Calculate metrics\n",
        "        metrics_dict = {\n",
        "            \"Dataset\": [\"Training\", \"Test\"],\n",
        "            \"Accuracy\": [\n",
        "                metrics.accuracy_score(y_train_cfg, y_train_pred),\n",
        "                metrics.accuracy_score(y_test_cfg, y_test_pred),\n",
        "            ],\n",
        "            \"F1 Score\": [\n",
        "                metrics.f1_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "                metrics.f1_score(y_test_cfg, y_test_pred, average='macro'),\n",
        "            ],\n",
        "            \"Recall\": [\n",
        "                metrics.recall_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "                metrics.recall_score(y_test_cfg, y_test_pred, average='macro'),\n",
        "            ],\n",
        "            \"Precision\": [\n",
        "                metrics.precision_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "                metrics.precision_score(y_test_cfg, y_test_pred, average='macro'),\n",
        "            ],\n",
        "            \"AUC-ROC\": [auc_train, auc_test]\n",
        "        }\n",
        "\n",
        "        df_metrics = pd.DataFrame(metrics_dict)\n",
        "        print(f\"\\nVoting Classifier ({voting_type}) Performance Metrics\")\n",
        "        print(df_metrics.to_string(index=False))\n",
        "\n",
        "        storeResults(\n",
        "            f'Voting Classifier ({voting_type})',\n",
        "            name,\n",
        "            metrics.accuracy_score(y_test_cfg, y_test_pred),\n",
        "            metrics.f1_score(y_test_cfg, y_test_pred, average='macro'),\n",
        "            metrics.recall_score(y_test_cfg, y_test_pred, average='macro'),\n",
        "            metrics.precision_score(y_test_cfg, y_test_pred, average='macro'),\n",
        "            auc_test\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYaIECwC6YFz"
      },
      "source": [
        "# Stacking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Qc106BqeAGo",
        "outputId": "aeec3a6f-ad87-48b9-e768-ab79c902d5f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Stacking Model Performance ===\n",
            "\n",
            "Training models with PCA configuration...\n",
            "\n",
            "Stacking Classifier Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  1.000000  1.000000 1.000000    1.00000   1.0000\n",
            "    Test  0.998146  0.960632 0.955924    0.96576   0.9998\n",
            "\n",
            "Training models with LDA configuration...\n",
            "\n",
            "Stacking Classifier Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  1.000000  1.000000 1.000000   1.000000   1.0000\n",
            "    Test  0.994891  0.966255 0.961412   0.971545   0.9998\n"
          ]
        }
      ],
      "source": [
        "# Configuration list to store different data setups\n",
        "configurations = []\n",
        "\n",
        "le = LabelEncoder()\n",
        "# Fit on the union to avoid unseen class errors if test has a class not in train\n",
        "le.fit(pd.concat([y_train.astype(str), y_test.astype(str)], axis=0))\n",
        "\n",
        "y_train_enc = le.transform(y_train.astype(str))\n",
        "y_test_enc  = le.transform(y_test.astype(str))\n",
        "\n",
        "configurations.append((f'PCA', X_train_pca, X_test_pca,y_train_enc))\n",
        "configurations.append((f'LDA', X_train_lda, X_test_lda, y_train_enc))\n",
        "\n",
        "\n",
        "# Step 7: Run Stacking Classifier on different configurations\n",
        "print(\"\\n=== Stacking Model Performance ===\")\n",
        "\n",
        "# base learners\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=None,\n",
        "    min_samples_split=2,\n",
        "    min_samples_leaf=1,\n",
        "    max_features=\"sqrt\",\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "knn = KNeighborsClassifier(\n",
        "    n_neighbors=11,\n",
        "    weights=\"distance\",\n",
        "    metric=\"minkowski\",\n",
        "    p=2,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "cat = CatBoostClassifier(\n",
        "    learning_rate=0.05,\n",
        "    depth=3,\n",
        "    n_estimators=100,\n",
        "    bagging_temperature=0.05,\n",
        "    boosting_type='Plain',\n",
        "    random_seed=42,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# Train each model\n",
        "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "    print(f\"\\nTraining models with {name} configuration...\")\n",
        "\n",
        "    rf.fit(X_train_cfg, y_train_cfg)\n",
        "    knn.fit(X_train_cfg, y_train_cfg)\n",
        "    cat.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    # Define y_test_cfg for each configuration\n",
        "    y_test_cfg = y_test_enc\n",
        "    # Create stacking classifier\n",
        "    base_estimators = [\n",
        "        ('rf', rf),\n",
        "        ('knn', knn),\n",
        "        ('cat', cat)\n",
        "    ]\n",
        "\n",
        "    # Meta-learner\n",
        "    meta_learner = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "    # Stacking classifier\n",
        "    stacking_clf = StackingClassifier(\n",
        "        estimators=base_estimators,\n",
        "        final_estimator=meta_learner,\n",
        "        cv=5,  # Use 5-fold cross-validation to train meta-learner\n",
        "        stack_method='predict_proba',  # Use probabilities for meta-features\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # Fit stacking classifier\n",
        "    stacking_clf.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    # Predictions\n",
        "    y_train_pred = stacking_clf.predict(X_train_cfg)\n",
        "    y_test_pred = stacking_clf.predict(X_test_cfg)\n",
        "    y_train_proba = stacking_clf.predict_proba(X_train_cfg)\n",
        "    y_test_proba = stacking_clf.predict_proba(X_test_cfg)\n",
        "\n",
        "    # Calculate metrics\n",
        "    # Calculate metrics\n",
        "    metrics_dict = {\n",
        "            \"Dataset\": [\"Training\", \"Test\"],\n",
        "            \"Accuracy\": [\n",
        "                metrics.accuracy_score(y_train_cfg, y_train_pred),\n",
        "                metrics.accuracy_score(y_test_cfg, y_test_pred),\n",
        "            ],\n",
        "            \"F1 Score\": [\n",
        "                metrics.f1_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "                metrics.f1_score(y_test_cfg, y_test_pred, average='macro'),\n",
        "            ],\n",
        "            \"Recall\": [\n",
        "                metrics.recall_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "                metrics.recall_score(y_test_cfg, y_test_pred, average='macro'),\n",
        "            ],\n",
        "            \"Precision\": [\n",
        "                metrics.precision_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "                metrics.precision_score(y_test_cfg, y_test_pred, average='macro'),\n",
        "            ],\n",
        "            \"AUC-ROC\": [auc_train, auc_test]\n",
        "        }\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics_dict)\n",
        "    print(f\"\\nStacking Classifier Performance Metrics\")\n",
        "    print(df_metrics.to_string(index=False))\n",
        "\n",
        "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test_cfg), y_test_proba, multi_class='ovr', average='macro')\n",
        "\n",
        "    storeResults(\n",
        "           'Stacking Classifier', name,\n",
        "            metrics.accuracy_score(y_test_cfg, y_test_pred),\n",
        "            metrics.f1_score(y_test_cfg, y_test_pred, average='macro'),\n",
        "            metrics.recall_score(y_test_cfg, y_test_pred, average='macro'),\n",
        "            metrics.precision_score(y_test_cfg, y_test_pred, average='macro'),\n",
        "            auc_test\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5oHNQ_wZMbz"
      },
      "source": [
        "# Data Frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXhgVw5_nMH7",
        "outputId": "d1690664-fe7a-4e35-f712-c02a977c007b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================================================================\n",
            "MODEL PERFORMANCE RESULTS\n",
            "====================================================================================================\n",
            "                         ML Model Configuration Accuracy F1 Score  Recall Precision ROC_AUC\n",
            "           Support Vector Machine           PCA  98.709%  77.373% 78.234%   76.586% 99.830%\n",
            "           Support Vector Machine           LDA  95.570%  73.736% 74.755%   72.833% 99.413%\n",
            "                    Random Forest           PCA  99.823%  96.674% 95.635%   97.898% 99.990%\n",
            "                    Random Forest           LDA  99.455%  96.768% 96.105%   97.512% 99.748%\n",
            "                              KNN           PCA  99.737%  97.078% 96.441%   97.798% 99.488%\n",
            "                              KNN           LDA  99.368%  95.882% 95.168%   96.724% 99.430%\n",
            "                Gradient Boosting           PCA  99.064%  87.878% 86.355%   90.804% 98.000%\n",
            "                Gradient Boosting           LDA  96.642%  84.033% 82.327%   88.044% 95.818%\n",
            "                      AdaBoosting           PCA  89.049%  61.928% 59.247%   66.140% 97.047%\n",
            "                      AdaBoosting           LDA  90.282%  64.088% 62.880%   65.594% 97.868%\n",
            "                       XGBoosting           PCA  99.789%  97.017% 96.922%   97.118% 99.990%\n",
            "                       XGBoosting           LDA  98.489%  90.579% 88.927%   93.211% 99.932%\n",
            "                      CatBoosting           PCA  94.995%  71.512% 71.425%   71.622% 99.390%\n",
            "                      CatBoosting           LDA  93.167%  70.476% 71.655%   69.560% 99.274%\n",
            "               Bagging Classifier           PCA  94.676%  70.915% 72.015%   70.201% 99.198%\n",
            "               Bagging Classifier           LDA  93.632%  71.666% 73.442%   70.235% 99.193%\n",
            "         Voting Classifier (hard)           PCA  99.817%  97.231% 96.532%   98.011%  0.000%\n",
            "         Voting Classifier (soft)           PCA  99.808%  97.199% 96.522%   97.957% 99.993%\n",
            "Voting Classifier (weighted_hard)           PCA  99.823%  97.239% 96.545%   98.014%  0.000%\n",
            "Voting Classifier (weighted_soft)           PCA  99.815%  96.668% 95.621%   97.901% 99.993%\n",
            "         Voting Classifier (hard)           LDA  99.420%  96.182% 95.203%   97.348%  0.000%\n",
            "         Voting Classifier (soft)           LDA  99.401%  96.136% 95.181%   97.279% 99.980%\n",
            "Voting Classifier (weighted_hard)           LDA  99.416%  96.174% 95.200%   97.336%  0.000%\n",
            "Voting Classifier (weighted_soft)           LDA  99.409%  96.159% 95.198%   97.306% 99.980%\n",
            "              Stacking Classifier           PCA  99.815%  96.063% 95.592%   96.576% 99.980%\n",
            "              Stacking Classifier           LDA  99.489%  96.626% 96.141%   97.154% 99.980%\n",
            "\n",
            "Results saved to model_results.csv\n",
            "\n",
            "====================================================================================================\n",
            "SORTED MODEL PERFORMANCE RESULTS (by Accuracy and F1 Score)\n",
            "====================================================================================================\n",
            "                         ML Model Configuration Accuracy F1 Score  Recall Precision ROC_AUC\n",
            "Voting Classifier (weighted_hard)           PCA  99.823%  97.239% 96.545%   98.014%  0.000%\n",
            "                    Random Forest           PCA  99.823%  96.674% 95.635%   97.898% 99.990%\n",
            "         Voting Classifier (hard)           PCA  99.817%  97.231% 96.532%   98.011%  0.000%\n",
            "Voting Classifier (weighted_soft)           PCA  99.815%  96.668% 95.621%   97.901% 99.993%\n",
            "              Stacking Classifier           PCA  99.815%  96.063% 95.592%   96.576% 99.980%\n",
            "         Voting Classifier (soft)           PCA  99.808%  97.199% 96.522%   97.957% 99.993%\n",
            "                       XGBoosting           PCA  99.789%  97.017% 96.922%   97.118% 99.990%\n",
            "                              KNN           PCA  99.737%  97.078% 96.441%   97.798% 99.488%\n",
            "              Stacking Classifier           LDA  99.489%  96.626% 96.141%   97.154% 99.980%\n",
            "                    Random Forest           LDA  99.455%  96.768% 96.105%   97.512% 99.748%\n",
            "         Voting Classifier (hard)           LDA  99.420%  96.182% 95.203%   97.348%  0.000%\n",
            "Voting Classifier (weighted_hard)           LDA  99.416%  96.174% 95.200%   97.336%  0.000%\n",
            "Voting Classifier (weighted_soft)           LDA  99.409%  96.159% 95.198%   97.306% 99.980%\n",
            "         Voting Classifier (soft)           LDA  99.401%  96.136% 95.181%   97.279% 99.980%\n",
            "                              KNN           LDA  99.368%  95.882% 95.168%   96.724% 99.430%\n",
            "                Gradient Boosting           PCA  99.064%  87.878% 86.355%   90.804% 98.000%\n",
            "           Support Vector Machine           PCA  98.709%  77.373% 78.234%   76.586% 99.830%\n",
            "                       XGBoosting           LDA  98.489%  90.579% 88.927%   93.211% 99.932%\n",
            "                Gradient Boosting           LDA  96.642%  84.033% 82.327%   88.044% 95.818%\n",
            "           Support Vector Machine           LDA  95.570%  73.736% 74.755%   72.833% 99.413%\n",
            "                      CatBoosting           PCA  94.995%  71.512% 71.425%   71.622% 99.390%\n",
            "               Bagging Classifier           PCA  94.676%  70.915% 72.015%   70.201% 99.198%\n",
            "               Bagging Classifier           LDA  93.632%  71.666% 73.442%   70.235% 99.193%\n",
            "                      CatBoosting           LDA  93.167%  70.476% 71.655%   69.560% 99.274%\n",
            "                      AdaBoosting           LDA  90.282%  64.088% 62.880%   65.594% 97.868%\n",
            "                      AdaBoosting           PCA  89.049%  61.928% 59.247%   66.140% 97.047%\n",
            "\n",
            "Sorted results saved to sorted_model_results.csv\n",
            "\n",
            "====================================================================================================\n",
            "TOP CONFIGURATION PER MODEL\n",
            "====================================================================================================\n",
            "                         ML Model Configuration Accuracy F1 Score  Recall Precision ROC_AUC\n",
            "                      AdaBoosting           LDA  90.282%  64.088% 62.880%   65.594% 97.868%\n",
            "               Bagging Classifier           PCA  94.676%  70.915% 72.015%   70.201% 99.198%\n",
            "                      CatBoosting           PCA  94.995%  71.512% 71.425%   71.622% 99.390%\n",
            "                Gradient Boosting           PCA  99.064%  87.878% 86.355%   90.804% 98.000%\n",
            "                              KNN           PCA  99.737%  97.078% 96.441%   97.798% 99.488%\n",
            "                    Random Forest           PCA  99.823%  96.674% 95.635%   97.898% 99.990%\n",
            "              Stacking Classifier           PCA  99.815%  96.063% 95.592%   96.576% 99.980%\n",
            "           Support Vector Machine           PCA  98.709%  77.373% 78.234%   76.586% 99.830%\n",
            "         Voting Classifier (hard)           PCA  99.817%  97.231% 96.532%   98.011%  0.000%\n",
            "         Voting Classifier (soft)           PCA  99.808%  97.199% 96.522%   97.957% 99.993%\n",
            "Voting Classifier (weighted_hard)           PCA  99.823%  97.239% 96.545%   98.014%  0.000%\n",
            "Voting Classifier (weighted_soft)           PCA  99.815%  96.668% 95.621%   97.901% 99.993%\n",
            "                       XGBoosting           PCA  99.789%  97.017% 96.922%   97.118% 99.990%\n",
            "\n",
            "Top configuration per model saved to top_configurations.csv\n"
          ]
        }
      ],
      "source": [
        "# Creating the dataframe\n",
        "\n",
        "# Make sure output folder exists\n",
        "out_dir = Path(\"final_results\")\n",
        "out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "result = pd.DataFrame({\n",
        "    'ML Model': ML_Model,\n",
        "    'Configuration': ML_Config,\n",
        "    'Accuracy': [f\"{acc * 100:.3f}%\" for acc in accuracy],\n",
        "    'F1 Score': [f\"{f1 * 100:.3f}%\" for f1 in f1_score],\n",
        "    'Recall': [f\"{rec * 100:.3f}%\" for rec in recall],\n",
        "    'Precision': [f\"{prec * 100:.3f}%\" for prec in precision],\n",
        "    'ROC_AUC': [f\"{roc * 100:.3f}%\" for roc in auc_roc],\n",
        "})\n",
        "\n",
        "# Remove duplicates based on model and configuration\n",
        "result.drop_duplicates(subset=[\"ML Model\", \"Configuration\"], inplace=True)\n",
        "\n",
        "# Display the result\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"MODEL PERFORMANCE RESULTS\")\n",
        "print(\"=\" * 100)\n",
        "print(result.to_string(index=False))\n",
        "\n",
        "# Save the result to a CSV file\n",
        "result.to_csv('final_results/model_results.csv', index=False)\n",
        "print(\"\\nResults saved to model_results.csv\")\n",
        "\n",
        "# Sort by Accuracy and F1 Score\n",
        "sorted_result = result.sort_values(by=['Accuracy', 'F1 Score'], ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Display the sorted result\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"SORTED MODEL PERFORMANCE RESULTS (by Accuracy and F1 Score)\")\n",
        "print(\"=\" * 100)\n",
        "print(sorted_result.to_string(index=False))\n",
        "\n",
        "# Save the sorted result\n",
        "sorted_result.to_csv('final_results/sorted_model_results.csv', index=False)\n",
        "print(\"\\nSorted results saved to sorted_model_results.csv\")\n",
        "\n",
        "# Extract top configuration per ML model\n",
        "top_per_model = sorted_result.groupby('ML Model', as_index=False).first()\n",
        "\n",
        "# Display and save the top configuration table\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"TOP CONFIGURATION PER MODEL\")\n",
        "print(\"=\" * 100)\n",
        "print(top_per_model.to_string(index=False))\n",
        "\n",
        "top_per_model.to_csv('final_results/top_configurations.csv', index=False)\n",
        "print(\"\\nTop configuration per model saved to top_configurations.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
