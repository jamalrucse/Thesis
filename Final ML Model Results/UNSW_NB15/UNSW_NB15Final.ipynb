{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEXLxeOtIqKY"
      },
      "source": [
        "# **Interactive 3D Visualization Framework For Machine Learning Based Network Intrusion Detection Systems**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bh-Cs1HMIGlm"
      },
      "source": [
        "# Libraries\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Iu70wj01szV",
        "outputId": "de5f1eca-475a-47fb-e972-e90e77da7d43"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "from math import log2\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n",
        "from sklearn.compose import ColumnTransformer, make_column_selector as selector\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, RFECV, RFE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier,AdaBoostClassifier, BaggingClassifier,VotingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "\n",
        "# Enable inline plotting for Jupyter notebooks\n",
        "%matplotlib inline\n",
        "\n",
        "# Filter warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rmt8sv_Jfpv"
      },
      "source": [
        "# Dataset Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qE0v7OZv18Wv"
      },
      "outputs": [],
      "source": [
        "#Loading data into dataframe\n",
        "data1 = pd.read_csv('UNSW_NB15_Train.csv')\n",
        "data2 = pd.read_csv('UNSW_NB15_Test.csv')\n",
        "\n",
        "# Define target variable\n",
        "target_col = 'label'\n",
        "X_train = data1.drop(columns=[target_col])\n",
        "y_train = data1[target_col]\n",
        "\n",
        "# Separate features and target for test\n",
        "X_test = data2.drop(columns=[target_col])\n",
        "y_test = data2[target_col]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0qVBosRJomj"
      },
      "source": [
        "# Pipeline Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BJFU6besq3v",
        "outputId": "6ea8a9bc-5922-438f-8da9-a899a94b33dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After One Hot shapes: (82332, 201) (175341, 201)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# step 1.1: Safe fallback to keep this cell runnable\n",
        "_need_fallback = False\n",
        "try:\n",
        "    X_train\n",
        "    X_test\n",
        "    y_train\n",
        "    y_test\n",
        "except NameError:\n",
        "    _need_fallback = True\n",
        "\n",
        "if _need_fallback:\n",
        "    rng = np.random.RandomState(42)\n",
        "    n = 300\n",
        "    df_all = pd.DataFrame({\n",
        "        \"num1\": rng.randn(n),\n",
        "        \"num2\": rng.rand(n) * 5,\n",
        "        \"cat1\": rng.choice([\"tcp\", \"udp\", \"icmp\"], size=n),\n",
        "        \"cat2\": rng.choice([\"low\", \"med\", \"high\"], size=n),\n",
        "    })\n",
        "    y_all = ((df_all[\"num1\"] + 0.8 * df_all[\"num2\"] + (df_all[\"cat1\"] == \"tcp\").astype(int) + (df_all[\"cat2\"] == \"high\").astype(int) + rng.randn(n)*0.5) > 3).astype(int)\n",
        "    idx = np.arange(n)\n",
        "    rng.shuffle(idx)\n",
        "    cut = int(n*0.75)\n",
        "    tr, te = idx[:cut], idx[cut:]\n",
        "    X_train = df_all.iloc[tr].reset_index(drop=True)\n",
        "    X_test  = df_all.iloc[te].reset_index(drop=True)\n",
        "    y_train = y_all.iloc[tr].reset_index(drop=True)\n",
        "    y_test  = y_all.iloc[te].reset_index(drop=True)\n",
        "\n",
        "if not isinstance(X_train, pd.DataFrame):\n",
        "    X_train = pd.DataFrame(X_train).copy()\n",
        "if not isinstance(X_test, pd.DataFrame):\n",
        "    X_test = pd.DataFrame(X_test).copy()\n",
        "\n",
        "\n",
        "\n",
        "# step 1.2 Identify column types\n",
        "cat_cols = selector(dtype_include=[\"category\", \"object\"])(X_train)\n",
        "num_cols = selector(dtype_include=np.number)(X_train)\n",
        "\n",
        "# step 1.3 One Hot on categoricals and MinMax on numerics, fit on train, transform both\n",
        "cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
        "num_cols = X_train.select_dtypes(exclude=[\"object\", \"category\"]).columns.tolist()\n",
        "\n",
        "ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
        "\n",
        "X_train_cat = ohe.fit_transform(X_train[cat_cols]) if len(cat_cols) > 0 else np.empty((len(X_train), 0))\n",
        "X_test_cat  = ohe.transform(X_test[cat_cols])      if len(cat_cols) > 0 else np.empty((len(X_test), 0))\n",
        "\n",
        "X_train_num = X_train[num_cols].to_numpy(dtype=float) if len(num_cols) > 0 else np.empty((len(X_train), 0))\n",
        "X_test_num  = X_test[num_cols].to_numpy(dtype=float)  if len(num_cols) > 0 else np.empty((len(X_test), 0))\n",
        "\n",
        "X_train_oh = np.hstack([X_train_cat, X_train_num])\n",
        "X_test_oh  = np.hstack([X_test_cat,  X_test_num])\n",
        "\n",
        "print(\"After One Hot shapes:\", X_train_oh.shape, X_test_oh.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_0tqOiHnj9y",
        "outputId": "8a58ffb8-1866-48b8-be17-96b93f220669"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After Normalization shapes: (82332, 201) (175341, 201)\n"
          ]
        }
      ],
      "source": [
        "# ================================\n",
        "# STEP 2: Normalization with MinMaxScaler\n",
        "# ================================\n",
        "scaler_minmax = MinMaxScaler()\n",
        "X_train_normalized = scaler_minmax.fit_transform(X_train_oh)\n",
        "X_test_normalized  = scaler_minmax.transform(X_test_oh)\n",
        "\n",
        "print(\"After Normalization shapes:\", X_train_normalized.shape, X_test_normalized.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bs2tHbK9nqMS"
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "# STEP 3.1: Training set information gain calculation\n",
        "# ================================\n",
        "mi = mutual_info_classif(X_train_normalized, np.asarray(y_train), random_state=42, discrete_features=False)\n",
        "\n",
        "# 3.2: Convert scores to weights in [0,1], avoid exact zeros\n",
        "mi_max = np.max(mi) if np.max(mi) > 0 else 1.0\n",
        "weights = mi / mi_max\n",
        "weights = np.where(weights == 0, 1e-6, weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQV7xhoenzEL"
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "# STEP 4: Weighted Transform each feature\n",
        "# ================================\n",
        "X_train_weighted = X_train_normalized * weights\n",
        "X_test_weighted  = X_test_normalized  * weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3dMNgnIoCWc"
      },
      "outputs": [],
      "source": [
        "# STEP 5: Transform features to zero mean\n",
        "# ================================\n",
        "train_means = X_train_weighted.mean(axis=0)\n",
        "X_train_centered = X_train_weighted - train_means\n",
        "X_test_centered  = X_test_weighted  - train_means"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiZtTQ2CBXQC",
        "outputId": "ca736140-4da9-4376-9450-d10bbfa08370"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== PCA output ===\n",
            "Number of components that explain 99.0% variance: 15\n",
            "\n",
            "=== LDA  output ===\n",
            "LDA components used: 1\n"
          ]
        }
      ],
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "# 6.1: PCA on SFS output to explain at least 95 percent variance\n",
        "print(\"\\n=== PCA output ===\")\n",
        "pca_probe = PCA().fit(X_train_centered)\n",
        "cum_var = np.cumsum(pca_probe.explained_variance_ratio_)\n",
        "n_components = int(np.argmax(cum_var >= 0.99) + 1)\n",
        "pca = PCA(n_components=max(1, n_components)).fit(X_train_centered)\n",
        "X_train_pca = pca.transform(X_train_centered)\n",
        "X_test_pca  = pca.transform(X_test_centered)\n",
        "print(f'Number of components that explain 99.0% variance: {pca.n_components_}')\n",
        "\n",
        "\n",
        "# 6.2: LDA output\n",
        "\n",
        "if len(np.unique(y_train)) > 1:\n",
        "    print(\"\\n=== LDA  output ===\")\n",
        "    n_classes = len(np.unique(y_train))\n",
        "    n_comp_lda = min(max(1, n_classes - 1), X_train_centered.shape[1])\n",
        "    lda = LDA(n_components=n_comp_lda).fit(X_train_pca, y_train)\n",
        "    X_train_lda = lda.transform(X_train_pca)\n",
        "    X_test_lda  = lda.transform(X_test_pca)\n",
        "    print(f\"LDA components used: {n_comp_lda}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SB6_xOy4Lxmk"
      },
      "source": [
        "# ML Model Results Storage Framework"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fODUcnRmKkZ0",
        "outputId": "70384b48-d948-44e3-89de-71b3f4d15a97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model results storage framework loaded successfully!\n",
            "Available functions:\n",
            "- storeResults(model, config, accuracy, f1, recall, precision, auc_roc)\n",
            "- displayAndSaveResults(filename_prefix='model_results')\n",
            "- clearResults()\n",
            "- plotModelComparison(result_df)\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# ML MODEL RESULTS STORAGE FRAMEWORK\n",
        "# =============================================================================\n",
        "\n",
        "# Creating holders to store the model performance results\n",
        "ML_Model = []\n",
        "ML_Config = []\n",
        "accuracy = []\n",
        "f1_score = []\n",
        "recall = []\n",
        "precision = []\n",
        "auc_roc = []  # Adding a holder for AUC-ROC\n",
        "\n",
        "# Function to call for storing the results\n",
        "def storeResults(model, config, a, b, c, d, e):\n",
        "    \"\"\"\n",
        "    Store model performance results\n",
        "\n",
        "    Parameters:\n",
        "    model: Name of the ML model\n",
        "    config: Configuration name (preprocessing steps applied)\n",
        "    a: Accuracy score\n",
        "    b: F1 score\n",
        "    c: Recall score\n",
        "    d: Precision score\n",
        "    e: AUC-ROC score\n",
        "    \"\"\"\n",
        "    ML_Model.append(model)\n",
        "    ML_Config.append(config)\n",
        "    accuracy.append(round(a, 6))\n",
        "    f1_score.append(round(b, 6))\n",
        "    recall.append(round(c, 6))\n",
        "    precision.append(round(d, 6))\n",
        "    auc_roc.append(round(e, 6))\n",
        "\n",
        "# Function to display and save results\n",
        "def displayAndSaveResults(filename_prefix='model_results'):\n",
        "    \"\"\"\n",
        "    Create dataframe from results, display, and save to CSV\n",
        "\n",
        "    Parameters:\n",
        "    filename_prefix: Prefix for the CSV filenames\n",
        "    \"\"\"\n",
        "    # Creating the dataframe\n",
        "    result = pd.DataFrame({\n",
        "        'ML Model': ML_Model,\n",
        "        'Configuration': ML_Config,\n",
        "        'Accuracy': [f\"{acc * 100:.3f}%\" for acc in accuracy],\n",
        "        'F1 Score': [f\"{f1 * 100:.3f}%\" for f1 in f1_score],\n",
        "        'Recall': [f\"{rec * 100:.3f}%\" for rec in recall],\n",
        "        'Precision': [f\"{prec * 100:.3f}%\" for prec in precision],\n",
        "        'ROC_AUC': [f\"{roc * 100:.3f}%\" for roc in auc_roc],\n",
        "    })\n",
        "\n",
        "    # Remove duplicates if any\n",
        "    result.drop_duplicates(subset=[\"ML Model\", \"Configuration\"], inplace=True)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(\"MODEL PERFORMANCE RESULTS\")\n",
        "    print(\"=\"*100)\n",
        "    print(result.to_string(index=False))\n",
        "\n",
        "    # Saving the result to a CSV file\n",
        "    result.to_csv(f'{filename_prefix}.csv', index=False)\n",
        "    print(f\"\\nResults saved to {filename_prefix}.csv\")\n",
        "\n",
        "    # Sorting the dataframe on accuracy and F1 Score\n",
        "    sorted_result = result.sort_values(by=['Accuracy', 'F1 Score'], ascending=False).reset_index(drop=True)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(\"SORTED MODEL PERFORMANCE RESULTS (by Accuracy and F1 Score)\")\n",
        "    print(\"=\"*100)\n",
        "    print(sorted_result.to_string(index=False))\n",
        "\n",
        "    # Saving the sorted result to a CSV file\n",
        "    sorted_result.to_csv(f'sorted_{filename_prefix}.csv', index=False)\n",
        "    print(f\"\\nSorted results saved to sorted_{filename_prefix}.csv\")\n",
        "\n",
        "    return result, sorted_result\n",
        "\n",
        "# Function to clear results (useful when running multiple experiments)\n",
        "def clearResults():\n",
        "    \"\"\"Clear all stored results\"\"\"\n",
        "    global ML_Model, ML_Config, accuracy, f1_score, recall, precision, auc_roc\n",
        "    ML_Model.clear()\n",
        "    ML_Config.clear()\n",
        "    accuracy.clear()\n",
        "    f1_score.clear()\n",
        "    recall.clear()\n",
        "    precision.clear()\n",
        "    auc_roc.clear()\n",
        "    print(\"Results cleared!\")\n",
        "\n",
        "# Function to plot model comparison\n",
        "def plotModelComparison(result_df):\n",
        "    \"\"\"\n",
        "    Create visualization comparing model performances\n",
        "\n",
        "    Parameters:\n",
        "    result_df: DataFrame with model results\n",
        "    \"\"\"\n",
        "    # Convert percentage strings back to floats for plotting\n",
        "    metrics_cols = ['Accuracy', 'F1 Score', 'Recall', 'Precision', 'ROC_AUC']\n",
        "    plot_df = result_df.copy()\n",
        "\n",
        "    for col in metrics_cols:\n",
        "        plot_df[col] = plot_df[col].str.rstrip('%').astype(float)\n",
        "\n",
        "    # Create subplot for each metric\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "    axes = axes.ravel()\n",
        "\n",
        "    for idx, metric in enumerate(metrics_cols):\n",
        "        # Group by model and get mean performance across configurations\n",
        "        model_performance = plot_df.groupby('ML Model')[metric].mean().sort_values(ascending=False)\n",
        "\n",
        "        # Create bar plot\n",
        "        ax = axes[idx]\n",
        "        bars = ax.bar(range(len(model_performance)), model_performance.values,\n",
        "                      color=plt.cm.Blues(np.linspace(0.4, 0.9, len(model_performance))))\n",
        "        ax.set_xticks(range(len(model_performance)))\n",
        "        ax.set_xticklabels(model_performance.index, rotation=45, ha='right')\n",
        "        ax.set_ylabel(f'{metric} (%)')\n",
        "        ax.set_title(f'Average {metric} by Model', fontweight='bold')\n",
        "        ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "        # Add value labels on bars\n",
        "        for bar in bars:\n",
        "            height = bar.get_height()\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                   f'{height:.1f}%', ha='center', va='bottom')\n",
        "\n",
        "    # Hide the last subplot if we have 5 metrics\n",
        "    if len(metrics_cols) == 5:\n",
        "        axes[5].set_visible(False)\n",
        "\n",
        "    plt.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "print(\"Model results storage framework loaded successfully!\")\n",
        "print(\"Available functions:\")\n",
        "print(\"- storeResults(model, config, accuracy, f1, recall, precision, auc_roc)\")\n",
        "print(\"- displayAndSaveResults(filename_prefix='model_results')\")\n",
        "print(\"- clearResults()\")\n",
        "print(\"- plotModelComparison(result_df)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHwhvio-qU3A"
      },
      "source": [
        "# SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiHesVrJO0mI",
        "outputId": "12606681-d568-43cd-efd9-316d6f01319b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== SVM Model Performance  ===\n",
            "\n",
            "Running SVM with PCA configuration...\n",
            "\n",
            "SVM Model Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training   1.00000  1.000000 1.000000   1.000000      1.0\n",
            "    Test   0.99424  0.993407 0.995768   0.991142      1.0\n",
            "\n",
            "Running SVM with LDA configuration...\n",
            "\n",
            "SVM Model Performance Metrics\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training       1.0       1.0     1.0        1.0      1.0\n",
            "    Test       1.0       1.0     1.0        1.0      1.0\n"
          ]
        }
      ],
      "source": [
        "# Configuration list to store different data setups\n",
        "configurations = []\n",
        "\n",
        "configurations.append((f'PCA', X_train_pca, X_test_pca, y_train))\n",
        "configurations.append((f'LDA', X_train_lda, X_test_lda, y_train))\n",
        "\n",
        "# Step 7: Run SVM  on different configurations\n",
        "print(\"\\n=== SVM Model Performance  ===\")\n",
        "svm_results = {}\n",
        "svm = SVC(\n",
        "    kernel=\"rbf\",\n",
        "    C=1.0,\n",
        "    gamma=\"scale\",\n",
        "    probability=True,\n",
        "    random_state=42\n",
        "    )\n",
        "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "    print(f\"\\nRunning SVM with {name} configuration...\")\n",
        "\n",
        "    svm.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    y_train_pred = svm.predict(X_train_cfg)\n",
        "    y_test_pred  = svm.predict(X_test_cfg)\n",
        "\n",
        "    y_train_proba = svm.predict_proba(X_train_cfg)\n",
        "    y_test_proba  = svm.predict_proba(X_test_cfg)\n",
        "\n",
        "    # cache what we need for confusion matrices\n",
        "    svm_results[name] = {\n",
        "        \"y_test_pred\": y_test_pred,\n",
        "        \"y_train_pred\": y_train_pred,\n",
        "        \"y_test_proba\": y_test_proba,\n",
        "        \"y_train_proba\": y_train_proba,\n",
        "        \"classes\": svm.classes_,\n",
        "    }\n",
        "    metrics_dict = {\n",
        "        \"Dataset\": [\"Training\", \"Test\"],\n",
        "        \"Accuracy\": [\n",
        "            metrics.accuracy_score(y_train_cfg, y_train_pred),\n",
        "            metrics.accuracy_score(y_test,      y_test_pred),\n",
        "        ],\n",
        "        \"F1 Score\": [\n",
        "            metrics.f1_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.f1_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"Recall\": [\n",
        "            metrics.recall_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.recall_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"Precision\": [\n",
        "            metrics.precision_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.precision_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"AUC-ROC\": [\n",
        "            metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_proba, multi_class='ovr', average='macro'),\n",
        "            metrics.roc_auc_score(pd.get_dummies(y_test),      y_test_proba,  multi_class='ovr', average='macro'),\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics_dict)\n",
        "    print(\"\\nSVM Model Performance Metrics\")\n",
        "    print(df_metrics.to_string(index=False))\n",
        "\n",
        "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_proba, multi_class='ovr', average='macro')\n",
        "    storeResults('Support Vector Machine', name,\n",
        "                 metrics.accuracy_score(y_test, y_test_pred),\n",
        "                 metrics.f1_score(y_test, y_test_pred, average='macro'),\n",
        "                 metrics.recall_score(y_test, y_test_pred, average='macro'),\n",
        "                 metrics.precision_score(y_test, y_test_pred, average='macro'),\n",
        "                 auc_score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSUpXhuBEBOL",
        "outputId": "1dc36d13-0f17-406a-e187-164531ab2fbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Confusion Matrix: PCA ===\n",
            "\n",
            "Table 5.1\n",
            "Multi category classification confusion matrix for the NSL KDD dataset.\n",
            "\n",
            "Predicted\n",
            "\n",
            "                     Normal Abnormal Recall (%)\n",
            "       Normal         56000        0      100.0\n",
            "Actual Abnormal        1010   118331       99.2\n",
            "       Precision (%)   98.2    100.0           \n",
            "Saved to UNSW_NB15_confusion_table_SVM_PCA.csv\n",
            "\n",
            "=== Confusion Matrix: LDA ===\n",
            "\n",
            "Table 5.1\n",
            "Multi category classification confusion matrix for the NSL KDD dataset.\n",
            "\n",
            "Predicted\n",
            "\n",
            "                     Normal Abnormal Recall (%)\n",
            "       Normal         56000        0      100.0\n",
            "Actual Abnormal           0   119341      100.0\n",
            "       Precision (%)  100.0    100.0           \n",
            "Saved to UNSW_NB15_confusion_table_SVM_LDA.csv\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def nslkdd_confusion_table(y_true, y_pred, class_order=None, title_no = 5.1):\n",
        "    if class_order is None:\n",
        "        classes = list(pd.unique(pd.Series(list(y_true) + list(y_pred))))\n",
        "    else:\n",
        "        present = set(pd.unique(pd.Series(list(y_true) + list(y_pred))))\n",
        "        classes = [c for c in class_order if c in present] or list(present)\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
        "    df = pd.DataFrame(cm, index=[f\"Actual  {c}\" for c in classes], columns=classes)\n",
        "\n",
        "    # --- map numeric 0/1 to desired display names (Normal/Abnormal) ---\n",
        "    if set(classes) == {0, 1}:\n",
        "        disp = list(class_order) if (class_order and len(class_order) == 2) else [\"Normal\", \"Abnormal\"]\n",
        "        name_map = {0: disp[0], 1: disp[1]}\n",
        "        df.columns = [name_map.get(c, c) for c in df.columns]\n",
        "        df.index   = [f\"Actual  {name_map.get(c, c)}\" for c in classes]\n",
        "    # -------------------------------------------------------------------\n",
        "\n",
        "    row_tot = cm.sum(axis=1)\n",
        "    recalls = np.divide(np.diag(cm), row_tot, out=np.zeros_like(row_tot, dtype=float), where=row_tot != 0) * 100.0\n",
        "    df[\"Recall (%)\"] = np.round(recalls, 1)\n",
        "\n",
        "    col_tot = cm.sum(axis=0)\n",
        "    precisions = np.divide(np.diag(cm), col_tot, out=np.zeros_like(col_tot, dtype=float), where=col_tot != 0) * 100.0\n",
        "\n",
        "    display_cols = list(df.columns)[:len(classes)]\n",
        "    prec_row = pd.Series(np.round(precisions, 1), index=display_cols, name=\"Precision (%)\")\n",
        "    prec_row[\"Recall (%)\"] = \"\"\n",
        "    df = pd.concat([df, prec_row.to_frame().T], axis=0)\n",
        "\n",
        "    # ===== Make \"Actual\" appear ONCE and centered on the class rows =====\n",
        "    n_class_rows = len(classes)              # rows before the final Precision row\n",
        "    mid = n_class_rows // 2                  # middle row index\n",
        "    # strip the leading \"Actual  \" from current row labels for display\n",
        "    class_names = [idx.replace(\"Actual  \", \"\", 1) for idx in df.index[:n_class_rows]]\n",
        "\n",
        "    new_index = []\n",
        "    for i, name in enumerate(class_names):\n",
        "        left = \"Actual\" if i == mid else \"\"\n",
        "        new_index.append((left, name))\n",
        "    new_index.append((\"\", \"Precision (%)\"))  # last row\n",
        "    df.index = pd.MultiIndex.from_tuples(new_index)\n",
        "    # ====================================================================\n",
        "\n",
        "    print(f\"\\nTable {title_no}\")\n",
        "    print(\"Multi category classification confusion matrix for the NSL KDD dataset.\\n\")\n",
        "    print(\"Predicted\\n\")\n",
        "    print(df.to_string())\n",
        "    return df\n",
        "\n",
        "# usage (unchanged)\n",
        "nsl_order = [\"Normal\", \"Abnormal\"]\n",
        "for name, out in svm_results.items():\n",
        "    print(f\"\\n=== Confusion Matrix: {name} ===\")\n",
        "    tbl = nslkdd_confusion_table(y_test, out[\"y_test_pred\"], class_order=nsl_order, title_no=5.1)\n",
        "    csv_path = f\"UNSW_NB15_confusion_table_SVM_{name}.csv\"\n",
        "    tbl.to_csv(csv_path, index=True)\n",
        "    print(f\"Saved to {csv_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfra_hFKaulE"
      },
      "source": [
        "# Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvIm4D6eQZ50",
        "outputId": "b08998b5-42b3-4d2c-a080-7c1afdd7dba6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Random Forestr Model Performance ===\n",
            "\n",
            "Running Random Forest with PCA configuration...\n",
            "\n",
            "RF Model Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  1.000000  1.000000 1.000000   1.000000      1.0\n",
            "    Test  0.992523  0.991454 0.994507   0.988562      1.0\n",
            "\n",
            "Running Random Forest with LDA configuration...\n",
            "\n",
            "RF Model Performance Metrics\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training       1.0       1.0     1.0        1.0      1.0\n",
            "    Test       1.0       1.0     1.0        1.0      1.0\n"
          ]
        }
      ],
      "source": [
        "# Configuration list to store different data setups\n",
        "configurations = []\n",
        "\n",
        "configurations.append((f'PCA', X_train_pca, X_test_pca, y_train))\n",
        "configurations.append((f'LDA', X_train_lda, X_test_lda, y_train))\n",
        "\n",
        "# Step 7: Run RF different configurations\n",
        "print(\"\\n=== Random Forestr Model Performance ===\")\n",
        "rf_results = {}\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=None,\n",
        "    min_samples_split=2,\n",
        "    min_samples_leaf=1,\n",
        "    max_features=\"sqrt\",\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "    print(f\"\\nRunning Random Forest with {name} configuration...\")\n",
        "\n",
        "    rf.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    y_train_pred = rf.predict(X_train_cfg)\n",
        "    y_test_pred  = rf.predict(X_test_cfg)\n",
        "\n",
        "    y_train_proba = rf.predict_proba(X_train_cfg)\n",
        "    y_test_proba  = rf.predict_proba(X_test_cfg)\n",
        "\n",
        "    # cache what we need for confusion matrices (and more if you want)\n",
        "    rf_results[name] = {\n",
        "        \"y_test_pred\": y_test_pred,\n",
        "        \"y_train_pred\": y_train_pred,\n",
        "        \"y_test_proba\": y_test_proba,\n",
        "        \"y_train_proba\": y_train_proba,\n",
        "        \"classes\": rf.classes_,\n",
        "    }\n",
        "    metrics_dict = {\n",
        "        \"Dataset\": [\"Training\", \"Test\"],\n",
        "        \"Accuracy\": [\n",
        "            metrics.accuracy_score(y_train_cfg, y_train_pred),\n",
        "            metrics.accuracy_score(y_test,      y_test_pred),\n",
        "        ],\n",
        "        \"F1 Score\": [\n",
        "            metrics.f1_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.f1_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"Recall\": [\n",
        "            metrics.recall_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.recall_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"Precision\": [\n",
        "            metrics.precision_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.precision_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"AUC-ROC\": [\n",
        "            metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_proba, multi_class='ovr', average='macro'),\n",
        "            metrics.roc_auc_score(pd.get_dummies(y_test),      y_test_proba,  multi_class='ovr', average='macro'),\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics_dict)\n",
        "    print(\"\\nRF Model Performance Metrics\")\n",
        "    print(df_metrics.to_string(index=False))\n",
        "\n",
        "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_proba, multi_class='ovr', average='macro')\n",
        "    storeResults('Random Forest', name,\n",
        "                 metrics.accuracy_score(y_test, y_test_pred),\n",
        "                 metrics.f1_score(y_test, y_test_pred, average='macro'),\n",
        "                 metrics.recall_score(y_test, y_test_pred, average='macro'),\n",
        "                 metrics.precision_score(y_test, y_test_pred, average='macro'),\n",
        "                 auc_score)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdS4ifU0A_9W",
        "outputId": "e7036110-6105-41dd-a443-372008ae4489"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Confusion Matrix: PCA ===\n",
            "\n",
            "Table 5.1\n",
            "Multi category classification confusion matrix for the NSL KDD dataset.\n",
            "\n",
            "Predicted\n",
            "\n",
            "                     Normal Abnormal Recall (%)\n",
            "       Normal         56000        0      100.0\n",
            "Actual Abnormal        1311   118030       98.9\n",
            "       Precision (%)   97.7    100.0           \n",
            "Saved to UNSW_NB15_confusion_table_RF_PCA.csv\n",
            "\n",
            "=== Confusion Matrix: LDA ===\n",
            "\n",
            "Table 5.1\n",
            "Multi category classification confusion matrix for the NSL KDD dataset.\n",
            "\n",
            "Predicted\n",
            "\n",
            "                     Normal Abnormal Recall (%)\n",
            "       Normal         56000        0      100.0\n",
            "Actual Abnormal           0   119341      100.0\n",
            "       Precision (%)  100.0    100.0           \n",
            "Saved to UNSW_NB15_confusion_table_RF_LDA.csv\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def nslkdd_confusion_table(y_true, y_pred, class_order=None, title_no = 5.1):\n",
        "    if class_order is None:\n",
        "        classes = list(pd.unique(pd.Series(list(y_true) + list(y_pred))))\n",
        "    else:\n",
        "        present = set(pd.unique(pd.Series(list(y_true) + list(y_pred))))\n",
        "        classes = [c for c in class_order if c in present] or list(present)\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
        "    df = pd.DataFrame(cm, index=[f\"Actual  {c}\" for c in classes], columns=classes)\n",
        "\n",
        "    # --- map numeric 0/1 to desired display names (Normal/Abnormal) ---\n",
        "    if set(classes) == {0, 1}:\n",
        "        disp = list(class_order) if (class_order and len(class_order) == 2) else [\"Normal\", \"Abnormal\"]\n",
        "        name_map = {0: disp[0], 1: disp[1]}\n",
        "        df.columns = [name_map.get(c, c) for c in df.columns]\n",
        "        df.index   = [f\"Actual  {name_map.get(c, c)}\" for c in classes]\n",
        "    # -------------------------------------------------------------------\n",
        "\n",
        "    row_tot = cm.sum(axis=1)\n",
        "    recalls = np.divide(np.diag(cm), row_tot, out=np.zeros_like(row_tot, dtype=float), where=row_tot != 0) * 100.0\n",
        "    df[\"Recall (%)\"] = np.round(recalls, 1)\n",
        "\n",
        "    col_tot = cm.sum(axis=0)\n",
        "    precisions = np.divide(np.diag(cm), col_tot, out=np.zeros_like(col_tot, dtype=float), where=col_tot != 0) * 100.0\n",
        "\n",
        "    display_cols = list(df.columns)[:len(classes)]\n",
        "    prec_row = pd.Series(np.round(precisions, 1), index=display_cols, name=\"Precision (%)\")\n",
        "    prec_row[\"Recall (%)\"] = \"\"\n",
        "    df = pd.concat([df, prec_row.to_frame().T], axis=0)\n",
        "\n",
        "    # ===== Make \"Actual\" appear ONCE and centered on the class rows =====\n",
        "    n_class_rows = len(classes)              # rows before the final Precision row\n",
        "    mid = n_class_rows // 2                  # middle row index\n",
        "    # strip the leading \"Actual  \" from current row labels for display\n",
        "    class_names = [idx.replace(\"Actual  \", \"\", 1) for idx in df.index[:n_class_rows]]\n",
        "\n",
        "    new_index = []\n",
        "    for i, name in enumerate(class_names):\n",
        "        left = \"Actual\" if i == mid else \"\"\n",
        "        new_index.append((left, name))\n",
        "    new_index.append((\"\", \"Precision (%)\"))  # last row\n",
        "    df.index = pd.MultiIndex.from_tuples(new_index)\n",
        "    # ====================================================================\n",
        "\n",
        "    print(f\"\\nTable {title_no}\")\n",
        "    print(\"Multi category classification confusion matrix for the NSL KDD dataset.\\n\")\n",
        "    print(\"Predicted\\n\")\n",
        "    print(df.to_string())\n",
        "    return df\n",
        "\n",
        "# usage (unchanged)\n",
        "nsl_order = [\"Normal\", \"Abnormal\"]\n",
        "for name, out in rf_results.items():\n",
        "    print(f\"\\n=== Confusion Matrix: {name} ===\")\n",
        "    tbl = nslkdd_confusion_table(y_test, out[\"y_test_pred\"], class_order=nsl_order, title_no=5.1)\n",
        "    csv_path = f\"UNSW_NB15_confusion_table_RF_{name}.csv\"\n",
        "    tbl.to_csv(csv_path, index=True)\n",
        "    print(f\"Saved to {csv_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Km6NhCQObcrt"
      },
      "source": [
        " # KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRI3hY30RI6N",
        "outputId": "ef35c9c0-a604-487a-d761-bebbd0f58d60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== KNN Model Performance ===\n",
            "\n",
            "Running KNN with PCA configuration...\n",
            "\n",
            "KNN Model Performance Metrics\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training       1.0       1.0     1.0        1.0      1.0\n",
            "    Test       1.0       1.0     1.0        1.0      1.0\n",
            "\n",
            "Running KNN with LDA configuration...\n",
            "\n",
            "KNN Model Performance Metrics\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training       1.0       1.0     1.0        1.0      1.0\n",
            "    Test       1.0       1.0     1.0        1.0      1.0\n"
          ]
        }
      ],
      "source": [
        "# Configuration list to store different data setups\n",
        "configurations = []\n",
        "\n",
        "configurations.append((f'PCA', X_train_pca, X_test_pca, y_train))\n",
        "configurations.append((f'LDA', X_train_lda, X_test_lda, y_train))\n",
        "\n",
        "\n",
        "# Step 7: Run KNN  on different configurations\n",
        "print(\"\\n=== KNN Model Performance ===\")\n",
        "\n",
        "knn = KNeighborsClassifier(\n",
        "    n_neighbors=11,\n",
        "    weights=\"distance\",\n",
        "    metric=\"minkowski\",\n",
        "    p=2,\n",
        "    n_jobs=-1\n",
        ")\n",
        "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "    print(f\"\\nRunning KNN with {name} configuration...\")\n",
        "\n",
        "    knn.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    y_train_pred = knn.predict(X_train_cfg)\n",
        "    y_test_pred  = knn.predict(X_test_cfg)\n",
        "\n",
        "    y_train_proba = knn.predict_proba(X_train_cfg)\n",
        "    y_test_proba  = knn.predict_proba(X_test_cfg)\n",
        "\n",
        "\n",
        "    metrics_dict = {\n",
        "        \"Dataset\": [\"Training\", \"Test\"],\n",
        "        \"Accuracy\": [\n",
        "            metrics.accuracy_score(y_train_cfg, y_train_pred),\n",
        "            metrics.accuracy_score(y_test,      y_test_pred),\n",
        "        ],\n",
        "        \"F1 Score\": [\n",
        "            metrics.f1_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.f1_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"Recall\": [\n",
        "            metrics.recall_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.recall_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"Precision\": [\n",
        "            metrics.precision_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.precision_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"AUC-ROC\": [\n",
        "            metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_proba, multi_class='ovr', average='macro'),\n",
        "            metrics.roc_auc_score(pd.get_dummies(y_test),      y_test_proba,  multi_class='ovr', average='macro'),\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics_dict)\n",
        "    print(\"\\nKNN Model Performance Metrics\")\n",
        "    print(df_metrics.to_string(index=False))\n",
        "\n",
        "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_proba, multi_class='ovr', average='macro')\n",
        "    storeResults('KNN', name,\n",
        "                 metrics.accuracy_score(y_test, y_test_pred),\n",
        "                 metrics.f1_score(y_test, y_test_pred, average='macro'),\n",
        "                 metrics.recall_score(y_test, y_test_pred, average='macro'),\n",
        "                 metrics.precision_score(y_test, y_test_pred, average='macro'),\n",
        "                 auc_score)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbLnqqocdWZR"
      },
      "source": [
        "# Gradient Boosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyc9p_6zRqUK",
        "outputId": "cba8594d-79ff-4222-f5af-c2ccca4321ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Gradient Boosting Model Performance ===\n",
            "\n",
            "Running Gradient Boosting with PCA configuration...\n",
            "\n",
            "Gradient Boosting Model Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  1.000000   1.00000 1.000000    1.00000 1.000000\n",
            "    Test  0.990881   0.98959 0.993301    0.98612 0.993958\n",
            "\n",
            "Running Gradient Boosting with LDA configuration...\n",
            "\n",
            "Gradient Boosting Model Performance Metrics\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training       1.0       1.0     1.0        1.0      1.0\n",
            "    Test       1.0       1.0     1.0        1.0      1.0\n"
          ]
        }
      ],
      "source": [
        "# Configuration list to store different data setups\n",
        "configurations = []\n",
        "\n",
        "configurations.append((f'PCA', X_train_pca, X_test_pca, y_train))\n",
        "configurations.append((f'LDA', X_train_lda, X_test_lda, y_train))\n",
        "\n",
        "# Step 7: Running Gradient Boosting  on different configurations\n",
        "print(\"\\n=== Gradient Boosting Model Performance ===\")\n",
        "\n",
        "gbc = GradientBoostingClassifier(\n",
        "    n_estimators=200,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=3,\n",
        "    min_samples_split=2,\n",
        "    min_samples_leaf=1,\n",
        "    subsample=1.0,\n",
        "    random_state=42\n",
        ")\n",
        "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "    print(f\"\\nRunning Gradient Boosting with {name} configuration...\")\n",
        "\n",
        "    gbc.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    y_train_pred = gbc.predict(X_train_cfg)\n",
        "    y_test_pred  = gbc.predict(X_test_cfg)\n",
        "\n",
        "    y_train_proba = gbc.predict_proba(X_train_cfg)\n",
        "    y_test_proba  = gbc.predict_proba(X_test_cfg)\n",
        "\n",
        "\n",
        "    metrics_dict = {\n",
        "        \"Dataset\": [\"Training\", \"Test\"],\n",
        "        \"Accuracy\": [\n",
        "            metrics.accuracy_score(y_train_cfg, y_train_pred),\n",
        "            metrics.accuracy_score(y_test,      y_test_pred),\n",
        "        ],\n",
        "        \"F1 Score\": [\n",
        "            metrics.f1_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.f1_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"Recall\": [\n",
        "            metrics.recall_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.recall_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"Precision\": [\n",
        "            metrics.precision_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.precision_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"AUC-ROC\": [\n",
        "            metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_proba, multi_class='ovr', average='macro'),\n",
        "            metrics.roc_auc_score(pd.get_dummies(y_test),      y_test_proba,  multi_class='ovr', average='macro'),\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics_dict)\n",
        "    print(\"\\nGradient Boosting Model Performance Metrics\")\n",
        "    print(df_metrics.to_string(index=False))\n",
        "\n",
        "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_proba, multi_class='ovr', average='macro')\n",
        "    storeResults('Gradient Boosting', name,\n",
        "                 metrics.accuracy_score(y_test, y_test_pred),\n",
        "                 metrics.f1_score(y_test, y_test_pred, average='macro'),\n",
        "                 metrics.recall_score(y_test, y_test_pred, average='macro'),\n",
        "                 metrics.precision_score(y_test, y_test_pred, average='macro'),\n",
        "                 auc_score)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hfc0wi85eWOv"
      },
      "source": [
        "#AdaBoosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzUf7LW6SQrF",
        "outputId": "fe19fb84-0883-4cf6-fe4a-e9469fd21503"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== AdaBoost Model Performance ===\n",
            "\n",
            "Running AdaBoost with PCA configuration...\n",
            "\n",
            "AdaBoosting Model Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  1.000000   1.00000 1.000000    1.00000 1.000000\n",
            "    Test  0.990881   0.98959 0.993301    0.98612 0.993301\n",
            "\n",
            "Running AdaBoost with LDA configuration...\n",
            "\n",
            "AdaBoosting Model Performance Metrics\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training       1.0       1.0     1.0        1.0      1.0\n",
            "    Test       1.0       1.0     1.0        1.0      1.0\n"
          ]
        }
      ],
      "source": [
        "# Configuration list to store different data setups\n",
        "configurations = []\n",
        "\n",
        "configurations.append((f'PCA', X_train_pca, X_test_pca, y_train))\n",
        "configurations.append((f'LDA', X_train_lda, X_test_lda, y_train))\n",
        "\n",
        "# Step 7: Run AdaBoost  on different configurations\n",
        "print(\"\\n=== AdaBoost Model Performance ===\")\n",
        "\n",
        "ada = AdaBoostClassifier(\n",
        "        estimator=DecisionTreeClassifier(max_depth=1, random_state=42),\n",
        "        n_estimators=200,\n",
        "        learning_rate=0.1,\n",
        "        algorithm=\"SAMME\",\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "    print(f\"\\nRunning AdaBoost with {name} configuration...\")\n",
        "\n",
        "    ada.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    y_train_pred = ada.predict(X_train_cfg)\n",
        "    y_test_pred  = ada.predict(X_test_cfg)\n",
        "\n",
        "    y_train_proba = ada.predict_proba(X_train_cfg)\n",
        "    y_test_proba  = ada.predict_proba(X_test_cfg)\n",
        "\n",
        "\n",
        "    metrics_dict = {\n",
        "        \"Dataset\": [\"Training\", \"Test\"],\n",
        "        \"Accuracy\": [\n",
        "            metrics.accuracy_score(y_train_cfg, y_train_pred),\n",
        "            metrics.accuracy_score(y_test,      y_test_pred),\n",
        "        ],\n",
        "        \"F1 Score\": [\n",
        "            metrics.f1_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.f1_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"Recall\": [\n",
        "            metrics.recall_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.recall_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"Precision\": [\n",
        "            metrics.precision_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.precision_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"AUC-ROC\": [\n",
        "            metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_proba, multi_class='ovr', average='macro'),\n",
        "            metrics.roc_auc_score(pd.get_dummies(y_test),      y_test_proba,  multi_class='ovr', average='macro'),\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics_dict)\n",
        "    print(\"\\nAdaBoosting Model Performance Metrics\")\n",
        "    print(df_metrics.to_string(index=False))\n",
        "\n",
        "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_proba, multi_class='ovr', average='macro')\n",
        "    storeResults('AdaBoosting', name,\n",
        "                 metrics.accuracy_score(y_test, y_test_pred),\n",
        "                 metrics.f1_score(y_test, y_test_pred, average='macro'),\n",
        "                 metrics.recall_score(y_test, y_test_pred, average='macro'),\n",
        "                 metrics.precision_score(y_test, y_test_pred, average='macro'),\n",
        "                 auc_score)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyjP2M1SY-E6"
      },
      "source": [
        "# XGBoosting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZR9tnx0S5hD",
        "outputId": "e639551d-92e6-4b58-cdfe-2e69d5d956e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== XGBoost Model Performance ===\n",
            "\n",
            "Running XGBoosting with PCA configuration...\n",
            "\n",
            "XGBoosting Model Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  1.000000   1.00000 1.000000   1.000000 1.000000\n",
            "    Test  0.540182   0.53567 0.662207   0.704941 0.999992\n",
            "\n",
            "Running XGBoosting with LDA configuration...\n",
            "\n",
            "XGBoosting Model Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  1.000000  1.000000 1.000000     1.0000 1.000000\n",
            "    Test  0.999681  0.999633 0.999765     0.9995 0.999765\n"
          ]
        }
      ],
      "source": [
        "# Configuration list to store different data setups\n",
        "configurations = []\n",
        "\n",
        "configurations.append((f'PCA', X_train_pca, X_test_pca, y_train))\n",
        "configurations.append((f'LDA', X_train_lda, X_test_lda, y_train))\n",
        "\n",
        "\n",
        "# Step 7: Run XGBoosting on different configurations\n",
        "print(\"\\n=== XGBoost Model Performance ===\")\n",
        "\n",
        "xgb = XGBClassifier(\n",
        "    n_estimators=200,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=6,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    tree_method=\"hist\",\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "    print(f\"\\nRunning XGBoosting with {name} configuration...\")\n",
        "\n",
        "    xgb.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    y_train_pred = xgb.predict(X_train_cfg)\n",
        "    y_test_pred  = xgb.predict(X_test_cfg)\n",
        "\n",
        "    y_train_proba = xgb.predict_proba(X_train_cfg)\n",
        "    y_test_proba  = xgb.predict_proba(X_test_cfg)\n",
        "\n",
        "\n",
        "    metrics_dict = {\n",
        "        \"Dataset\": [\"Training\", \"Test\"],\n",
        "        \"Accuracy\": [\n",
        "            metrics.accuracy_score(y_train_cfg, y_train_pred),\n",
        "            metrics.accuracy_score(y_test,      y_test_pred),\n",
        "        ],\n",
        "        \"F1 Score\": [\n",
        "            metrics.f1_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.f1_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"Recall\": [\n",
        "            metrics.recall_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.recall_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"Precision\": [\n",
        "            metrics.precision_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.precision_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"AUC-ROC\": [\n",
        "            metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_proba, multi_class='ovr', average='macro'),\n",
        "            metrics.roc_auc_score(pd.get_dummies(y_test),      y_test_proba,  multi_class='ovr', average='macro'),\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics_dict)\n",
        "    print(\"\\nXGBoosting Model Performance Metrics\")\n",
        "    print(df_metrics.to_string(index=False))\n",
        "\n",
        "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_proba, multi_class='ovr', average='macro')\n",
        "    storeResults('XGBoosting', name,\n",
        "                 metrics.accuracy_score(y_test, y_test_pred),\n",
        "                 metrics.f1_score(y_test, y_test_pred, average='macro'),\n",
        "                 metrics.recall_score(y_test, y_test_pred, average='macro'),\n",
        "                 metrics.precision_score(y_test, y_test_pred, average='macro'),\n",
        "                 auc_score)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xFOCq0pwChT"
      },
      "source": [
        "#CatBoosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzTaGUy_wHGq",
        "outputId": "1411742c-9e57-46a0-e4c9-cb898836a2df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== CatBoost Model Performance ===\n",
            "\n",
            "Running CatBoosting with PCA configuration...\n",
            "\n",
            "CatBoosting Model Performance Metrics\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training       1.0       1.0     1.0        1.0      1.0\n",
            "    Test       1.0       1.0     1.0        1.0      1.0\n",
            "\n",
            "Running CatBoosting with LDA configuration...\n",
            "\n",
            "CatBoosting Model Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.999830  0.999828 0.999811   0.999846 0.999999\n",
            "    Test  0.999875  0.999856 0.999804   0.999908 0.999988\n"
          ]
        }
      ],
      "source": [
        "# Configuration list to store different data setups\n",
        "configurations = []\n",
        "\n",
        "configurations.append((f'PCA', X_train_pca, X_test_pca, y_train))\n",
        "configurations.append((f'LDA', X_train_lda, X_test_lda, y_train))\n",
        "\n",
        "\n",
        "# Step 7: Run CatBoosting on different configurations\n",
        "print(\"\\n=== CatBoost Model Performance ===\")\n",
        "cat = CatBoostClassifier(\n",
        "\n",
        "\n",
        "    bagging_temperature =0.05,\n",
        "    boosting_type = 'Plain',\n",
        "    learning_rate=0.05,\n",
        "    depth=3,\n",
        "    n_estimators=100,\n",
        "    random_seed=42,\n",
        "    silent =True\n",
        "\n",
        ")\n",
        "\n",
        "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "    print(f\"\\nRunning CatBoosting with {name} configuration...\")\n",
        "\n",
        "    cat.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    y_train_pred = cat.predict(X_train_cfg)\n",
        "    y_test_pred  = cat.predict(X_test_cfg)\n",
        "\n",
        "    y_train_proba = cat.predict_proba(X_train_cfg)\n",
        "    y_test_proba  = cat.predict_proba(X_test_cfg)\n",
        "\n",
        "\n",
        "    metrics_dict = {\n",
        "        \"Dataset\": [\"Training\", \"Test\"],\n",
        "        \"Accuracy\": [\n",
        "            metrics.accuracy_score(y_train_cfg, y_train_pred),\n",
        "            metrics.accuracy_score(y_test,      y_test_pred),\n",
        "        ],\n",
        "        \"F1 Score\": [\n",
        "            metrics.f1_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.f1_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"Recall\": [\n",
        "            metrics.recall_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.recall_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"Precision\": [\n",
        "            metrics.precision_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.precision_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"AUC-ROC\": [\n",
        "            metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_proba, multi_class='ovr', average='macro'),\n",
        "            metrics.roc_auc_score(pd.get_dummies(y_test),      y_test_proba,  multi_class='ovr', average='macro'),\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics_dict)\n",
        "    print(\"\\nCatBoosting Model Performance Metrics\")\n",
        "    print(df_metrics.to_string(index=False))\n",
        "\n",
        "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_proba, multi_class='ovr', average='macro')\n",
        "    storeResults('CatBoosting', name,\n",
        "                 metrics.accuracy_score(y_test, y_test_pred),\n",
        "                 metrics.f1_score(y_test, y_test_pred, average='macro'),\n",
        "                 metrics.recall_score(y_test, y_test_pred, average='macro'),\n",
        "                 metrics.precision_score(y_test, y_test_pred, average='macro'),\n",
        "                 auc_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmOcixna2M5w"
      },
      "source": [
        "#Bagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NM05mZ_7UqbV",
        "outputId": "d3ac026a-3ad3-438a-d5bc-92960aef9966"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Bagging Model Performance ===\n",
            "\n",
            "Running Bagging with PCA configuration...\n",
            "\n",
            "Bagging Model Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  1.000000   1.00000 1.000000    1.00000 1.000000\n",
            "    Test  0.990881   0.98959 0.993301    0.98612 0.994235\n",
            "\n",
            "Running Bagging with LDA configuration...\n",
            "\n",
            "Bagging Model Performance Metrics\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training       1.0       1.0     1.0        1.0      1.0\n",
            "    Test       1.0       1.0     1.0        1.0      1.0\n"
          ]
        }
      ],
      "source": [
        "# Configuration list to store different data setups\n",
        "configurations = []\n",
        "\n",
        "configurations.append((f'PCA', X_train_pca, X_test_pca, y_train))\n",
        "configurations.append((f'LDA', X_train_lda, X_test_lda, y_train))\n",
        "\n",
        "# Step 7: Run Bagging Classifier on different configurations\n",
        "print(\"\\n=== Bagging Model Performance ===\")\n",
        "\n",
        "bag = BaggingClassifier(\n",
        "        estimator=DecisionTreeClassifier(max_depth=5, random_state=42),\n",
        "        n_estimators=200,\n",
        "        max_samples=1.0,\n",
        "        max_features=1.0,\n",
        "        bootstrap=True,\n",
        "        bootstrap_features=False,\n",
        "        n_jobs=-1,\n",
        "        random_state=42\n",
        "    )\n",
        "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "    print(f\"\\nRunning Bagging with {name} configuration...\")\n",
        "\n",
        "    bag.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    y_train_pred = bag.predict(X_train_cfg)\n",
        "    y_test_pred  = bag.predict(X_test_cfg)\n",
        "\n",
        "    y_train_proba = bag.predict_proba(X_train_cfg)\n",
        "    y_test_proba  = bag.predict_proba(X_test_cfg)\n",
        "\n",
        "\n",
        "    metrics_dict = {\n",
        "        \"Dataset\": [\"Training\", \"Test\"],\n",
        "        \"Accuracy\": [\n",
        "            metrics.accuracy_score(y_train_cfg, y_train_pred),\n",
        "            metrics.accuracy_score(y_test,      y_test_pred),\n",
        "        ],\n",
        "        \"F1 Score\": [\n",
        "            metrics.f1_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.f1_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"Recall\": [\n",
        "            metrics.recall_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.recall_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"Precision\": [\n",
        "            metrics.precision_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.precision_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"AUC-ROC\": [\n",
        "            metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_proba, multi_class='ovr', average='macro'),\n",
        "            metrics.roc_auc_score(pd.get_dummies(y_test),      y_test_proba,  multi_class='ovr', average='macro'),\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics_dict)\n",
        "    print(\"\\nBagging Model Performance Metrics\")\n",
        "    print(df_metrics.to_string(index=False))\n",
        "\n",
        "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_proba, multi_class='ovr', average='macro')\n",
        "    storeResults('Bagging Classifier', name,\n",
        "                 metrics.accuracy_score(y_test, y_test_pred),\n",
        "                 metrics.f1_score(y_test, y_test_pred, average='macro'),\n",
        "                 metrics.recall_score(y_test, y_test_pred, average='macro'),\n",
        "                 metrics.precision_score(y_test, y_test_pred, average='macro'),\n",
        "                 auc_score)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mmlx0Lwv4lMI"
      },
      "source": [
        "# Voting Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "930yU9PYVQhQ",
        "outputId": "4dfa000a-be29-45ea-d71d-3e105bc9aa32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Voting Model Performance ===\n",
            "\n",
            "Training models with PCA configuration...\n",
            "\n",
            "=== Voting Classifier (hard) with PCA ===\n",
            "\n",
            "Voting Classifier (hard) Performance Metrics\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training       1.0       1.0     1.0        1.0        0\n",
            "    Test       1.0       1.0     1.0        1.0        0\n",
            "\n",
            "=== Voting Classifier (soft) with PCA ===\n",
            "\n",
            "Voting Classifier (soft) Performance Metrics\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training       1.0       1.0     1.0        1.0      1.0\n",
            "    Test       1.0       1.0     1.0        1.0      1.0\n",
            "\n",
            "=== Voting Classifier (weighted_hard) with PCA ===\n",
            "\n",
            "Voting Classifier (weighted_hard) Performance Metrics\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training       1.0       1.0     1.0        1.0        0\n",
            "    Test       1.0       1.0     1.0        1.0        0\n",
            "\n",
            "=== Voting Classifier (weighted_soft) with PCA ===\n",
            "\n",
            "Voting Classifier (weighted_soft) Performance Metrics\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training       1.0       1.0     1.0        1.0      1.0\n",
            "    Test       1.0       1.0     1.0        1.0      1.0\n",
            "\n",
            "Training models with LDA configuration...\n",
            "\n",
            "=== Voting Classifier (hard) with LDA ===\n",
            "\n",
            "Voting Classifier (hard) Performance Metrics\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training       1.0       1.0     1.0        1.0        0\n",
            "    Test       1.0       1.0     1.0        1.0        0\n",
            "\n",
            "=== Voting Classifier (soft) with LDA ===\n",
            "\n",
            "Voting Classifier (soft) Performance Metrics\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training       1.0       1.0     1.0        1.0      1.0\n",
            "    Test       1.0       1.0     1.0        1.0      1.0\n",
            "\n",
            "=== Voting Classifier (weighted_hard) with LDA ===\n",
            "\n",
            "Voting Classifier (weighted_hard) Performance Metrics\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training       1.0       1.0     1.0        1.0        0\n",
            "    Test       1.0       1.0     1.0        1.0        0\n",
            "\n",
            "=== Voting Classifier (weighted_soft) with LDA ===\n",
            "\n",
            "Voting Classifier (weighted_soft) Performance Metrics\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training       1.0       1.0     1.0        1.0      1.0\n",
            "    Test       1.0       1.0     1.0        1.0      1.0\n"
          ]
        }
      ],
      "source": [
        "# Configuration list to store different data setups\n",
        "configurations = []\n",
        "\n",
        "configurations.append((f'PCA', X_train_pca, X_test_pca, y_train))\n",
        "configurations.append((f'LDA', X_train_lda, X_test_lda, y_train))\n",
        "\n",
        "\n",
        "# Step 7: Run Voting Classifier on different configurations\n",
        "print(\"\\n=== Voting Model Performance ===\")\n",
        "\n",
        "# base learners\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=None,\n",
        "    min_samples_split=2,\n",
        "    min_samples_leaf=1,\n",
        "    max_features=\"sqrt\",\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "knn = KNeighborsClassifier(\n",
        "    n_neighbors=11,\n",
        "    weights=\"distance\",\n",
        "    metric=\"minkowski\",\n",
        "    p=2,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "cat = CatBoostClassifier(\n",
        "    learning_rate=0.05,\n",
        "    depth=3,\n",
        "    n_estimators=100,\n",
        "    bagging_temperature=0.05,\n",
        "    boosting_type='Plain',\n",
        "    random_seed=42,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "    print(f\"\\nTraining models with {name} configuration...\")\n",
        "\n",
        "    rf.fit(X_train_cfg, y_train_cfg)\n",
        "    knn.fit(X_train_cfg, y_train_cfg)\n",
        "    cat.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    # Define y_test_cfg for each configuration\n",
        "    y_test_cfg = y_test  # Use the corresponding test set 'y_test'\n",
        "\n",
        "    # Voting configurations using the actual model objects\n",
        "    voting_configs = [\n",
        "        (\"hard\", VotingClassifier(estimators=[(\"rf\", rf), (\"knn\", knn), (\"cat\", cat)], voting=\"hard\")),\n",
        "        (\"soft\", VotingClassifier(estimators=[(\"rf\", rf), (\"knn\", knn), (\"cat\", cat)], voting=\"soft\")),\n",
        "        (\"weighted_hard\", VotingClassifier(estimators=[(\"rf\", rf), (\"knn\", knn), (\"cat\", cat)], voting=\"hard\", weights=[0.3, 0.3, 0.4])),\n",
        "        (\"weighted_soft\", VotingClassifier(estimators=[(\"rf\", rf), (\"knn\", knn), (\"cat\", cat)], voting=\"soft\", weights=[0.4, 0.3, 0.3]))\n",
        "    ]\n",
        "\n",
        "    for voting_type, voting_clf in voting_configs:\n",
        "        print(f\"\\n=== Voting Classifier ({voting_type}) with {name} ===\")\n",
        "\n",
        "        # Fit voting classifier\n",
        "        voting_clf.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "        # Predictions\n",
        "        y_train_pred = voting_clf.predict(X_train_cfg)\n",
        "        y_test_pred = voting_clf.predict(X_test_cfg)\n",
        "\n",
        "        # Check if voting is hard or soft for probabilities\n",
        "        if 'hard' in voting_type:\n",
        "            # For hard voting, we cannot get probabilities, so set AUC-ROC to 0\n",
        "            auc_train = 0\n",
        "            auc_test = 0\n",
        "        else:\n",
        "            # For soft voting, we can get probabilities\n",
        "            y_train_proba = voting_clf.predict_proba(X_train_cfg)\n",
        "            y_test_proba = voting_clf.predict_proba(X_test_cfg)\n",
        "            auc_train = metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_proba, multi_class='ovr', average='macro')\n",
        "            auc_test = metrics.roc_auc_score(pd.get_dummies(y_test_cfg), y_test_proba, multi_class='ovr', average='macro')\n",
        "\n",
        "        # Calculate metrics\n",
        "        metrics_dict = {\n",
        "            \"Dataset\": [\"Training\", \"Test\"],\n",
        "            \"Accuracy\": [\n",
        "                metrics.accuracy_score(y_train_cfg, y_train_pred),\n",
        "                metrics.accuracy_score(y_test_cfg, y_test_pred),\n",
        "            ],\n",
        "            \"F1 Score\": [\n",
        "                metrics.f1_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "                metrics.f1_score(y_test_cfg, y_test_pred, average='macro'),\n",
        "            ],\n",
        "            \"Recall\": [\n",
        "                metrics.recall_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "                metrics.recall_score(y_test_cfg, y_test_pred, average='macro'),\n",
        "            ],\n",
        "            \"Precision\": [\n",
        "                metrics.precision_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "                metrics.precision_score(y_test_cfg, y_test_pred, average='macro'),\n",
        "            ],\n",
        "            \"AUC-ROC\": [auc_train, auc_test]\n",
        "        }\n",
        "\n",
        "        df_metrics = pd.DataFrame(metrics_dict)\n",
        "        print(f\"\\nVoting Classifier ({voting_type}) Performance Metrics\")\n",
        "        print(df_metrics.to_string(index=False))\n",
        "\n",
        "        storeResults(\n",
        "            f'Voting Classifier ({voting_type})',\n",
        "            name,\n",
        "            metrics.accuracy_score(y_test_cfg, y_test_pred),\n",
        "            metrics.f1_score(y_test_cfg, y_test_pred, average='macro'),\n",
        "            metrics.recall_score(y_test_cfg, y_test_pred, average='macro'),\n",
        "            metrics.precision_score(y_test_cfg, y_test_pred, average='macro'),\n",
        "            auc_test\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYaIECwC6YFz"
      },
      "source": [
        "# Stacking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Qc106BqeAGo",
        "outputId": "4dec447f-8ba6-4b94-95e4-d3858a2a9a6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Stacking Model Performance ===\n",
            "\n",
            "Training models with PCA configuration...\n",
            "\n",
            "Stacking Classifier Performance Metrics\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training       1.0       1.0     1.0        1.0      1.0\n",
            "    Test       1.0       1.0     1.0        1.0      1.0\n",
            "\n",
            "Training models with LDA configuration...\n",
            "\n",
            "Stacking Classifier Performance Metrics\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training       1.0       1.0     1.0        1.0      1.0\n",
            "    Test       1.0       1.0     1.0        1.0      1.0\n"
          ]
        }
      ],
      "source": [
        "# Configuration list to store different data setups\n",
        "configurations = []\n",
        "\n",
        "configurations.append((f'PCA', X_train_pca, X_test_pca, y_train))\n",
        "configurations.append((f'LDA', X_train_lda, X_test_lda, y_train))\n",
        "\n",
        "# Step 7: Run Stacking Classifier on different configurations\n",
        "print(\"\\n=== Stacking Model Performance ===\")\n",
        "\n",
        "# base learners\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=None,\n",
        "    min_samples_split=2,\n",
        "    min_samples_leaf=1,\n",
        "    max_features=\"sqrt\",\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "knn = KNeighborsClassifier(\n",
        "    n_neighbors=11,\n",
        "    weights=\"distance\",\n",
        "    metric=\"minkowski\",\n",
        "    p=2,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "cat = CatBoostClassifier(\n",
        "    learning_rate=0.05,\n",
        "    depth=3,\n",
        "    n_estimators=100,\n",
        "    bagging_temperature=0.05,\n",
        "    boosting_type='Plain',\n",
        "    random_seed=42,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# Train each model\n",
        "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "    print(f\"\\nTraining models with {name} configuration...\")\n",
        "\n",
        "    rf.fit(X_train_cfg, y_train_cfg)\n",
        "    knn.fit(X_train_cfg, y_train_cfg)\n",
        "    cat.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    # Create stacking classifier\n",
        "    base_estimators = [\n",
        "        ('rf', rf),\n",
        "        ('knn', knn),\n",
        "        ('cat', cat)\n",
        "    ]\n",
        "\n",
        "    # Meta-learner\n",
        "    meta_learner = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "    # Stacking classifier\n",
        "    stacking_clf = StackingClassifier(\n",
        "        estimators=base_estimators,\n",
        "        final_estimator=meta_learner,\n",
        "        cv=5,  # Use 5-fold cross-validation to train meta-learner\n",
        "        stack_method='predict_proba',  # Use probabilities for meta-features\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # Fit stacking classifier\n",
        "    stacking_clf.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    # Predictions\n",
        "    y_train_pred = stacking_clf.predict(X_train_cfg)\n",
        "    y_test_pred = stacking_clf.predict(X_test_cfg)\n",
        "    y_train_proba = stacking_clf.predict_proba(X_train_cfg)\n",
        "    y_test_proba = stacking_clf.predict_proba(X_test_cfg)\n",
        "\n",
        "    # Calculate metrics\n",
        "    metrics_dict = {\n",
        "        \"Dataset\": [\"Training\", \"Test\"],\n",
        "        \"Accuracy\": [\n",
        "            metrics.accuracy_score(y_train_cfg, y_train_pred),\n",
        "            metrics.accuracy_score(y_test, y_test_pred),\n",
        "        ],\n",
        "        \"F1 Score\": [\n",
        "            metrics.f1_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.f1_score(y_test, y_test_pred, average='macro'),\n",
        "        ],\n",
        "        \"Recall\": [\n",
        "            metrics.recall_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.recall_score(y_test, y_test_pred, average='macro'),\n",
        "        ],\n",
        "        \"Precision\": [\n",
        "            metrics.precision_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.precision_score(y_test, y_test_pred, average='macro'),\n",
        "        ],\n",
        "        \"AUC-ROC\": [\n",
        "            metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_proba, multi_class='ovr', average='macro'),\n",
        "            metrics.roc_auc_score(pd.get_dummies(y_test), y_test_proba, multi_class='ovr', average='macro'),\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics_dict)\n",
        "    print(f\"\\nStacking Classifier Performance Metrics\")\n",
        "    print(df_metrics.to_string(index=False))\n",
        "\n",
        "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_proba, multi_class='ovr', average='macro')\n",
        "\n",
        "    storeResults(\n",
        "        'Stacking Classifier', name,\n",
        "        metrics.accuracy_score(y_test, y_test_pred),\n",
        "        metrics.f1_score(y_test, y_test_pred, average='macro'),\n",
        "        metrics.recall_score(y_test, y_test_pred, average='macro'),\n",
        "        metrics.precision_score(y_test, y_test_pred, average='macro'),\n",
        "        auc_score\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5oHNQ_wZMbz"
      },
      "source": [
        "# Data Frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXhgVw5_nMH7",
        "outputId": "d14e0fe2-dd8d-4518-de0d-288ee7032634"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================================================================\n",
            "MODEL PERFORMANCE RESULTS\n",
            "====================================================================================================\n",
            "                         ML Model Configuration Accuracy F1 Score   Recall Precision  ROC_AUC\n",
            "           Support Vector Machine           PCA  99.424%  99.341%  99.577%   99.114% 100.000%\n",
            "           Support Vector Machine           LDA 100.000% 100.000% 100.000%  100.000% 100.000%\n",
            "                    Random Forest           PCA  99.252%  99.145%  99.451%   98.856% 100.000%\n",
            "                    Random Forest           LDA 100.000% 100.000% 100.000%  100.000% 100.000%\n",
            "                              KNN           PCA 100.000% 100.000% 100.000%  100.000% 100.000%\n",
            "                              KNN           LDA 100.000% 100.000% 100.000%  100.000% 100.000%\n",
            "                Gradient Boosting           PCA  99.088%  98.959%  99.330%   98.612%  99.396%\n",
            "                Gradient Boosting           LDA 100.000% 100.000% 100.000%  100.000% 100.000%\n",
            "                      AdaBoosting           PCA  99.088%  98.959%  99.330%   98.612%  99.330%\n",
            "                      AdaBoosting           LDA 100.000% 100.000% 100.000%  100.000% 100.000%\n",
            "                       XGBoosting           PCA  54.018%  53.567%  66.221%   70.494%  99.999%\n",
            "                       XGBoosting           LDA  99.968%  99.963%  99.977%   99.950%  99.977%\n",
            "                      CatBoosting           PCA 100.000% 100.000% 100.000%  100.000% 100.000%\n",
            "                      CatBoosting           LDA  99.987%  99.986%  99.980%   99.991%  99.999%\n",
            "               Bagging Classifier           PCA  99.088%  98.959%  99.330%   98.612%  99.424%\n",
            "               Bagging Classifier           LDA 100.000% 100.000% 100.000%  100.000% 100.000%\n",
            "         Voting Classifier (hard)           PCA 100.000% 100.000% 100.000%  100.000%   0.000%\n",
            "         Voting Classifier (soft)           PCA 100.000% 100.000% 100.000%  100.000% 100.000%\n",
            "Voting Classifier (weighted_hard)           PCA 100.000% 100.000% 100.000%  100.000%   0.000%\n",
            "Voting Classifier (weighted_soft)           PCA 100.000% 100.000% 100.000%  100.000% 100.000%\n",
            "         Voting Classifier (hard)           LDA 100.000% 100.000% 100.000%  100.000%   0.000%\n",
            "         Voting Classifier (soft)           LDA 100.000% 100.000% 100.000%  100.000% 100.000%\n",
            "Voting Classifier (weighted_hard)           LDA 100.000% 100.000% 100.000%  100.000%   0.000%\n",
            "Voting Classifier (weighted_soft)           LDA 100.000% 100.000% 100.000%  100.000% 100.000%\n",
            "              Stacking Classifier           PCA 100.000% 100.000% 100.000%  100.000% 100.000%\n",
            "              Stacking Classifier           LDA 100.000% 100.000% 100.000%  100.000% 100.000%\n",
            "\n",
            "Results saved to model_results.csv\n",
            "\n",
            "====================================================================================================\n",
            "SORTED MODEL PERFORMANCE RESULTS (by Accuracy and F1 Score)\n",
            "====================================================================================================\n",
            "                         ML Model Configuration Accuracy F1 Score   Recall Precision  ROC_AUC\n",
            "                      CatBoosting           LDA  99.987%  99.986%  99.980%   99.991%  99.999%\n",
            "                       XGBoosting           LDA  99.968%  99.963%  99.977%   99.950%  99.977%\n",
            "           Support Vector Machine           PCA  99.424%  99.341%  99.577%   99.114% 100.000%\n",
            "                    Random Forest           PCA  99.252%  99.145%  99.451%   98.856% 100.000%\n",
            "                Gradient Boosting           PCA  99.088%  98.959%  99.330%   98.612%  99.396%\n",
            "                      AdaBoosting           PCA  99.088%  98.959%  99.330%   98.612%  99.330%\n",
            "               Bagging Classifier           PCA  99.088%  98.959%  99.330%   98.612%  99.424%\n",
            "                       XGBoosting           PCA  54.018%  53.567%  66.221%   70.494%  99.999%\n",
            "           Support Vector Machine           LDA 100.000% 100.000% 100.000%  100.000% 100.000%\n",
            "                    Random Forest           LDA 100.000% 100.000% 100.000%  100.000% 100.000%\n",
            "                              KNN           PCA 100.000% 100.000% 100.000%  100.000% 100.000%\n",
            "                              KNN           LDA 100.000% 100.000% 100.000%  100.000% 100.000%\n",
            "                Gradient Boosting           LDA 100.000% 100.000% 100.000%  100.000% 100.000%\n",
            "                      AdaBoosting           LDA 100.000% 100.000% 100.000%  100.000% 100.000%\n",
            "                      CatBoosting           PCA 100.000% 100.000% 100.000%  100.000% 100.000%\n",
            "               Bagging Classifier           LDA 100.000% 100.000% 100.000%  100.000% 100.000%\n",
            "         Voting Classifier (hard)           PCA 100.000% 100.000% 100.000%  100.000%   0.000%\n",
            "         Voting Classifier (soft)           PCA 100.000% 100.000% 100.000%  100.000% 100.000%\n",
            "Voting Classifier (weighted_hard)           PCA 100.000% 100.000% 100.000%  100.000%   0.000%\n",
            "Voting Classifier (weighted_soft)           PCA 100.000% 100.000% 100.000%  100.000% 100.000%\n",
            "         Voting Classifier (hard)           LDA 100.000% 100.000% 100.000%  100.000%   0.000%\n",
            "         Voting Classifier (soft)           LDA 100.000% 100.000% 100.000%  100.000% 100.000%\n",
            "Voting Classifier (weighted_hard)           LDA 100.000% 100.000% 100.000%  100.000%   0.000%\n",
            "Voting Classifier (weighted_soft)           LDA 100.000% 100.000% 100.000%  100.000% 100.000%\n",
            "              Stacking Classifier           PCA 100.000% 100.000% 100.000%  100.000% 100.000%\n",
            "              Stacking Classifier           LDA 100.000% 100.000% 100.000%  100.000% 100.000%\n",
            "\n",
            "Sorted results saved to sorted_model_results.csv\n",
            "\n",
            "====================================================================================================\n",
            "TOP CONFIGURATION PER MODEL\n",
            "====================================================================================================\n",
            "                         ML Model Configuration Accuracy F1 Score   Recall Precision  ROC_AUC\n",
            "                      AdaBoosting           PCA  99.088%  98.959%  99.330%   98.612%  99.330%\n",
            "               Bagging Classifier           PCA  99.088%  98.959%  99.330%   98.612%  99.424%\n",
            "                      CatBoosting           LDA  99.987%  99.986%  99.980%   99.991%  99.999%\n",
            "                Gradient Boosting           PCA  99.088%  98.959%  99.330%   98.612%  99.396%\n",
            "                              KNN           PCA 100.000% 100.000% 100.000%  100.000% 100.000%\n",
            "                    Random Forest           PCA  99.252%  99.145%  99.451%   98.856% 100.000%\n",
            "              Stacking Classifier           PCA 100.000% 100.000% 100.000%  100.000% 100.000%\n",
            "           Support Vector Machine           PCA  99.424%  99.341%  99.577%   99.114% 100.000%\n",
            "         Voting Classifier (hard)           PCA 100.000% 100.000% 100.000%  100.000%   0.000%\n",
            "         Voting Classifier (soft)           PCA 100.000% 100.000% 100.000%  100.000% 100.000%\n",
            "Voting Classifier (weighted_hard)           PCA 100.000% 100.000% 100.000%  100.000%   0.000%\n",
            "Voting Classifier (weighted_soft)           PCA 100.000% 100.000% 100.000%  100.000% 100.000%\n",
            "                       XGBoosting           LDA  99.968%  99.963%  99.977%   99.950%  99.977%\n",
            "\n",
            "Top configuration per model saved to top_configurations.csv\n"
          ]
        }
      ],
      "source": [
        "# Creating the dataframe\n",
        "# Make sure output folder exists\n",
        "out_dir = Path(\"final_results\")\n",
        "out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "result = pd.DataFrame({\n",
        "    'ML Model': ML_Model,\n",
        "    'Configuration': ML_Config,\n",
        "    'Accuracy': [f\"{acc * 100:.3f}%\" for acc in accuracy],\n",
        "    'F1 Score': [f\"{f1 * 100:.3f}%\" for f1 in f1_score],\n",
        "    'Recall': [f\"{rec * 100:.3f}%\" for rec in recall],\n",
        "    'Precision': [f\"{prec * 100:.3f}%\" for prec in precision],\n",
        "    'ROC_AUC': [f\"{roc * 100:.3f}%\" for roc in auc_roc],\n",
        "})\n",
        "\n",
        "# Remove duplicates based on model and configuration\n",
        "result.drop_duplicates(subset=[\"ML Model\", \"Configuration\"], inplace=True)\n",
        "\n",
        "# Display the result\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"MODEL PERFORMANCE RESULTS\")\n",
        "print(\"=\" * 100)\n",
        "print(result.to_string(index=False))\n",
        "\n",
        "# Save the result to a CSV file\n",
        "result.to_csv('final_results/model_results.csv', index=False)\n",
        "print(\"\\nResults saved to model_results.csv\")\n",
        "\n",
        "# Sort by Accuracy and F1 Score\n",
        "sorted_result = result.sort_values(by=['Accuracy', 'F1 Score'], ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Display the sorted result\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"SORTED MODEL PERFORMANCE RESULTS (by Accuracy and F1 Score)\")\n",
        "print(\"=\" * 100)\n",
        "print(sorted_result.to_string(index=False))\n",
        "\n",
        "# Save the sorted result\n",
        "sorted_result.to_csv('final_results/sorted_model_results.csv', index=False)\n",
        "print(\"\\nSorted results saved to sorted_model_results.csv\")\n",
        "\n",
        "# Extract top configuration per ML model\n",
        "top_per_model = sorted_result.groupby('ML Model', as_index=False).first()\n",
        "\n",
        "# Display and save the top configuration table\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"TOP CONFIGURATION PER MODEL\")\n",
        "print(\"=\" * 100)\n",
        "print(top_per_model.to_string(index=False))\n",
        "\n",
        "top_per_model.to_csv('final_results/top_configurations.csv', index=False)\n",
        "print(\"\\nTop configuration per model saved to top_configurations.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
