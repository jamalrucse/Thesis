{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEXLxeOtIqKY"
      },
      "source": [
        "# **Interactive 3D Visualization Framework For Machine Learning Based Network Intrusion Detection Systems**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bh-Cs1HMIGlm"
      },
      "source": [
        "# Libraries\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Iu70wj01szV",
        "outputId": "7dde420e-c714-4c56-db1a-6c5439dc0b16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: catboost in /usr/local/lib/python3.12/dist-packages (1.2.8)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.5)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "from math import log2\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n",
        "from sklearn.compose import ColumnTransformer, make_column_selector as selector\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, RFECV, RFE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier,AdaBoostClassifier, BaggingClassifier,VotingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import metrics\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "\n",
        "# Enable inline plotting for Jupyter notebooks\n",
        "%matplotlib inline\n",
        "\n",
        "# Filter warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rmt8sv_Jfpv"
      },
      "source": [
        "# Dataset Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qE0v7OZv18Wv",
        "outputId": "73725fcc-d130-4153-850a-7b3b40d8b836"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved:\n",
            "/content/kdd_train_5class.csv\n",
            "/content/kdd_test_5class.csv\n",
            "\n",
            "Training label distribution:\n",
            "labels\n",
            "Normal    67343\n",
            "DoS       45927\n",
            "Probe     11656\n",
            "R2L         995\n",
            "U2R          52\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Testing label distribution:\n",
            "labels\n",
            "Normal    11245\n",
            "DoS        8095\n",
            "Probe      2157\n",
            "R2L        1009\n",
            "U2R          38\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Load the datasets\n",
        "df_train = pd.read_csv(\"kdd_train.csv\")\n",
        "df_test  = pd.read_csv(\"kdd_test.csv\")\n",
        "\n",
        "label_col = \"labels\" if \"labels\" in df_train.columns else \"label\"\n",
        "\n",
        "# 39â†’5 mapping (covers train + test attack names)\n",
        "attack_to_5 = {\n",
        "    # Normal\n",
        "    \"normal\": \"Normal\",\n",
        "\n",
        "    # DoS\n",
        "    \"back\": \"DoS\", \"land\": \"DoS\", \"neptune\": \"DoS\", \"pod\": \"DoS\",\n",
        "    \"smurf\": \"DoS\", \"teardrop\": \"DoS\", \"apache2\": \"DoS\", \"mailbomb\": \"DoS\",\n",
        "    \"processtable\": \"DoS\", \"udpstorm\": \"DoS\", \"worm\": \"DoS\",\n",
        "\n",
        "    # Probe\n",
        "    \"ipsweep\": \"Probe\", \"nmap\": \"Probe\", \"portsweep\": \"Probe\", \"satan\": \"Probe\",\n",
        "    \"mscan\": \"Probe\", \"saint\": \"Probe\",\n",
        "\n",
        "    # R2L\n",
        "    \"ftp_write\": \"R2L\", \"guess_passwd\": \"R2L\", \"imap\": \"R2L\", \"multihop\": \"R2L\",\n",
        "    \"phf\": \"R2L\", \"spy\": \"R2L\", \"warezclient\": \"R2L\", \"warezmaster\": \"R2L\",\n",
        "    \"sendmail\": \"R2L\", \"named\": \"R2L\", \"snmpgetattack\": \"R2L\", \"snmpguess\": \"R2L\",\n",
        "    \"xlock\": \"R2L\", \"xsnoop\": \"R2L\", \"httptunnel\": \"R2L\",\n",
        "\n",
        "    # U2R\n",
        "    \"buffer_overflow\": \"U2R\", \"loadmodule\": \"U2R\", \"perl\": \"U2R\", \"rootkit\": \"U2R\",\n",
        "    \"ps\": \"U2R\", \"xterm\": \"U2R\",\n",
        "}\n",
        "\n",
        "# Update label column only\n",
        "df_train[label_col] = (\n",
        "    df_train[label_col].astype(str).str.lower().str.strip().map(attack_to_5)\n",
        ")\n",
        "df_test[label_col] = (\n",
        "    df_test[label_col].astype(str).str.lower().str.strip().map(attack_to_5)\n",
        ")\n",
        "\n",
        "# Save new CSVs with 5-class labels\n",
        "df_train.to_csv(\"kdd_train_5class.csv\", index=False)\n",
        "df_test.to_csv(\"kdd_test_5class.csv\", index=False)\n",
        "data1 = pd.read_csv('kdd_train_5class.csv')\n",
        "data2 = pd.read_csv('kdd_test_5class.csv')\n",
        "\n",
        "# Define target variable\n",
        "target_col = 'labels'\n",
        "X_train = data1.drop(columns=[target_col])\n",
        "y_train = data1[target_col]\n",
        "\n",
        "# Separate features and target for test\n",
        "X_test = data2.drop(columns=[target_col])\n",
        "y_test = data2[target_col]\n",
        "\n",
        "print(\"Saved:\")\n",
        "print(\"kdd_train_5class.csv\")\n",
        "print(\"kdd_test_5class.csv\")\n",
        "\n",
        "print(\"\\nTraining label distribution:\")\n",
        "print(df_train[label_col].value_counts())\n",
        "print(\"\\nTesting label distribution:\")\n",
        "print(df_test[label_col].value_counts())\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0qVBosRJomj"
      },
      "source": [
        "# Pipeline Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BJFU6besq3v",
        "outputId": "e901bd6d-824d-4ea3-eb63-04222c535ac5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After One Hot shapes: (125973, 122) (22544, 122)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# step 1.1: Safe fallback to keep this cell runnable\n",
        "_need_fallback = False\n",
        "try:\n",
        "    X_train\n",
        "    X_test\n",
        "    y_train\n",
        "    y_test\n",
        "except NameError:\n",
        "    _need_fallback = True\n",
        "\n",
        "if _need_fallback:\n",
        "    rng = np.random.RandomState(42)\n",
        "    n = 300\n",
        "    df_all = pd.DataFrame({\n",
        "        \"num1\": rng.randn(n),\n",
        "        \"num2\": rng.rand(n) * 5,\n",
        "        \"cat1\": rng.choice([\"tcp\", \"udp\", \"icmp\"], size=n),\n",
        "        \"cat2\": rng.choice([\"low\", \"med\", \"high\"], size=n),\n",
        "    })\n",
        "    y_all = ((df_all[\"num1\"] + 0.8 * df_all[\"num2\"] + (df_all[\"cat1\"] == \"tcp\").astype(int) + (df_all[\"cat2\"] == \"high\").astype(int) + rng.randn(n)*0.5) > 3).astype(int)\n",
        "    idx = np.arange(n)\n",
        "    rng.shuffle(idx)\n",
        "    cut = int(n*0.75)\n",
        "    tr, te = idx[:cut], idx[cut:]\n",
        "    X_train = df_all.iloc[tr].reset_index(drop=True)\n",
        "    X_test  = df_all.iloc[te].reset_index(drop=True)\n",
        "    y_train = y_all.iloc[tr].reset_index(drop=True)\n",
        "    y_test  = y_all.iloc[te].reset_index(drop=True)\n",
        "\n",
        "if not isinstance(X_train, pd.DataFrame):\n",
        "    X_train = pd.DataFrame(X_train).copy()\n",
        "if not isinstance(X_test, pd.DataFrame):\n",
        "    X_test = pd.DataFrame(X_test).copy()\n",
        "\n",
        "\n",
        "\n",
        "# step 1.2 Identify column types\n",
        "cat_cols = selector(dtype_include=[\"category\", \"object\"])(X_train)\n",
        "num_cols = selector(dtype_include=np.number)(X_train)\n",
        "\n",
        "# step 1.3 One Hot on categoricals and MinMax on numerics, fit on train, transform both\n",
        "cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
        "num_cols = X_train.select_dtypes(exclude=[\"object\", \"category\"]).columns.tolist()\n",
        "\n",
        "ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
        "\n",
        "X_train_cat = ohe.fit_transform(X_train[cat_cols]) if len(cat_cols) > 0 else np.empty((len(X_train), 0))\n",
        "X_test_cat  = ohe.transform(X_test[cat_cols])      if len(cat_cols) > 0 else np.empty((len(X_test), 0))\n",
        "\n",
        "X_train_num = X_train[num_cols].to_numpy(dtype=float) if len(num_cols) > 0 else np.empty((len(X_train), 0))\n",
        "X_test_num  = X_test[num_cols].to_numpy(dtype=float)  if len(num_cols) > 0 else np.empty((len(X_test), 0))\n",
        "\n",
        "X_train_oh = np.hstack([X_train_cat, X_train_num])\n",
        "X_test_oh  = np.hstack([X_test_cat,  X_test_num])\n",
        "\n",
        "print(\"After One Hot shapes:\", X_train_oh.shape, X_test_oh.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_0tqOiHnj9y",
        "outputId": "8fcc6d88-ce14-4d4c-f101-3160db45816a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After Normalization shapes: (125973, 122) (22544, 122)\n"
          ]
        }
      ],
      "source": [
        "# STEP 2: Normalization with MinMaxScaler\n",
        "# ================================\n",
        "scaler =  StandardScaler()\n",
        "X_train_normalized = scaler.fit_transform(X_train_oh)\n",
        "X_test_normalized  = scaler.transform(X_test_oh)\n",
        "\n",
        "print(\"After Normalization shapes:\", X_train_normalized.shape, X_test_normalized.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bs2tHbK9nqMS"
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "# STEP 3.1: Training set information gain calculation\n",
        "# ================================\n",
        "mi = mutual_info_classif(X_train_normalized, np.asarray(y_train), random_state=42, discrete_features=False)\n",
        "\n",
        "# 3.2: Convert scores to weights in [0,1], avoid exact zeros\n",
        "mi_max = np.max(mi) if np.max(mi) > 0 else 1.0\n",
        "weights = mi / mi_max\n",
        "weights = np.where(weights == 0, 1e-6, weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQV7xhoenzEL"
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "# STEP 4: Weighted Transform each feature\n",
        "# ================================\n",
        "X_train_weighted = X_train_normalized * weights\n",
        "X_test_weighted  = X_test_normalized  * weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3dMNgnIoCWc"
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "# STEP 5: Transform features to zero mean\n",
        "# ================================\n",
        "train_means = X_train_weighted.mean(axis=0)\n",
        "X_train_centered = X_train_weighted - train_means\n",
        "X_test_centered  = X_test_weighted  - train_means"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiZtTQ2CBXQC",
        "outputId": "b3480abb-3157-4f7a-8aae-dad4b02909d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== PCA output ===\n",
            "Number of components that explain 99.0% variance: 17\n",
            "\n",
            "=== LDA  output ===\n",
            "LDA components used: 4\n"
          ]
        }
      ],
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "# 6.1: PCA output to explain at least 99 percent variance\n",
        "print(\"\\n=== PCA output ===\")\n",
        "pca_probe = PCA().fit(X_train_centered)\n",
        "cum_var = np.cumsum(pca_probe.explained_variance_ratio_)\n",
        "n_components = int(np.argmax(cum_var >= 0.99) + 1)\n",
        "pca = PCA(n_components=max(1, n_components)).fit(X_train_centered)\n",
        "X_train_pca = pca.transform(X_train_centered)\n",
        "X_test_pca  = pca.transform(X_test_centered)\n",
        "print(f'Number of components that explain 99.0% variance: {pca.n_components_}')\n",
        "\n",
        "\n",
        "# 6.2: LDA output\n",
        "\n",
        "if len(np.unique(y_train)) > 1:\n",
        "    print(\"\\n=== LDA  output ===\")\n",
        "    n_classes = len(np.unique(y_train))\n",
        "    n_comp_lda = min(max(1, n_classes - 1), X_train_centered.shape[1])\n",
        "    lda = LDA(n_components=n_comp_lda).fit(X_train_pca, y_train)\n",
        "    X_train_lda = lda.transform(X_train_pca)\n",
        "    X_test_lda  = lda.transform(X_test_pca)\n",
        "    print(f\"LDA components used: {n_comp_lda}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SB6_xOy4Lxmk"
      },
      "source": [
        "# ML Model Results Storage Framework"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fODUcnRmKkZ0",
        "outputId": "f4388e34-488f-4413-e7c6-650a2b9d3727"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model results storage framework loaded successfully!\n",
            "Available functions:\n",
            "- storeResults(model, config, accuracy, f1, recall, precision, auc_roc)\n",
            "- displayAndSaveResults(filename_prefix='model_results')\n",
            "- clearResults()\n",
            "- plotModelComparison(result_df)\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# ML MODEL RESULTS STORAGE FRAMEWORK\n",
        "# =============================================================================\n",
        "\n",
        "# Creating holders to store the model performance results\n",
        "ML_Model = []\n",
        "ML_Config = []\n",
        "accuracy = []\n",
        "f1_score = []\n",
        "recall = []\n",
        "precision = []\n",
        "auc_roc = []  # Adding a holder for AUC-ROC\n",
        "\n",
        "# Function to call for storing the results\n",
        "def storeResults(model, config, a, b, c, d, e):\n",
        "    \"\"\"\n",
        "    Store model performance results\n",
        "\n",
        "    Parameters:\n",
        "    model: Name of the ML model\n",
        "    config: Configuration name (preprocessing steps applied)\n",
        "    a: Accuracy score\n",
        "    b: F1 score\n",
        "    c: Recall score\n",
        "    d: Precision score\n",
        "    e: AUC-ROC score\n",
        "    \"\"\"\n",
        "    ML_Model.append(model)\n",
        "    ML_Config.append(config)\n",
        "    accuracy.append(round(a, 6))\n",
        "    f1_score.append(round(b, 6))\n",
        "    recall.append(round(c, 6))\n",
        "    precision.append(round(d, 6))\n",
        "    auc_roc.append(round(e, 6))\n",
        "\n",
        "# Function to display and save results\n",
        "def displayAndSaveResults(filename_prefix='model_results'):\n",
        "    \"\"\"\n",
        "    Create dataframe from results, display, and save to CSV\n",
        "\n",
        "    Parameters:\n",
        "    filename_prefix: Prefix for the CSV filenames\n",
        "    \"\"\"\n",
        "    # Creating the dataframe\n",
        "    result = pd.DataFrame({\n",
        "        'ML Model': ML_Model,\n",
        "        'Configuration': ML_Config,\n",
        "        'Accuracy': [f\"{acc * 100:.3f}%\" for acc in accuracy],\n",
        "        'F1 Score': [f\"{f1 * 100:.3f}%\" for f1 in f1_score],\n",
        "        'Recall': [f\"{rec * 100:.3f}%\" for rec in recall],\n",
        "        'Precision': [f\"{prec * 100:.3f}%\" for prec in precision],\n",
        "        'ROC_AUC': [f\"{roc * 100:.3f}%\" for roc in auc_roc],\n",
        "    })\n",
        "\n",
        "    # Remove duplicates if any\n",
        "    result.drop_duplicates(subset=[\"ML Model\", \"Configuration\"], inplace=True)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(\"MODEL PERFORMANCE RESULTS\")\n",
        "    print(\"=\"*100)\n",
        "    print(result.to_string(index=False))\n",
        "\n",
        "    # Saving the result to a CSV file\n",
        "    result.to_csv(f'{filename_prefix}.csv', index=False)\n",
        "    print(f\"\\nResults saved to {filename_prefix}.csv\")\n",
        "\n",
        "    # Sorting the dataframe on accuracy and F1 Score\n",
        "    sorted_result = result.sort_values(by=['Accuracy', 'F1 Score'], ascending=False).reset_index(drop=True)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(\"SORTED MODEL PERFORMANCE RESULTS (by Accuracy and F1 Score)\")\n",
        "    print(\"=\"*100)\n",
        "    print(sorted_result.to_string(index=False))\n",
        "\n",
        "    # Saving the sorted result to a CSV file\n",
        "    sorted_result.to_csv(f'sorted_{filename_prefix}.csv', index=False)\n",
        "    print(f\"\\nSorted results saved to sorted_{filename_prefix}.csv\")\n",
        "\n",
        "    return result, sorted_result\n",
        "\n",
        "# Function to clear results (useful when running multiple experiments)\n",
        "def clearResults():\n",
        "    \"\"\"Clear all stored results\"\"\"\n",
        "    global ML_Model, ML_Config, accuracy, f1_score, recall, precision, auc_roc\n",
        "    ML_Model.clear()\n",
        "    ML_Config.clear()\n",
        "    accuracy.clear()\n",
        "    f1_score.clear()\n",
        "    recall.clear()\n",
        "    precision.clear()\n",
        "    auc_roc.clear()\n",
        "    print(\"Results cleared!\")\n",
        "\n",
        "# Function to plot model comparison\n",
        "def plotModelComparison(result_df):\n",
        "    \"\"\"\n",
        "    Create visualization comparing model performances\n",
        "\n",
        "    Parameters:\n",
        "    result_df: DataFrame with model results\n",
        "    \"\"\"\n",
        "    # Convert percentage strings back to floats for plotting\n",
        "    metrics_cols = ['Accuracy', 'F1 Score', 'Recall', 'Precision', 'ROC_AUC']\n",
        "    plot_df = result_df.copy()\n",
        "\n",
        "    for col in metrics_cols:\n",
        "        plot_df[col] = plot_df[col].str.rstrip('%').astype(float)\n",
        "\n",
        "    # Create subplot for each metric\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "    axes = axes.ravel()\n",
        "\n",
        "    for idx, metric in enumerate(metrics_cols):\n",
        "        # Group by model and get mean performance across configurations\n",
        "        model_performance = plot_df.groupby('ML Model')[metric].mean().sort_values(ascending=False)\n",
        "\n",
        "        # Create bar plot\n",
        "        ax = axes[idx]\n",
        "        bars = ax.bar(range(len(model_performance)), model_performance.values,\n",
        "                      color=plt.cm.Blues(np.linspace(0.4, 0.9, len(model_performance))))\n",
        "        ax.set_xticks(range(len(model_performance)))\n",
        "        ax.set_xticklabels(model_performance.index, rotation=45, ha='right')\n",
        "        ax.set_ylabel(f'{metric} (%)')\n",
        "        ax.set_title(f'Average {metric} by Model', fontweight='bold')\n",
        "        ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "        # Add value labels on bars\n",
        "        for bar in bars:\n",
        "            height = bar.get_height()\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                   f'{height:.1f}%', ha='center', va='bottom')\n",
        "\n",
        "    # Hide the last subplot if we have 5 metrics\n",
        "    if len(metrics_cols) == 5:\n",
        "        axes[5].set_visible(False)\n",
        "\n",
        "    plt.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "print(\"Model results storage framework loaded successfully!\")\n",
        "print(\"Available functions:\")\n",
        "print(\"- storeResults(model, config, accuracy, f1, recall, precision, auc_roc)\")\n",
        "print(\"- displayAndSaveResults(filename_prefix='model_results')\")\n",
        "print(\"- clearResults()\")\n",
        "print(\"- plotModelComparison(result_df)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHwhvio-qU3A"
      },
      "source": [
        "# SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiHesVrJO0mI",
        "outputId": "afe9ea5b-faa6-4461-e218-8b942efe434e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== SVM Model Performance  ===\n",
            "\n",
            "Running SVM with PCA configuration...\n",
            "\n",
            "SVM Model Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.978892  0.716378 0.686675   0.771514 0.994392\n",
            "    Test  0.900328  0.568250 0.559551   0.737911 0.946928\n",
            "\n",
            "Running SVM with LDA configuration...\n",
            "\n",
            "SVM Model Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.962643  0.697056 0.665594   0.754908 0.979532\n",
            "    Test  0.885247  0.556158 0.547907   0.719078 0.892811\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Configuration list to store different data setups\n",
        "configurations = []\n",
        "\n",
        "configurations.append((f'PCA', X_train_pca, X_test_pca, y_train))\n",
        "configurations.append((f'LDA', X_train_lda, X_test_lda, y_train))\n",
        "\n",
        "# Step 7: Run SVM  on different configurations\n",
        "print(\"\\n=== SVM Model Performance  ===\")\n",
        "svm_results = {}\n",
        "svm = SVC(\n",
        "    kernel=\"rbf\",\n",
        "    C=1.0,\n",
        "    gamma=\"scale\",\n",
        "    probability=True,\n",
        "    random_state=42\n",
        "    )\n",
        "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "    print(f\"\\nRunning SVM with {name} configuration...\")\n",
        "\n",
        "    svm.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    y_train_pred = svm.predict(X_train_cfg)\n",
        "    y_test_pred  = svm.predict(X_test_cfg)\n",
        "\n",
        "    y_train_proba = svm.predict_proba(X_train_cfg)\n",
        "    y_test_proba  = svm.predict_proba(X_test_cfg)\n",
        "\n",
        "    svm_results[name] = {\n",
        "        \"y_test_pred\": y_test_pred,\n",
        "        \"y_train_pred\": y_train_pred,\n",
        "        \"y_test_proba\": y_test_proba,\n",
        "        \"y_train_proba\": y_train_proba,\n",
        "        \"classes\": svm.classes_,\n",
        "    }\n",
        "    metrics_dict = {\n",
        "        \"Dataset\": [\"Training\", \"Test\"],\n",
        "        \"Accuracy\": [\n",
        "            metrics.accuracy_score(y_train_cfg, y_train_pred),\n",
        "            metrics.accuracy_score(y_test,      y_test_pred),\n",
        "        ],\n",
        "        \"F1 Score\": [\n",
        "            metrics.f1_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.f1_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"Recall\": [\n",
        "            metrics.recall_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.recall_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"Precision\": [\n",
        "            metrics.precision_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.precision_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"AUC-ROC\": [\n",
        "            metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_proba, multi_class='ovr', average='macro'),\n",
        "            metrics.roc_auc_score(pd.get_dummies(y_test),      y_test_proba,  multi_class='ovr', average='macro'),\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics_dict)\n",
        "    print(\"\\nSVM Model Performance Metrics\")\n",
        "    print(df_metrics.to_string(index=False))\n",
        "\n",
        "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_proba, multi_class='ovr', average='macro')\n",
        "    storeResults('Support Vector Machine', name,\n",
        "                 metrics.accuracy_score(y_test, y_test_pred),\n",
        "                 metrics.f1_score(y_test, y_test_pred, average='macro'),\n",
        "                 metrics.recall_score(y_test, y_test_pred, average='macro'),\n",
        "                 metrics.precision_score(y_test, y_test_pred, average='macro'),\n",
        "                 auc_score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_xA_FdzMAsM",
        "outputId": "6089c926-f2f7-4bf5-9c9b-24da7a2118e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Confusion Matrix: PCA ===\n",
            "\n",
            "Table 5.2\n",
            "Multi category classification confusion matrix for the NSL KDD dataset.\n",
            "\n",
            "Predicted\n",
            "\n",
            "                       DoS Normal Probe   R2L  U2R Recall (%)\n",
            "       DoS            7443    627    25     0    0       91.9\n",
            "       Normal          173  10972    97     3    0       97.6\n",
            "Actual Probe            66    266  1825     0    0       84.6\n",
            "       R2L               1    902    49    57    0        5.6\n",
            "       U2R               0     37     1     0    0        0.0\n",
            "       Precision (%)  96.9   85.7  91.4  95.0  0.0           \n",
            "Saved to NSL_KDD_confusion_table_SVM_PCA.csv\n",
            "\n",
            "=== Confusion Matrix: LDA ===\n",
            "\n",
            "Table 5.2\n",
            "Multi category classification confusion matrix for the NSL KDD dataset.\n",
            "\n",
            "Predicted\n",
            "\n",
            "                       DoS Normal Probe   R2L  U2R Recall (%)\n",
            "       DoS            7221    846    28     0    0       89.2\n",
            "       Normal          144  10911   186     4    0       97.0\n",
            "Actual Probe           119    272  1766     0    0       81.9\n",
            "       R2L              26    872    52    59    0        5.8\n",
            "       U2R               1     35     1     1    0        0.0\n",
            "       Precision (%)  96.1   84.3  86.9  92.2  0.0           \n",
            "Saved to NSL_KDD_confusion_table_SVM_LDA.csv\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def nslkdd_confusion_table(y_true, y_pred, class_order=None, title_no=5.2):\n",
        "    if class_order is None:\n",
        "        classes = list(pd.unique(pd.Series(list(y_true) + list(y_pred))))\n",
        "    else:\n",
        "        present = set(pd.unique(pd.Series(list(y_true) + list(y_pred))))\n",
        "        classes = [c for c in class_order if c in present] or list(present)\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
        "    df = pd.DataFrame(cm, index=classes, columns=classes)\n",
        "\n",
        "    # --- optional: map numeric 0..4 to provided names (for NSL-KDD 5-class) ---\n",
        "    if (class_order is not None and len(class_order) == len(classes)\n",
        "        and all(isinstance(c, (int, np.integer)) for c in classes)):\n",
        "        idx_sorted = sorted(classes)                         # preserve numeric order\n",
        "        name_map = {i: class_order[idx_sorted.index(i)] for i in idx_sorted}\n",
        "        df.columns = [name_map.get(c, c) for c in df.columns]\n",
        "        df.index   = [name_map.get(c, c) for c in df.index]\n",
        "    # ---------------------------------------------------------------------------\n",
        "\n",
        "    # recalls per row\n",
        "    row_tot = cm.sum(axis=1)\n",
        "    recalls = np.divide(np.diag(cm), row_tot, out=np.zeros_like(row_tot, dtype=float), where=row_tot != 0) * 100.0\n",
        "    df[\"Recall (%)\"] = np.round(recalls, 1)\n",
        "\n",
        "    # precisions per column\n",
        "    col_tot = cm.sum(axis=0)\n",
        "    precisions = np.divide(np.diag(cm), col_tot, out=np.zeros_like(col_tot, dtype=float), where=col_tot != 0) * 100.0\n",
        "    display_cols = list(df.columns)[:-1]  # exclude \"Recall (%)\"\n",
        "    prec_row = pd.Series(np.round(precisions, 1), index=display_cols, name=\"Precision (%)\")\n",
        "    prec_row[\"Recall (%)\"] = \"\"\n",
        "    df = pd.concat([df, prec_row.to_frame().T], axis=0)\n",
        "\n",
        "    # === Make \"Actual\" show once, in the MIDDLE of the class rows ===\n",
        "    row_labels = list(df.index)\n",
        "    n_class_rows = len(row_labels) - 1           # last row is \"Precision (%)\"\n",
        "    mid = n_class_rows // 2                      # middle position among class rows\n",
        "\n",
        "    mi = []\n",
        "    for i, lab in enumerate(row_labels):\n",
        "        if i < n_class_rows:                     # class rows\n",
        "            left = \"Actual\" if i == mid else \"\"  # only middle row shows \"Actual\"\n",
        "            mi.append((left, lab))\n",
        "        else:                                    # precision row\n",
        "            mi.append((\"\", \"Precision (%)\"))\n",
        "    df.index = pd.MultiIndex.from_tuples(mi)\n",
        "    # =================================================================\n",
        "\n",
        "    print(f\"\\nTable {title_no}\")\n",
        "    print(\"Multi category classification confusion matrix for the NSL KDD dataset.\\n\")\n",
        "    print(\"Predicted\\n\")\n",
        "    print(df.to_string())\n",
        "    return df\n",
        "\n",
        "# Usage (unchanged)\n",
        "nsl_order = [\"DoS\", \"Normal\", \"Probe\", \"R2L\", \"U2R\"]\n",
        "for name, out in svm_results.items():\n",
        "    print(f\"\\n=== Confusion Matrix: {name} ===\")\n",
        "    tbl = nslkdd_confusion_table(y_test, out[\"y_test_pred\"], class_order=nsl_order, title_no=5.2)\n",
        "    csv_path = f\"NSL_KDD_confusion_table_SVM_{name}.csv\"\n",
        "    tbl.to_csv(csv_path, index=True)\n",
        "    print(f\"Saved to {csv_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfra_hFKaulE"
      },
      "source": [
        "# Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvIm4D6eQZ50",
        "outputId": "689de4c0-8ad9-4ad9-91e9-67e4c9911634"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Random Forestr Model Performance ===\n",
            "\n",
            "Running Random Forest with PCA configuration...\n",
            "\n",
            "RF Model Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.999944  0.998028 0.996132   0.999962 1.000000\n",
            "    Test  0.917894  0.707349 0.650542   0.958085 0.877236\n",
            "\n",
            "Running Random Forest with LDA configuration...\n",
            "\n",
            "RF Model Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.999944  0.998028 0.996133   0.999961 1.000000\n",
            "    Test  0.909155  0.692572 0.641741   0.934379 0.867718\n"
          ]
        }
      ],
      "source": [
        "# Configuration list to store different data setups\n",
        "configurations = []\n",
        "\n",
        "configurations.append((f'PCA', X_train_pca, X_test_pca, y_train))\n",
        "configurations.append((f'LDA', X_train_lda, X_test_lda, y_train))\n",
        "\n",
        "# Step 7: Run RF different configurations\n",
        "print(\"\\n=== Random Forestr Model Performance ===\")\n",
        "\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=None,\n",
        "    min_samples_split=2,\n",
        "    min_samples_leaf=1,\n",
        "    max_features=\"sqrt\",\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "    print(f\"\\nRunning Random Forest with {name} configuration...\")\n",
        "\n",
        "    rf.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    y_train_pred = rf.predict(X_train_cfg)\n",
        "    y_test_pred  = rf.predict(X_test_cfg)\n",
        "\n",
        "    y_train_proba = rf.predict_proba(X_train_cfg)\n",
        "    y_test_proba  = rf.predict_proba(X_test_cfg)\n",
        "\n",
        "\n",
        "    metrics_dict = {\n",
        "        \"Dataset\": [\"Training\", \"Test\"],\n",
        "        \"Accuracy\": [\n",
        "            metrics.accuracy_score(y_train_cfg, y_train_pred),\n",
        "            metrics.accuracy_score(y_test,      y_test_pred),\n",
        "        ],\n",
        "        \"F1 Score\": [\n",
        "            metrics.f1_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.f1_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"Recall\": [\n",
        "            metrics.recall_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.recall_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"Precision\": [\n",
        "            metrics.precision_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.precision_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"AUC-ROC\": [\n",
        "            metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_proba, multi_class='ovr', average='macro'),\n",
        "            metrics.roc_auc_score(pd.get_dummies(y_test),      y_test_proba,  multi_class='ovr', average='macro'),\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics_dict)\n",
        "    print(\"\\nRF Model Performance Metrics\")\n",
        "    print(df_metrics.to_string(index=False))\n",
        "\n",
        "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_proba, multi_class='ovr', average='macro')\n",
        "    storeResults('Random Forest', name,\n",
        "                 metrics.accuracy_score(y_test, y_test_pred),\n",
        "                 metrics.f1_score(y_test, y_test_pred, average='macro'),\n",
        "                 metrics.recall_score(y_test, y_test_pred, average='macro'),\n",
        "                 metrics.precision_score(y_test, y_test_pred, average='macro'),\n",
        "                 auc_score)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Km6NhCQObcrt"
      },
      "source": [
        " # KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRI3hY30RI6N",
        "outputId": "db954884-41fd-4a92-a7eb-5e2f8f0ddf16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== KNN Model Performance ===\n",
            "\n",
            "Running KNN with PCA configuration...\n",
            "\n",
            "KNN Model Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.999944  0.998028 0.996120   0.999974 1.000000\n",
            "    Test  0.912349  0.702170 0.646281   0.951578 0.824784\n",
            "\n",
            "Running KNN with LDA configuration...\n",
            "\n",
            "KNN Model Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.999944  0.998028 0.996122   0.999972 1.000000\n",
            "    Test  0.914434  0.699349 0.647216   0.951831 0.822758\n"
          ]
        }
      ],
      "source": [
        "# Configuration list to store different data setups\n",
        "configurations = []\n",
        "\n",
        "configurations.append((f'PCA', X_train_pca, X_test_pca, y_train))\n",
        "configurations.append((f'LDA', X_train_lda, X_test_lda, y_train))\n",
        "\n",
        "\n",
        "# Step 7: Run KNN  on different configurations\n",
        "print(\"\\n=== KNN Model Performance ===\")\n",
        "\n",
        "knn = KNeighborsClassifier(\n",
        "    n_neighbors=11,\n",
        "    weights=\"distance\",\n",
        "    metric=\"minkowski\",\n",
        "    p=2,\n",
        "    n_jobs=-1\n",
        ")\n",
        "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "    print(f\"\\nRunning KNN with {name} configuration...\")\n",
        "\n",
        "    knn.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    y_train_pred = knn.predict(X_train_cfg)\n",
        "    y_test_pred  = knn.predict(X_test_cfg)\n",
        "\n",
        "    y_train_proba = knn.predict_proba(X_train_cfg)\n",
        "    y_test_proba  = knn.predict_proba(X_test_cfg)\n",
        "\n",
        "\n",
        "    metrics_dict = {\n",
        "        \"Dataset\": [\"Training\", \"Test\"],\n",
        "        \"Accuracy\": [\n",
        "            metrics.accuracy_score(y_train_cfg, y_train_pred),\n",
        "            metrics.accuracy_score(y_test,      y_test_pred),\n",
        "        ],\n",
        "        \"F1 Score\": [\n",
        "            metrics.f1_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.f1_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"Recall\": [\n",
        "            metrics.recall_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.recall_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"Precision\": [\n",
        "            metrics.precision_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.precision_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"AUC-ROC\": [\n",
        "            metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_proba, multi_class='ovr', average='macro'),\n",
        "            metrics.roc_auc_score(pd.get_dummies(y_test),      y_test_proba,  multi_class='ovr', average='macro'),\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics_dict)\n",
        "    print(\"\\nKNN Model Performance Metrics\")\n",
        "    print(df_metrics.to_string(index=False))\n",
        "\n",
        "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_proba, multi_class='ovr', average='macro')\n",
        "    storeResults('KNN', name,\n",
        "                 metrics.accuracy_score(y_test, y_test_pred),\n",
        "                 metrics.f1_score(y_test, y_test_pred, average='macro'),\n",
        "                 metrics.recall_score(y_test, y_test_pred, average='macro'),\n",
        "                 metrics.precision_score(y_test, y_test_pred, average='macro'),\n",
        "                 auc_score)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbLnqqocdWZR"
      },
      "source": [
        "# Gradient Boosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyc9p_6zRqUK",
        "outputId": "e0f246fb-404a-4ae1-f4e0-1037e7314115"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Gradient Boosting Model Performance ===\n",
            "\n",
            "Running Gradient Boosting with PCA configuration...\n",
            "\n",
            "Gradient Boosting Model Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.995245  0.887866 0.854834   0.934624 0.964626\n",
            "    Test  0.918116  0.647157 0.609985   0.915381 0.920718\n",
            "\n",
            "Running Gradient Boosting with LDA configuration...\n",
            "\n",
            "Gradient Boosting Model Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.977098  0.866553 0.813513   0.963530 0.940413\n",
            "    Test  0.895183  0.629065 0.594028   0.877357 0.918202\n"
          ]
        }
      ],
      "source": [
        "# Configuration list to store different data setups\n",
        "configurations = []\n",
        "\n",
        "configurations.append((f'PCA', X_train_pca, X_test_pca, y_train))\n",
        "configurations.append((f'LDA', X_train_lda, X_test_lda, y_train))\n",
        "\n",
        "# Step 7: Running Gradient Boosting  on different configurations\n",
        "print(\"\\n=== Gradient Boosting Model Performance ===\")\n",
        "\n",
        "gbc = GradientBoostingClassifier(\n",
        "    n_estimators=200,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=3,\n",
        "    min_samples_split=2,\n",
        "    min_samples_leaf=1,\n",
        "    subsample=1.0,\n",
        "    random_state=42\n",
        ")\n",
        "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "    print(f\"\\nRunning Gradient Boosting with {name} configuration...\")\n",
        "\n",
        "    gbc.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    y_train_pred = gbc.predict(X_train_cfg)\n",
        "    y_test_pred  = gbc.predict(X_test_cfg)\n",
        "\n",
        "    y_train_proba = gbc.predict_proba(X_train_cfg)\n",
        "    y_test_proba  = gbc.predict_proba(X_test_cfg)\n",
        "\n",
        "\n",
        "    metrics_dict = {\n",
        "        \"Dataset\": [\"Training\", \"Test\"],\n",
        "        \"Accuracy\": [\n",
        "            metrics.accuracy_score(y_train_cfg, y_train_pred),\n",
        "            metrics.accuracy_score(y_test,      y_test_pred),\n",
        "        ],\n",
        "        \"F1 Score\": [\n",
        "            metrics.f1_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.f1_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"Recall\": [\n",
        "            metrics.recall_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.recall_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"Precision\": [\n",
        "            metrics.precision_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.precision_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"AUC-ROC\": [\n",
        "            metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_proba, multi_class='ovr', average='macro'),\n",
        "            metrics.roc_auc_score(pd.get_dummies(y_test),      y_test_proba,  multi_class='ovr', average='macro'),\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics_dict)\n",
        "    print(\"\\nGradient Boosting Model Performance Metrics\")\n",
        "    print(df_metrics.to_string(index=False))\n",
        "\n",
        "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_proba, multi_class='ovr', average='macro')\n",
        "    storeResults('Gradient Boosting', name,\n",
        "                 metrics.accuracy_score(y_test, y_test_pred),\n",
        "                 metrics.f1_score(y_test, y_test_pred, average='macro'),\n",
        "                 metrics.recall_score(y_test, y_test_pred, average='macro'),\n",
        "                 metrics.precision_score(y_test, y_test_pred, average='macro'),\n",
        "                 auc_score)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hfc0wi85eWOv"
      },
      "source": [
        "#AdaBoosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzUf7LW6SQrF",
        "outputId": "c8d17500-840a-4278-c89c-954eac0e15da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== AdaBoost Model Performance ===\n",
            "\n",
            "Running AdaBoost with PCA configuration...\n",
            "\n",
            "AdaBoosting Model Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.920102  0.516664 0.496415   0.555583 0.972758\n",
            "    Test  0.840357  0.473985 0.459831   0.517886 0.919327\n",
            "\n",
            "Running AdaBoost with LDA configuration...\n",
            "\n",
            "AdaBoosting Model Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.937264  0.554370 0.547612   0.615372 0.982191\n",
            "    Test  0.860628  0.519465 0.515099   0.628611 0.922289\n"
          ]
        }
      ],
      "source": [
        "# Configuration list to store different data setups\n",
        "configurations = []\n",
        "\n",
        "configurations.append((f'PCA', X_train_pca, X_test_pca, y_train))\n",
        "configurations.append((f'LDA', X_train_lda, X_test_lda, y_train))\n",
        "\n",
        "# Step 7: Run AdaBoost  on different configurations\n",
        "print(\"\\n=== AdaBoost Model Performance ===\")\n",
        "\n",
        "ada = AdaBoostClassifier(\n",
        "        estimator=DecisionTreeClassifier(max_depth=1, random_state=42),\n",
        "        n_estimators=200,\n",
        "        learning_rate=0.1,\n",
        "        algorithm=\"SAMME\",\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "    print(f\"\\nRunning AdaBoost with {name} configuration...\")\n",
        "\n",
        "    ada.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    y_train_pred = ada.predict(X_train_cfg)\n",
        "    y_test_pred  = ada.predict(X_test_cfg)\n",
        "\n",
        "    y_train_proba = ada.predict_proba(X_train_cfg)\n",
        "    y_test_proba  = ada.predict_proba(X_test_cfg)\n",
        "\n",
        "\n",
        "    metrics_dict = {\n",
        "        \"Dataset\": [\"Training\", \"Test\"],\n",
        "        \"Accuracy\": [\n",
        "            metrics.accuracy_score(y_train_cfg, y_train_pred),\n",
        "            metrics.accuracy_score(y_test,      y_test_pred),\n",
        "        ],\n",
        "        \"F1 Score\": [\n",
        "            metrics.f1_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.f1_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"Recall\": [\n",
        "            metrics.recall_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.recall_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"Precision\": [\n",
        "            metrics.precision_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.precision_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"AUC-ROC\": [\n",
        "            metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_proba, multi_class='ovr', average='macro'),\n",
        "            metrics.roc_auc_score(pd.get_dummies(y_test),      y_test_proba,  multi_class='ovr', average='macro'),\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics_dict)\n",
        "    print(\"\\nAdaBoosting Model Performance Metrics\")\n",
        "    print(df_metrics.to_string(index=False))\n",
        "\n",
        "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_proba, multi_class='ovr', average='macro')\n",
        "    storeResults('AdaBoosting', name,\n",
        "                 metrics.accuracy_score(y_test, y_test_pred),\n",
        "                 metrics.f1_score(y_test, y_test_pred, average='macro'),\n",
        "                 metrics.recall_score(y_test, y_test_pred, average='macro'),\n",
        "                 metrics.precision_score(y_test, y_test_pred, average='macro'),\n",
        "                 auc_score)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyjP2M1SY-E6"
      },
      "source": [
        "# XGBoosting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZR9tnx0S5hD",
        "outputId": "bba9a068-a33d-4b74-84a3-6d9ab30c6c79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== XGBoost Model Performance ===\n",
            "\n",
            "Running XGBoosting with PCA configuration...\n",
            "\n",
            "XGBoosting Model Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.999103  0.967580 0.949228   0.989951 0.999992\n",
            "    Test  0.921842  0.701052 0.646955   0.942012 0.950115\n",
            "\n",
            "Running XGBoosting with LDA configuration...\n",
            "\n",
            "XGBoosting Model Performance Metrics\n",
            " Dataset  Accuracy  F1 Score  Recall  Precision  AUC-ROC\n",
            "Training  0.990815  0.889547 0.85080   0.965272 0.999748\n",
            "    Test  0.903611  0.658087 0.61437   0.939930 0.948013\n"
          ]
        }
      ],
      "source": [
        "# Configuration list to store different data setups\n",
        "configurations = []\n",
        "\n",
        "le = LabelEncoder()\n",
        "# Fit on the union to avoid unseen class errors if test has a class not in train\n",
        "le.fit(pd.concat([y_train.astype(str), y_test.astype(str)], axis=0))\n",
        "\n",
        "y_train_enc = le.transform(y_train.astype(str))\n",
        "y_test_enc  = le.transform(y_test.astype(str))\n",
        "\n",
        "configurations.append((f'PCA', X_train_pca, X_test_pca,y_train_enc))\n",
        "configurations.append((f'LDA', X_train_lda, X_test_lda, y_train_enc))\n",
        "\n",
        "\n",
        "# Step 7: Run XGBoosting on different configurations\n",
        "print(\"\\n=== XGBoost Model Performance ===\")\n",
        "\n",
        "xgb = XGBClassifier(\n",
        "    n_estimators=200,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=6,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    tree_method=\"hist\",\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "    print(f\"\\nRunning XGBoosting with {name} configuration...\")\n",
        "\n",
        "    xgb.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    y_train_pred = xgb.predict(X_train_cfg)\n",
        "    y_test_pred  = xgb.predict(X_test_cfg)\n",
        "\n",
        "    y_train_proba = xgb.predict_proba(X_train_cfg)\n",
        "    y_test_proba  = xgb.predict_proba(X_test_cfg)\n",
        "\n",
        "\n",
        "    metrics_dict = {\n",
        "        \"Dataset\": [\"Training\", \"Test\"],\n",
        "        \"Accuracy\": [\n",
        "            metrics.accuracy_score(y_train_cfg, y_train_pred),\n",
        "            metrics.accuracy_score(y_test_enc,      y_test_pred),\n",
        "        ],\n",
        "        \"F1 Score\": [\n",
        "            metrics.f1_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.f1_score(y_test_enc,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"Recall\": [\n",
        "            metrics.recall_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.recall_score(y_test_enc,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"Precision\": [\n",
        "            metrics.precision_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.precision_score(y_test_enc,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"AUC-ROC\": [\n",
        "            metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_proba, multi_class='ovr', average='macro'),\n",
        "            metrics.roc_auc_score(pd.get_dummies(y_test_enc),      y_test_proba,  multi_class='ovr', average='macro'),\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics_dict)\n",
        "    print(\"\\nXGBoosting Model Performance Metrics\")\n",
        "    print(df_metrics.to_string(index=False))\n",
        "\n",
        "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_proba, multi_class='ovr', average='macro')\n",
        "    storeResults('XGBoosting', name,\n",
        "                 metrics.accuracy_score(y_test_enc, y_test_pred),\n",
        "                 metrics.f1_score(y_test_enc, y_test_pred, average='macro'),\n",
        "                 metrics.recall_score(y_test_enc, y_test_pred, average='macro'),\n",
        "                 metrics.precision_score(y_test_enc, y_test_pred, average='macro'),\n",
        "                 auc_score)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xFOCq0pwChT"
      },
      "source": [
        "#CatBoosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzTaGUy_wHGq",
        "outputId": "7520bc85-83e3-4d69-f715-163cf81f0761"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== CatBoost Model Performance ===\n",
            "\n",
            "Running CatBoosting with PCA configuration...\n",
            "\n",
            "CatBoosting Model Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.961881  0.677632 0.641925   0.769787 0.983166\n",
            "    Test  0.882674  0.552371 0.545038   0.740712 0.925594\n",
            "\n",
            "Running CatBoosting with LDA configuration...\n",
            "\n",
            "CatBoosting Model Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.955363  0.685338 0.653310   0.747461 0.985957\n",
            "    Test  0.880678  0.554114 0.545225   0.718410 0.927734\n"
          ]
        }
      ],
      "source": [
        "# Configuration list to store different data setups\n",
        "configurations = []\n",
        "\n",
        "configurations.append((f'PCA', X_train_pca, X_test_pca, y_train))\n",
        "configurations.append((f'LDA', X_train_lda, X_test_lda, y_train))\n",
        "\n",
        "\n",
        "# Step 7: Run CatBoosting on different configurations\n",
        "print(\"\\n=== CatBoost Model Performance ===\")\n",
        "cat = CatBoostClassifier(\n",
        "\n",
        "\n",
        "    bagging_temperature =0.05,\n",
        "    boosting_type = 'Plain',\n",
        "    learning_rate=0.05,\n",
        "    depth=3,\n",
        "    n_estimators=100,\n",
        "    random_seed=42,\n",
        "    silent =True\n",
        "\n",
        ")\n",
        "\n",
        "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "    print(f\"\\nRunning CatBoosting with {name} configuration...\")\n",
        "\n",
        "    cat.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    y_train_pred = cat.predict(X_train_cfg)\n",
        "    y_test_pred  = cat.predict(X_test_cfg)\n",
        "\n",
        "    y_train_proba = cat.predict_proba(X_train_cfg)\n",
        "    y_test_proba  = cat.predict_proba(X_test_cfg)\n",
        "\n",
        "\n",
        "    metrics_dict = {\n",
        "        \"Dataset\": [\"Training\", \"Test\"],\n",
        "        \"Accuracy\": [\n",
        "            metrics.accuracy_score(y_train_cfg, y_train_pred),\n",
        "            metrics.accuracy_score(y_test,      y_test_pred),\n",
        "        ],\n",
        "        \"F1 Score\": [\n",
        "            metrics.f1_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.f1_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"Recall\": [\n",
        "            metrics.recall_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.recall_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"Precision\": [\n",
        "            metrics.precision_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.precision_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"AUC-ROC\": [\n",
        "            metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_proba, multi_class='ovr', average='macro'),\n",
        "            metrics.roc_auc_score(pd.get_dummies(y_test),      y_test_proba,  multi_class='ovr', average='macro'),\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics_dict)\n",
        "    print(\"\\nCatBoosting Model Performance Metrics\")\n",
        "    print(df_metrics.to_string(index=False))\n",
        "\n",
        "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_proba, multi_class='ovr', average='macro')\n",
        "    storeResults('CatBoosting', name,\n",
        "                 metrics.accuracy_score(y_test, y_test_pred),\n",
        "                 metrics.f1_score(y_test, y_test_pred, average='macro'),\n",
        "                 metrics.recall_score(y_test, y_test_pred, average='macro'),\n",
        "                 metrics.precision_score(y_test, y_test_pred, average='macro'),\n",
        "                 auc_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmOcixna2M5w"
      },
      "source": [
        "#Bagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NM05mZ_7UqbV",
        "outputId": "291d084a-c6d5-48ee-dc5a-c0ede0454dbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Bagging Model Performance ===\n",
            "\n",
            "Running Bagging with PCA configuration...\n",
            "\n",
            "Bagging Model Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.956094  0.566373 0.555465   0.580336 0.894925\n",
            "    Test  0.873847  0.527334 0.519321   0.543894 0.839667\n",
            "\n",
            "Running Bagging with LDA configuration...\n",
            "\n",
            "Bagging Model Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.958880  0.678052 0.643496   0.758094 0.976385\n",
            "    Test  0.883029  0.550847 0.544146   0.722318 0.922823\n"
          ]
        }
      ],
      "source": [
        "# Configuration list to store different data setups\n",
        "configurations = []\n",
        "\n",
        "configurations.append((f'PCA', X_train_pca, X_test_pca, y_train))\n",
        "configurations.append((f'LDA', X_train_lda, X_test_lda, y_train))\n",
        "\n",
        "# Step 7: Run Bagging Classifier on different configurations\n",
        "print(\"\\n=== Bagging Model Performance ===\")\n",
        "\n",
        "bag = BaggingClassifier(\n",
        "        estimator=DecisionTreeClassifier(max_depth=5, random_state=42),\n",
        "        n_estimators=200,\n",
        "        max_samples=1.0,\n",
        "        max_features=1.0,\n",
        "        bootstrap=True,\n",
        "        bootstrap_features=False,\n",
        "        n_jobs=-1,\n",
        "        random_state=42\n",
        "    )\n",
        "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "    print(f\"\\nRunning Bagging with {name} configuration...\")\n",
        "\n",
        "    bag.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    y_train_pred = bag.predict(X_train_cfg)\n",
        "    y_test_pred  = bag.predict(X_test_cfg)\n",
        "\n",
        "    y_train_proba = bag.predict_proba(X_train_cfg)\n",
        "    y_test_proba  = bag.predict_proba(X_test_cfg)\n",
        "\n",
        "\n",
        "    metrics_dict = {\n",
        "        \"Dataset\": [\"Training\", \"Test\"],\n",
        "        \"Accuracy\": [\n",
        "            metrics.accuracy_score(y_train_cfg, y_train_pred),\n",
        "            metrics.accuracy_score(y_test,      y_test_pred),\n",
        "        ],\n",
        "        \"F1 Score\": [\n",
        "            metrics.f1_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.f1_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"Recall\": [\n",
        "            metrics.recall_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.recall_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"Precision\": [\n",
        "            metrics.precision_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "            metrics.precision_score(y_test,      y_test_pred,  average='macro'),\n",
        "        ],\n",
        "        \"AUC-ROC\": [\n",
        "            metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_proba, multi_class='ovr', average='macro'),\n",
        "            metrics.roc_auc_score(pd.get_dummies(y_test),      y_test_proba,  multi_class='ovr', average='macro'),\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics_dict)\n",
        "    print(\"\\nBagging Model Performance Metrics\")\n",
        "    print(df_metrics.to_string(index=False))\n",
        "\n",
        "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test), y_test_proba, multi_class='ovr', average='macro')\n",
        "    storeResults('Bagging Classifier', name,\n",
        "                 metrics.accuracy_score(y_test, y_test_pred),\n",
        "                 metrics.f1_score(y_test, y_test_pred, average='macro'),\n",
        "                 metrics.recall_score(y_test, y_test_pred, average='macro'),\n",
        "                 metrics.precision_score(y_test, y_test_pred, average='macro'),\n",
        "                 auc_score)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mmlx0Lwv4lMI"
      },
      "source": [
        "# Voting Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "930yU9PYVQhQ",
        "outputId": "1dd77e38-1623-43b9-87f6-f434a3e996d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Voting Model Performance ===\n",
            "\n",
            "Training models with PCA configuration...\n",
            "\n",
            "=== Voting Classifier (hard) with PCA ===\n",
            "\n",
            "Voting Classifier (hard) Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.999944  0.998028 0.996132   0.999962        0\n",
            "    Test  0.918293  0.708529 0.651227   0.958300        0\n",
            "\n",
            "=== Voting Classifier (soft) with PCA ===\n",
            "\n",
            "Voting Classifier (soft) Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.999944  0.998028 0.996132   0.999962 1.000000\n",
            "    Test  0.918293  0.709427 0.651821   0.958422 0.954085\n",
            "\n",
            "=== Voting Classifier (weighted_hard) with PCA ===\n",
            "\n",
            "Voting Classifier (weighted_hard) Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.999944  0.998028 0.996132   0.999962        0\n",
            "    Test  0.918249  0.708476 0.651202   0.958211        0\n",
            "\n",
            "=== Voting Classifier (weighted_soft) with PCA ===\n",
            "\n",
            "Voting Classifier (weighted_soft) Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.999944  0.998028 0.996130   0.999964 1.000000\n",
            "    Test  0.918382  0.709189 0.651697   0.958429 0.954152\n",
            "\n",
            "Training models with LDA configuration...\n",
            "\n",
            "=== Voting Classifier (hard) with LDA ===\n",
            "\n",
            "Voting Classifier (hard) Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.999944  0.998028 0.996135   0.999959        0\n",
            "    Test  0.909865  0.695204 0.642823   0.950052        0\n",
            "\n",
            "=== Voting Classifier (soft) with LDA ===\n",
            "\n",
            "Voting Classifier (soft) Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.999944  0.998028 0.996132   0.999962  1.00000\n",
            "    Test  0.910176  0.695975 0.643530   0.951279  0.95079\n",
            "\n",
            "=== Voting Classifier (weighted_hard) with LDA ===\n",
            "\n",
            "Voting Classifier (weighted_hard) Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.999936  0.998011 0.996130   0.999930        0\n",
            "    Test  0.910220  0.695739 0.643633   0.950268        0\n",
            "\n",
            "=== Voting Classifier (weighted_soft) with LDA ===\n",
            "\n",
            "Voting Classifier (weighted_soft) Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.999944  0.998028 0.996133   0.999961 1.000000\n",
            "    Test  0.909910  0.695734 0.643239   0.951116 0.950927\n"
          ]
        }
      ],
      "source": [
        "# Configuration list to store different data setups\n",
        "configurations = []\n",
        "\n",
        "le = LabelEncoder()\n",
        "# Fit on the union to avoid unseen class errors if test has a class not in train\n",
        "le.fit(pd.concat([y_train.astype(str), y_test.astype(str)], axis=0))\n",
        "\n",
        "y_train_enc = le.transform(y_train.astype(str))\n",
        "y_test_enc  = le.transform(y_test.astype(str))\n",
        "\n",
        "configurations.append((f'PCA', X_train_pca, X_test_pca,y_train_enc))\n",
        "configurations.append((f'LDA', X_train_lda, X_test_lda, y_train_enc))\n",
        "\n",
        "\n",
        "# Step 7: Run Voting Classifier on different configurations\n",
        "print(\"\\n=== Voting Model Performance ===\")\n",
        "\n",
        "# base learners\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=None,\n",
        "    min_samples_split=2,\n",
        "    min_samples_leaf=1,\n",
        "    max_features=\"sqrt\",\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "knn = KNeighborsClassifier(\n",
        "    n_neighbors=11,\n",
        "    weights=\"distance\",\n",
        "    metric=\"minkowski\",\n",
        "    p=2,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "xgb = XGBClassifier(\n",
        "    n_estimators=200,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=6,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    tree_method=\"hist\",\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "    print(f\"\\nTraining models with {name} configuration...\")\n",
        "\n",
        "    rf.fit(X_train_cfg, y_train_cfg)\n",
        "    knn.fit(X_train_cfg, y_train_cfg)\n",
        "    xgb.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    # Define y_test_cfg for each configuration\n",
        "    y_test_cfg = y_test_enc\n",
        "    # Voting configurations using the actual model objects\n",
        "    # === Voting classifier configurations ===\n",
        "    voting_configs = [\n",
        "       (\"hard\", VotingClassifier(estimators=[(\"rf\", rf), (\"knn\", knn), (\"xgb\", xgb)], voting=\"hard\")),\n",
        "       (\"soft\", VotingClassifier(estimators=[(\"rf\", rf), (\"knn\", knn), (\"xgb\", xgb)], voting=\"soft\")),\n",
        "       (\"weighted_hard\", VotingClassifier(estimators=[(\"rf\", rf), (\"knn\", knn), (\"xgb\", xgb)], voting=\"hard\", weights=[0.3, 0.3, 0.4])),\n",
        "       (\"weighted_soft\", VotingClassifier(estimators=[(\"rf\", rf), (\"knn\", knn), (\"xgb\", xgb)], voting=\"soft\", weights=[0.4, 0.3, 0.3]))\n",
        "    ]\n",
        "\n",
        "    for voting_type, voting_clf in voting_configs:\n",
        "        print(f\"\\n=== Voting Classifier ({voting_type}) with {name} ===\")\n",
        "\n",
        "        # Fit voting classifier\n",
        "        voting_clf.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "        # Predictions\n",
        "        y_train_pred = voting_clf.predict(X_train_cfg)\n",
        "        y_test_pred = voting_clf.predict(X_test_cfg)\n",
        "\n",
        "        # Check if voting is hard or soft for probabilities\n",
        "        if 'hard' in voting_type:\n",
        "            # For hard voting, we cannot get probabilities, so set AUC-ROC to 0\n",
        "            auc_train = 0\n",
        "            auc_test = 0\n",
        "        else:\n",
        "            # For soft voting, we can get probabilities\n",
        "            y_train_proba = voting_clf.predict_proba(X_train_cfg)\n",
        "            y_test_proba = voting_clf.predict_proba(X_test_cfg)\n",
        "            auc_train = metrics.roc_auc_score(pd.get_dummies(y_train_cfg), y_train_proba, multi_class='ovr', average='macro')\n",
        "            auc_test = metrics.roc_auc_score(pd.get_dummies(y_test_cfg), y_test_proba, multi_class='ovr', average='macro')\n",
        "\n",
        "        # Calculate metrics\n",
        "        metrics_dict = {\n",
        "            \"Dataset\": [\"Training\", \"Test\"],\n",
        "            \"Accuracy\": [\n",
        "                metrics.accuracy_score(y_train_cfg, y_train_pred),\n",
        "                metrics.accuracy_score(y_test_cfg, y_test_pred),\n",
        "            ],\n",
        "            \"F1 Score\": [\n",
        "                metrics.f1_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "                metrics.f1_score(y_test_cfg, y_test_pred, average='macro'),\n",
        "            ],\n",
        "            \"Recall\": [\n",
        "                metrics.recall_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "                metrics.recall_score(y_test_cfg, y_test_pred, average='macro'),\n",
        "            ],\n",
        "            \"Precision\": [\n",
        "                metrics.precision_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "                metrics.precision_score(y_test_cfg, y_test_pred, average='macro'),\n",
        "            ],\n",
        "            \"AUC-ROC\": [auc_train, auc_test]\n",
        "        }\n",
        "\n",
        "        df_metrics = pd.DataFrame(metrics_dict)\n",
        "        print(f\"\\nVoting Classifier ({voting_type}) Performance Metrics\")\n",
        "        print(df_metrics.to_string(index=False))\n",
        "\n",
        "        storeResults(\n",
        "            f'Voting Classifier ({voting_type})',\n",
        "            name,\n",
        "            metrics.accuracy_score(y_test_cfg, y_test_pred),\n",
        "            metrics.f1_score(y_test_cfg, y_test_pred, average='macro'),\n",
        "            metrics.recall_score(y_test_cfg, y_test_pred, average='macro'),\n",
        "            metrics.precision_score(y_test_cfg, y_test_pred, average='macro'),\n",
        "            auc_test\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYaIECwC6YFz"
      },
      "source": [
        "# Stacking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Qc106BqeAGo",
        "outputId": "caaa8e33-67ed-4d91-d2af-1ed7b907b409"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Stacking Model Performance ===\n",
            "\n",
            "Training models with PCA configuration...\n",
            "\n",
            "Stacking Classifier Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.999944  0.998028 0.996135   0.999959 1.000000\n",
            "    Test  0.918958  0.708942 0.652363   0.958117 0.950927\n",
            "\n",
            "Training models with LDA configuration...\n",
            "\n",
            "Stacking Classifier Performance Metrics\n",
            " Dataset  Accuracy  F1 Score   Recall  Precision  AUC-ROC\n",
            "Training  0.999730  0.926245 0.892288   0.999878 1.000000\n",
            "    Test  0.909466  0.662397 0.616453   0.952689 0.950927\n"
          ]
        }
      ],
      "source": [
        "# Configuration list to store different data setups\n",
        "configurations = []\n",
        "\n",
        "le = LabelEncoder()\n",
        "# Fit on the union to avoid unseen class errors if test has a class not in train\n",
        "le.fit(pd.concat([y_train.astype(str), y_test.astype(str)], axis=0))\n",
        "\n",
        "y_train_enc = le.transform(y_train.astype(str))\n",
        "y_test_enc  = le.transform(y_test.astype(str))\n",
        "\n",
        "configurations.append((f'PCA', X_train_pca, X_test_pca,y_train_enc))\n",
        "configurations.append((f'LDA', X_train_lda, X_test_lda, y_train_enc))\n",
        "\n",
        "\n",
        "# Step 7: Run Stacking Classifier on different configurations\n",
        "print(\"\\n=== Stacking Model Performance ===\")\n",
        "\n",
        "# base learners\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=None,\n",
        "    min_samples_split=2,\n",
        "    min_samples_leaf=1,\n",
        "    max_features=\"sqrt\",\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "knn = KNeighborsClassifier(\n",
        "    n_neighbors=11,\n",
        "    weights=\"distance\",\n",
        "    metric=\"minkowski\",\n",
        "    p=2,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "cat = CatBoostClassifier(\n",
        "    learning_rate=0.05,\n",
        "    depth=3,\n",
        "    n_estimators=100,\n",
        "    bagging_temperature=0.05,\n",
        "    boosting_type='Plain',\n",
        "    random_seed=42,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# Train each model\n",
        "for name, X_train_cfg, X_test_cfg, y_train_cfg in configurations:\n",
        "    print(f\"\\nTraining models with {name} configuration...\")\n",
        "\n",
        "    rf.fit(X_train_cfg, y_train_cfg)\n",
        "    knn.fit(X_train_cfg, y_train_cfg)\n",
        "    cat.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    # Define y_test_cfg for each configuration\n",
        "    y_test_cfg = y_test_enc\n",
        "    # Create stacking classifier\n",
        "    base_estimators = [\n",
        "        ('rf', rf),\n",
        "        ('knn', knn),\n",
        "        ('cat', cat)\n",
        "    ]\n",
        "\n",
        "    # Meta-learner\n",
        "    meta_learner = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "    # Stacking classifier\n",
        "    stacking_clf = StackingClassifier(\n",
        "        estimators=base_estimators,\n",
        "        final_estimator=meta_learner,\n",
        "        cv=5,  # Use 5-fold cross-validation to train meta-learner\n",
        "        stack_method='predict_proba',  # Use probabilities for meta-features\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # Fit stacking classifier\n",
        "    stacking_clf.fit(X_train_cfg, y_train_cfg)\n",
        "\n",
        "    # Predictions\n",
        "    y_train_pred = stacking_clf.predict(X_train_cfg)\n",
        "    y_test_pred = stacking_clf.predict(X_test_cfg)\n",
        "    y_train_proba = stacking_clf.predict_proba(X_train_cfg)\n",
        "    y_test_proba = stacking_clf.predict_proba(X_test_cfg)\n",
        "\n",
        "    # Calculate metrics\n",
        "    # Calculate metrics\n",
        "    metrics_dict = {\n",
        "            \"Dataset\": [\"Training\", \"Test\"],\n",
        "            \"Accuracy\": [\n",
        "                metrics.accuracy_score(y_train_cfg, y_train_pred),\n",
        "                metrics.accuracy_score(y_test_cfg, y_test_pred),\n",
        "            ],\n",
        "            \"F1 Score\": [\n",
        "                metrics.f1_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "                metrics.f1_score(y_test_cfg, y_test_pred, average='macro'),\n",
        "            ],\n",
        "            \"Recall\": [\n",
        "                metrics.recall_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "                metrics.recall_score(y_test_cfg, y_test_pred, average='macro'),\n",
        "            ],\n",
        "            \"Precision\": [\n",
        "                metrics.precision_score(y_train_cfg, y_train_pred, average='macro'),\n",
        "                metrics.precision_score(y_test_cfg, y_test_pred, average='macro'),\n",
        "            ],\n",
        "            \"AUC-ROC\": [auc_train, auc_test]\n",
        "        }\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics_dict)\n",
        "    print(f\"\\nStacking Classifier Performance Metrics\")\n",
        "    print(df_metrics.to_string(index=False))\n",
        "\n",
        "    auc_score = metrics.roc_auc_score(pd.get_dummies(y_test_cfg), y_test_proba, multi_class='ovr', average='macro')\n",
        "\n",
        "    storeResults(\n",
        "           'Stacking Classifier', name,\n",
        "            metrics.accuracy_score(y_test_cfg, y_test_pred),\n",
        "            metrics.f1_score(y_test_cfg, y_test_pred, average='macro'),\n",
        "            metrics.recall_score(y_test_cfg, y_test_pred, average='macro'),\n",
        "            metrics.precision_score(y_test_cfg, y_test_pred, average='macro'),\n",
        "            auc_test\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5oHNQ_wZMbz"
      },
      "source": [
        "# Data Frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXhgVw5_nMH7",
        "outputId": "9b8d5016-e9fe-43e4-bbf6-57fda7a48053"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "====================================================================================================\n",
            "MODEL PERFORMANCE RESULTS\n",
            "====================================================================================================\n",
            "                         ML Model Configuration Accuracy F1 Score  Recall Precision ROC_AUC\n",
            "           Support Vector Machine           PCA  90.033%  56.825% 55.955%   73.791% 94.693%\n",
            "           Support Vector Machine           LDA  88.525%  55.616% 54.791%   71.908% 89.281%\n",
            "                    Random Forest           PCA  91.789%  70.735% 65.054%   95.808% 87.724%\n",
            "                    Random Forest           LDA  90.916%  69.257% 64.174%   93.438% 86.772%\n",
            "                              KNN           PCA  91.235%  70.217% 64.628%   95.158% 82.478%\n",
            "                              KNN           LDA  91.443%  69.935% 64.722%   95.183% 82.276%\n",
            "                Gradient Boosting           PCA  91.812%  64.716% 60.998%   91.538% 92.072%\n",
            "                Gradient Boosting           LDA  89.518%  62.907% 59.403%   87.736% 91.820%\n",
            "                      AdaBoosting           PCA  84.036%  47.398% 45.983%   51.789% 91.933%\n",
            "                      AdaBoosting           LDA  86.063%  51.946% 51.510%   62.861% 92.229%\n",
            "                       XGBoosting           PCA  92.184%  70.105% 64.695%   94.201% 95.011%\n",
            "                       XGBoosting           LDA  90.361%  65.809% 61.437%   93.993% 94.801%\n",
            "                      CatBoosting           PCA  88.267%  55.237% 54.504%   74.071% 92.559%\n",
            "                      CatBoosting           LDA  88.068%  55.411% 54.522%   71.841% 92.773%\n",
            "               Bagging Classifier           PCA  87.385%  52.733% 51.932%   54.389% 83.967%\n",
            "               Bagging Classifier           LDA  88.303%  55.085% 54.415%   72.232% 92.282%\n",
            "         Voting Classifier (hard)           PCA  91.829%  70.853% 65.123%   95.830%  0.000%\n",
            "         Voting Classifier (soft)           PCA  91.829%  70.943% 65.182%   95.842% 95.408%\n",
            "Voting Classifier (weighted_hard)           PCA  91.825%  70.848% 65.120%   95.821%  0.000%\n",
            "Voting Classifier (weighted_soft)           PCA  91.838%  70.919% 65.170%   95.843% 95.415%\n",
            "         Voting Classifier (hard)           LDA  90.987%  69.520% 64.282%   95.005%  0.000%\n",
            "         Voting Classifier (soft)           LDA  91.018%  69.597% 64.353%   95.128% 95.079%\n",
            "Voting Classifier (weighted_hard)           LDA  91.022%  69.574% 64.363%   95.027%  0.000%\n",
            "Voting Classifier (weighted_soft)           LDA  90.991%  69.573% 64.324%   95.112% 95.093%\n",
            "              Stacking Classifier           PCA  91.896%  70.894% 65.236%   95.812% 95.093%\n",
            "              Stacking Classifier           LDA  90.947%  66.240% 61.645%   95.269% 95.093%\n",
            "\n",
            "Results saved to model_results.csv\n",
            "\n",
            "====================================================================================================\n",
            "SORTED MODEL PERFORMANCE RESULTS (by Accuracy and F1 Score)\n",
            "====================================================================================================\n",
            "                         ML Model Configuration Accuracy F1 Score  Recall Precision ROC_AUC\n",
            "                       XGBoosting           PCA  92.184%  70.105% 64.695%   94.201% 95.011%\n",
            "              Stacking Classifier           PCA  91.896%  70.894% 65.236%   95.812% 95.093%\n",
            "Voting Classifier (weighted_soft)           PCA  91.838%  70.919% 65.170%   95.843% 95.415%\n",
            "         Voting Classifier (soft)           PCA  91.829%  70.943% 65.182%   95.842% 95.408%\n",
            "         Voting Classifier (hard)           PCA  91.829%  70.853% 65.123%   95.830%  0.000%\n",
            "Voting Classifier (weighted_hard)           PCA  91.825%  70.848% 65.120%   95.821%  0.000%\n",
            "                Gradient Boosting           PCA  91.812%  64.716% 60.998%   91.538% 92.072%\n",
            "                    Random Forest           PCA  91.789%  70.735% 65.054%   95.808% 87.724%\n",
            "                              KNN           LDA  91.443%  69.935% 64.722%   95.183% 82.276%\n",
            "                              KNN           PCA  91.235%  70.217% 64.628%   95.158% 82.478%\n",
            "Voting Classifier (weighted_hard)           LDA  91.022%  69.574% 64.363%   95.027%  0.000%\n",
            "         Voting Classifier (soft)           LDA  91.018%  69.597% 64.353%   95.128% 95.079%\n",
            "Voting Classifier (weighted_soft)           LDA  90.991%  69.573% 64.324%   95.112% 95.093%\n",
            "         Voting Classifier (hard)           LDA  90.987%  69.520% 64.282%   95.005%  0.000%\n",
            "              Stacking Classifier           LDA  90.947%  66.240% 61.645%   95.269% 95.093%\n",
            "                    Random Forest           LDA  90.916%  69.257% 64.174%   93.438% 86.772%\n",
            "                       XGBoosting           LDA  90.361%  65.809% 61.437%   93.993% 94.801%\n",
            "           Support Vector Machine           PCA  90.033%  56.825% 55.955%   73.791% 94.693%\n",
            "                Gradient Boosting           LDA  89.518%  62.907% 59.403%   87.736% 91.820%\n",
            "           Support Vector Machine           LDA  88.525%  55.616% 54.791%   71.908% 89.281%\n",
            "               Bagging Classifier           LDA  88.303%  55.085% 54.415%   72.232% 92.282%\n",
            "                      CatBoosting           PCA  88.267%  55.237% 54.504%   74.071% 92.559%\n",
            "                      CatBoosting           LDA  88.068%  55.411% 54.522%   71.841% 92.773%\n",
            "               Bagging Classifier           PCA  87.385%  52.733% 51.932%   54.389% 83.967%\n",
            "                      AdaBoosting           LDA  86.063%  51.946% 51.510%   62.861% 92.229%\n",
            "                      AdaBoosting           PCA  84.036%  47.398% 45.983%   51.789% 91.933%\n",
            "\n",
            "Sorted results saved to sorted_model_results.csv\n",
            "\n",
            "====================================================================================================\n",
            "TOP CONFIGURATION PER MODEL\n",
            "====================================================================================================\n",
            "                         ML Model Configuration Accuracy F1 Score  Recall Precision ROC_AUC\n",
            "                      AdaBoosting           LDA  86.063%  51.946% 51.510%   62.861% 92.229%\n",
            "               Bagging Classifier           LDA  88.303%  55.085% 54.415%   72.232% 92.282%\n",
            "                      CatBoosting           PCA  88.267%  55.237% 54.504%   74.071% 92.559%\n",
            "                Gradient Boosting           PCA  91.812%  64.716% 60.998%   91.538% 92.072%\n",
            "                              KNN           LDA  91.443%  69.935% 64.722%   95.183% 82.276%\n",
            "                    Random Forest           PCA  91.789%  70.735% 65.054%   95.808% 87.724%\n",
            "              Stacking Classifier           PCA  91.896%  70.894% 65.236%   95.812% 95.093%\n",
            "           Support Vector Machine           PCA  90.033%  56.825% 55.955%   73.791% 94.693%\n",
            "         Voting Classifier (hard)           PCA  91.829%  70.853% 65.123%   95.830%  0.000%\n",
            "         Voting Classifier (soft)           PCA  91.829%  70.943% 65.182%   95.842% 95.408%\n",
            "Voting Classifier (weighted_hard)           PCA  91.825%  70.848% 65.120%   95.821%  0.000%\n",
            "Voting Classifier (weighted_soft)           PCA  91.838%  70.919% 65.170%   95.843% 95.415%\n",
            "                       XGBoosting           PCA  92.184%  70.105% 64.695%   94.201% 95.011%\n",
            "\n",
            "Top configuration per model saved to top_configurations.csv\n"
          ]
        }
      ],
      "source": [
        "# Creating the dataframe\n",
        "\n",
        "# Make sure output folder exists\n",
        "out_dir = Path(\"final_results\")\n",
        "out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "result = pd.DataFrame({\n",
        "    'ML Model': ML_Model,\n",
        "    'Configuration': ML_Config,\n",
        "    'Accuracy': [f\"{acc * 100:.3f}%\" for acc in accuracy],\n",
        "    'F1 Score': [f\"{f1 * 100:.3f}%\" for f1 in f1_score],\n",
        "    'Recall': [f\"{rec * 100:.3f}%\" for rec in recall],\n",
        "    'Precision': [f\"{prec * 100:.3f}%\" for prec in precision],\n",
        "    'ROC_AUC': [f\"{roc * 100:.3f}%\" for roc in auc_roc],\n",
        "})\n",
        "\n",
        "# Remove duplicates based on model and configuration\n",
        "result.drop_duplicates(subset=[\"ML Model\", \"Configuration\"], inplace=True)\n",
        "\n",
        "# Display the result\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"MODEL PERFORMANCE RESULTS\")\n",
        "print(\"=\" * 100)\n",
        "print(result.to_string(index=False))\n",
        "\n",
        "# Save the result to a CSV file\n",
        "result.to_csv('final_results/model_results.csv', index=False)\n",
        "print(\"\\nResults saved to model_results.csv\")\n",
        "\n",
        "# Sort by Accuracy and F1 Score\n",
        "sorted_result = result.sort_values(by=['Accuracy', 'F1 Score'], ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Display the sorted result\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"SORTED MODEL PERFORMANCE RESULTS (by Accuracy and F1 Score)\")\n",
        "print(\"=\" * 100)\n",
        "print(sorted_result.to_string(index=False))\n",
        "\n",
        "# Save the sorted result\n",
        "sorted_result.to_csv('final_results/sorted_model_results.csv', index=False)\n",
        "print(\"\\nSorted results saved to sorted_model_results.csv\")\n",
        "\n",
        "# Extract top configuration per ML model\n",
        "top_per_model = sorted_result.groupby('ML Model', as_index=False).first()\n",
        "\n",
        "# Display and save the top configuration table\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"TOP CONFIGURATION PER MODEL\")\n",
        "print(\"=\" * 100)\n",
        "print(top_per_model.to_string(index=False))\n",
        "\n",
        "top_per_model.to_csv('final_results/top_configurations.csv', index=False)\n",
        "print(\"\\nTop configuration per model saved to top_configurations.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
